<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【好文】滴滴出行技术总监：关于技术选型的那些事儿]]></title>
    <url>%2F2018%2F07%2F27%2Farticle%2Farticle_1%2F</url>
    <content type="text"><![CDATA[转载：https://news.cnblogs.com/n/563792/ 杜欢，滴滴出行技术总监，负责滴滴小巴业务的技术管理工作。在互联网领域已经有十年工作经验，曾就职于微软、百度，也曾自主创业两次，来到滴滴之后也经历过很多项目和业务的变化，是一个“什么都懂”工程师，对前端、客户端、服务端、运维等方面都有不少实战经验。平时是一个 ACG 宅，也喜欢阅读各种技术和非技术的文章扩大视野，不愿主动交谈，但一旦放松了就聊到停不下来。 技术选型案例 今天会聊技术选型这个话题，主要就是因为我经历相对比较丰富，亲历过不少项目选型的过程，自己也做过不少靠谱或者不靠谱的决策，在这个方面也有些自己的思考。我想先从几个案例开始，像讲故事一样聊聊选型背后的事，作为话题的开始。 在我刚开始工作时就经历过一次很大的选型事件，我是这件事情的旁观者。当时公司希望做一个非常酷炫的手机界面系统，恰逢 Windows Vista 一系列新技术的发布，包括 WPF、Silverlight、C# 这些技术非常火，公司对它们抱有极高的期望，所以就想第一时间用在新一代 Windows Mobile 上面。确实界面开发和各种效果可以做的很酷炫也节省了界面开发时间，但是很尴尬的遇到了另外一个问题，性能问题。 这些东西都是跑在移动设备上面，当年的移动设备内存能有 32MB，CPU 能到 1GHz 就很不错了，根本不能很好的支撑这一整套界面系统对性能的要求。后来，当公司发现确实在当时的硬件环境下突破性能问题，就对所有界面做了一次重写，回到了用 C++ 和各种 API 传统写界面方式上才解决问题，这里面涉及到将近一千名工程师一年多的时间，可以说是个很大的人力和时间的损失。 当时我还不是很理解，为什么公司不能更早一点止损，后来我慢慢发现，这真的是当局者迷，当一个决策作出之后大家就天然的希望能通过努力来解决眼前的问题，结果反而越陷越深。这也意味着最初选型的时候得十分谨慎，特别是选型影响面巨大时保守点会更好。 后来加入了真正的互联网公司，我看到了技术选型是稳定压倒一切。比如 gcc、linux 内核这些非常底层和关键的东西，在互联网公司里基本不会去追最新版，只是保持了解和跟进，非常克制的将一些 patch 和功能引入到线上环境，真正上线也会经历相当久的灰度验证过程。 我印象挺深的是当年（2009 年）对 lighttpd 和 apache 的选型，当时 lighttpd 单机性能明显优于 apache，同时也支持 php 扩展，能够以 mod 形式运行 php，看起来使用 lighttpd 全面替换 apache 就好了，但实际上为了业务稳定性，真正的用法是将 lighttpd 做反向代理，后面还是使用 apache + mod_php 来提供服务。这里面的思考就是对于一个新技术的天然不信任，在技术接受程度还不够高且公司内没有人能吃透这个技术的情况下，不愿意让自己的业务做第一个吃螃蟹的人。 谨慎确实是个美德，不过如果在一个非常追求速度的业务里，这可能也意味着过于保守，会延误时机。 我在自己创业的过程中选型就比较激进，也玩的比较 high。 比如我会积极的使用 MongoDB，我对它灵活的数据结构、强大的查询语句和内置的高可用机制等非常认可，当它刚刚 1.0 的时候就将它用在一些不重要的数据上，后来等到 2.x 发布后就开始尝试用在新业务上作为核心数据库。我也曾经遇到一些严重的坑，比如数据损坏、扩容不及时造成停机等，但是由于业务对这些问题容忍度较高，同时也有一些兜底方案，所以还不至于成为业务瓶颈，总体来说利大于弊，可以节省业务开发人员的宝贵时间。 我也曾决策使用 Node.js 作为主力服务器开发工具，当时（2013 年）因为客户端要使用 Javascript 作为主力语言，服务端和客户端会有不少能够复用的代码，所以挺想使用 Node.js 来提升开发效率。 为了验证 Node.js 是否靠谱，我自己通读了源码、阅读了不少相关文章、看了下官方 release note 及社区活跃程度（github issues、stackoverflow 讨论等）、还做了一些基本的压测，最后的结论是，它的性能可以满足要求，在稳定性方面基本合格，考虑到只是用它做无状态服务，且单台服务器上都会跑多个实例（当时使用 supervisord 管理），简单的崩溃不会对系统有明显影响，再加上当时确实也有些公司将它作为主力服务，所以最终选择了它。 后来加入滴滴后，我在技术选型方面综合了以前所有的经验，有做得好的，也有犯错的时候。 2015 年滴滴有一个很大的内部代码重构项目，涉及到服务端和客户端大量代码。客户端的技术选型做的相对较好，针对当时代码库多业务耦合严重，大家开发时候模块间冲突频繁的问题，评估并引入了 cocoapods 和 maven/gradle 作为 iOS 和 Android 的项目拆分工具，并且通过代码重构，将客户端项目分成几个独立的仓库，可以让业务独立开发的同时，也能通过构建脚本轻松的整合成一个完成的 app。 服务端的选型则比较错误，当时考虑到滴滴的业务模式非常类似于 erlang 的 actor 模型，一个叫车流程会涉及到非常多可复用的 actor，如果我们直接实现一个分布式的 actor 模型和数据流管理机制，那么很多问题就迎刃而解了。可是当时并不存在一套这样的机制，我们自己在实现的时候采用 Go + kafka 分别实现 actor 和数据流存储，过程中遇到了 kafka 消息丢失不好定位、actor 模型过于抽象不容易在整个团队贯彻执行等问题，最终放弃了整个方案。 技术选型方法论技术选型关键需要思考三个角度：技术、业务和人。 角度之一：技术 技术选型首先考虑的当然是技术本身，这里提到的技术包括语言、框架、工具、设计模式、开发模式等。 在选择技术时有两个大原则。第一，要取其长避其短；第二，要关注技术的发展前景。 每种技术都是有它特定的适用场景的，“没有银弹”。开发者经常犯的错误就是盲目追新，当一个新语言、框架、工具出现后，特别是开发者自己学会了这种新技术后，就会有种“拿着锤子找钉子”的感觉，将新技术滥用于各种项目。 比如最近几年 Go 在国内很火，我自己也非常使用它开发项目，但绝对不应该将它用于所有项目。Go 的优点是上手快、运行时性能高、方便的使用多核运算能力等，经常被提起的特性是超轻线程 goroutine、内置的内存队列 chan、极快的编译速度，非常适合于编写各种无状态应用服务，无需使用任何的第三方框架都能轻松写出一个高性能的 http 服务。 但它的缺点也非常明显，最痛的一点是 gc。Go 在设计之初就号称要实现一个世界上最优秀的 gc，可惜直到今天也还差的较远，最近一年才实现了 jvm 几年前就做到的并发 gc，并且没有很好的方法解决内存碎片和对象过多带来的性能问题。这些缺陷使得 Go 不太适合做有状态服务，特别不适合做内存管理相关的服务，在这些场景里面还是 C/C++ 更加可靠。 技术的发展前景也是一个重要考虑因素。有些技术设计的很好，比如我个人挺喜欢一个叫做 Io 的语言，但我不会把它用于真实项目，因为这个语言缺乏社区和长期支持，就算设计理念写的再好，里面也必然有各种 bug 和不足，如果没人能够解决就会带来严重的问题。技术的“前景”可以从几个维度来判断，有没有长期规划、有没有持续投入的人或者社区、问题解决的速度如何、业界使用案例及口碑、源码质量。 选择一个技术最低限的标准是，技术的生命周期必须显著长于项目的生命周期。想象一下，如果项目还没做完这个技术就不被维护了，那将是怎样一种窘境。拿去年很火的 Vue.js 来说，尤大在规划、投入和解决问题速度方面都没有问题，这是这个技术能火起来的基本保障，再加上设计优雅、源码确实写的不错，它的成功并不偶然。可以预见，随着尤大全职开发这个框架并且社区贡献者越来越多，Vue.js 能持续几年应该问题不大。 滴滴的 web app，比如微信钱包里面的滴滴入口，就在去年年底全面改用 Vue.js 重构了一版，我们就是看中了 Vue.js 在移动应用开发中的优势再加上对它的前景有信心。在重构前，我们为了确认 Vue.js 真的能承担如此大任，公共前端团队在 2016 年花了半年的时间整体梳理和评估了 Vue.js 1.0 和 2.0 的全部源码，为此还出了一本书，在公司大规模使用前也在滴滴小巴业务和行程分享功能里做了试点，效果非常不错，最终才真正下定决心广泛推广。 技术的发展前景是动态变化的，当一个技术走向了末路，我们也应该勇敢的扬弃。拿 jQuery 为例，最开始它是前端开发的必需品，当时很多前端同学离开了 $ 函数就不会写代码了，它在简化 DOM 操作、抹平浏览器间差异做出了极其重要的贡献。但是随着浏览器越来越标准和趋同，jQuery 的亮点已经不再吸引人，它的插件开发模式逐步被模块化开发给取代，再加上各种历史包袱，它所适用的项目也会变得越来越少，新项目在选型的时候就不推荐优先考虑 jQuery 了。 对于一家大型公司来说，其核心业务的技术选型更需谨慎，看前景时甚至需要考虑技术的独立性。依然把 Go 当做一个例子，当前核心 Go Authors 基本都受雇于 Google，也没有一个独立运作的基金会来负责语言的长期维护，更没有一个公开透明的决策机制来决定语言的未来，假如 Google 出于某种原因停止投入或者改变语言的发展方向，那么这对一家大型公司来说可能会是毁灭性打击。立志于成为一家千亿美元规模的公司，或者是 Google 的潜在竞争对手，在选择使用 Go 时就应该更加谨慎，不要盲从。 角度之二：业务 技术选型必须贴着业务来选择，不同业务阶段会有不同的选型方式。 处于初创期的业务，选型的关键词是“灵活”。只要一个技术够用且开发效率足够高，那么就可以选择它。初创的业务往往带有风险性和不确定性，朝令夕改、反复试错是常态，技术必须适应业务的节奏，然后才是其他方面。MongoDB 是一个很好的例子，相比 MySQL，它的数据结构灵活多变，相比一般的 KV 存储，它又具有类似 SQL 的复杂查询能力，再加上它内置的傻瓜式高可用和水平扩展机制，让它能够很好的适应初创业务对效率的追求。 等业务进入稳定期，选型的关键词是“可靠”。技术始终是业务的基石，当业务稳定了技术不稳，那就会成为业务的一块短板，就必须要修正。当年 Twitter 放弃 RoR 选择 Java 系框架，这就是个很好的例子。RoR 以快速开发著称，但同时 ruby 的性能非常有限，Twitter 工程团队针对 ruby 虚拟机做了非常多性能优化可是依然不能达到预期，再加上当时的 Twitter 为了提升前端体验，全面使用模块化和异步化的方法加载页面，服务端已经基本不怎么负责渲染页面，而专注于提供各种 RESTful API，RoR 的优势也不太明显了。 当业务步入维护期，选型的关键词是“妥协”。代码永远有变乱的趋势，一般经过一两年就有必要对代码来一次大一点的重构。在这种时候，必须得正视各种遗留代码的迁移成本，如果改变技术选型会带来遗留代码重写，这背后带来的代价业务无法承受，那么我们就不得不考虑在现有技术选型之上做一些小修小补或者螺旋式上升的重构。 正因为技术选型和业务相关，我们能够观察到一些很明显的现象：新技术往往被早期创业团队或大公司的新兴业务使用；中大型公司的核心业务则更倾向于用一些稳定了几年的技术；一个公司如果长期使用一种技术，就会倾向于一直使用下去，甚至连版本都不更新的使用下去。这现象背后都是有道理的。 角度之三：人 技术选型过程中最终影响决策的还是人本身，这里要强调一下，我说的“人”是指的个人，而不是团队。 技术选型的决策流程一定得专制。决策者可以在调研的时候体恤民情，并把团队现状当做一个因素考虑进来，但绝对不能采用类似“少数服从多数”、“按着大家习惯来”的方式选型。专制可以使技术选型更加的客观，考虑的更加全面，并且使得权责统一。 并不是每个人都懂得怎么为项目负责，一个基层的开发人员思考的更多的可能是技术是否有挑战、能否做出彩、甚至未来好不好找工作，这些主观因素可能会给选型带来灾难性的后果。专制也使得“螺旋式上升”成为可能，很多时候我们没法一蹴而就的使用某种技术，这时候需要有一个领路人，带着大家坚定的朝一条曲折的路线前进才能获得成功。 技术选型也非常依赖于人的能力。选型是一件很难被标准化的过程，选型的决策质量跟人的眼界、经验、业务敏感度、逻辑性等息息相关。就我自己来说，我在面临一个选型问题时首先考虑的是去学习，看看公司内外类似的问题如何解决的，避免自己闭门造车，然后思考所有的可能性，列举最核心需要考虑的因素，心里列一个方案优劣对比，最后将这些逻辑整理清楚，落地成一个决策。 滴滴在决策客户端动态化方向时就是以这样的方式来进行的，我们将业界所有可能的方案都拿出来，理解他们的优缺点，然后在某次会议上几个核心同学在白板上列了一张表格，以考虑的因素为行，可能的方案为列，分别评估各个方案在每种因素里的优劣势，最终确定了一个结论。我们选择的路是偏向于客户端开发的动态化方案，在保留所有代码和工具链的前提下做到对开发者透明的动态化，这样能让整体迁移和维护代价变得最小，当然，这条路开发难度也相当大，幸好我们当时也找到了最合适的人，我们依然可以在能接受的时间里实现整个方案。 培养技术选型的能力 可以看到，要想做好技术选型还是挺难的，要想做好得有足够的知识积累和实际踩坑的经历才行。如果一个不太懂得如何选型的新人想学着做好这件事，那可以先从小项目开始做尝试，慢慢积累经验。技术选型对人来说最重要的还是“逻辑性”，每一个决策背后都藏着许多假设和事实，我们通过不断挑战这些背后的东西来逐步成长。 比如在需要使用缓存来加快数据访问速度的场景中，我们可能会很自然的选择 redis 作为缓存服务。这看似“直觉”的决策，背后也是由一系列假设和事实组成。可以问自己一连串问题，看看在具体的场景下这个决策是不是真的正确，例如，缓存服务有没有 redis 之外的选项、是否可以在内存里直接缓存、redis 是否稳定、redis 性能是否满足需求、数据库访问速度瓶颈究竟在哪等等问题，很可能最终结果还是“ 使用 redis 做缓存”这个直观方案，但正因为有分析的过程，让我们在下一次做决策可以更迅速、更自信。 如何保持敏感性和广度 技术选型是个很需要经验的活，得有大量的信息积累和输入，再根据具体现实情况输出一个结果。我们在选型的时候最忌讳的是临时抱佛脚、用网上收集一些碎片知识来决策，这是非常危险的，我们得确保自己所有思考都是基于以前的事实，还要弄清楚这些事实背后的假设，这都需要让知识内化形成经验。 我一直在想，“经验”的本质是什么，有什么方法能够确定自己的经验增长了，而不是不断在重复一些很熟悉的东西。我现在的结论是，经验等于“知识索引”的完备程度。 我们一生中会积累很多的知识，如果把我们的大脑比作数据库的话，那我们一定有一部分脑存储贡献给了内容的索引，它能帮助我们将关联知识更快的取出来，并且辅助决策。经验增长等同于我们知识索引的增长，意味着我们能轻易的调动更多的关联知识来做更全面的决策。 要想建立好这个知识索引，我们得保持技术敏感性和广度，也就是要做到持续的信息输入、内化，并发现信息之间的关联性，建立索引，记下来。说起来容易，做起来还是挺有难度的。 首先难在信息输入量大，忘记了怎么办。我们的大脑不是磁盘，不常用的知识就会忘记，忘记了就跟没看过是一回事。我的经验是一定要对知识进行压缩，记住的是最关键的细节，并且反复的去回味这个细节。 比如我学习各种语言的时候就会非常留意一些最有特色的语法特性和应用场景，像 C++，我一直记得很早以前看过的细节，像编译器默认会生成哪些类方法，默认析构、拷贝构造、operator = 等，默认生成的类方法有哪些场景需要显示禁用，什么时候要在构造函数用 explicit 等，我看这些细节已经超过十五年的时间了，依然记忆尤新。 看起来好像有点难度，实际上不难，大家想想自己学过的英文单词，再怎么样最常见的几百个英文单词还是能清楚的记得含义的，而技术的知识点其实压缩之后会远小于英文单词的个数，记忆负担不会有想象中那么大。 然后难在信息更新速度太快，跟不上技术发展怎么办。我学习了非常多技术之后就会发现这确实是个难解的问题，像前端开发，每年都会有新的框架和开发方式出现，ES7 的语法如果不去提前了解，过两年可能连 Javascript 语法都看不懂了。 我在这个问题上也是有些焦虑的，不过多少还是有应对的方式，就是坚持碎片化学习，增量更新过时的内容，只要形成习惯也还是能够慢慢的找到自己的节奏。如果有些技术实在细节太多，比如 Node.js 这种，我以前曾经通读过源码，仔细研究过内部设计，但随着它不断发展现在我也不太敢说对它内部有多熟悉，那我会考虑大胆的放弃追新，等着我可能需要用它的时候再统一更新到最新的知识。 最后难在信息究竟如何存入知识索引，知识太零散形成不了体系，建不了索引怎么办。最入门的做法是看书，看别人是怎么将知识变成一个个章节的信息。要想掌握建立索引背后的方法论，我的经验是先从两个相近的技术开始，找到建索引的感觉，然后再铺开去学习更多知识。有这样困惑的开发者往往在学习方面有些贪心，觉得自己记性好可以囫囵吞枣式的将知识强行内化，这样做短期可以，长期还是会遗忘，也形成不了经验。 其实技术知识之间非常像，有很多共性的点可以挖掘。比如客户端和前端开发，各个框架在 View 生命周期管理、消息派发机制等方面非常像，后端开发则更加的套路化，无论用那种语言，最基本的分布式服务原理、缓存、队列、数据库等基础组件原理，都万变不离其宗。 如果我们更宏观的看每个领域，甚至于都能发现领域之间的知识体系划分也很类似。作为表现层的前端和客户端，知识体系都可以分为语言、API、工程化、框架和设计模式。比如前端的语言包括 HTML、CSS、Javascript 和一些稍小众的 TypeScript、CoffeeScript 等，API 就是各种标准、接口的使用、能够实现的效果、平台限制等，工程化就是各种打包工具、代码转化工具、辅助开发工具等，框架就是像 Vue、React 等，设计模式就是像 PWA、redux 等。 相应的，刚刚说的这些知识都能找到在 iOS 或 Android 里几乎对应的知识，无非换了一些细节，这里我就不继续展开了。服务端也是这样，知识体系最顶层的部分也很少，具体到细节，只是要了解每一个实现背后的优劣。 总结一下，技术选型依赖于经验，经验又来源于知识索引的建设，这依赖于平时的总结和不断的新知识输入，技术是一辈子的事，必须得投入大量时间维持状态。学无止尽，大家一起共勉。]]></content>
      <categories>
        <category>好文</category>
      </categories>
      <tags>
        <tag>技术选型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[许式伟、张宴——系统架构运维思路对话]]></title>
    <url>%2F2018%2F07%2F10%2Fjava%2Fjava_4%2F</url>
    <content type="text"><![CDATA[许式伟：作为系统架构师，您一般会从哪些方面来保证网站的高可用性（降低故障时间）？张宴：很多因素都会导致网站发生故障，从而影响网站的高可用性，比如服务器硬件故障、软件系统故障、IDC机房故障、程序上线前测试未发现的Bug、遭受分布式攻击、突发访问人数剧增等。 一套良好的网站系统架构，应该尽可能地避免只有一台服务器、一个数据库、一套软件节点等单点故障的存在。单点故障一旦发生，将直接导致网站服务不可用，恢复正常服务所需的时间也比较长，甚至还可能无法恢复。负载均衡集群、双节点热备、分布式处理等都可以用来解决单点故障，比如提供相同业务的Web服务器、MySQL数据库从库，都可以构建负载均衡集群。一旦集群中的一台服务器、一个服务出现故障，自动实时摘除，对用户来说是不可感知的，不会影响到整个网站的访问，可以为运维工程师留下足够的时间去排查和解决故障。 对于重要的MySQL数据库主库，我们习惯于从硬件层和软件层来实现热备，避免单点。越是复杂的设备，发生故障的概率越大。在磁盘没有损坏的情况下，应用程序导致服务器宕机的概率，远高于简单的磁盘阵列宕机的概率。所以，从硬件层解决的话，可以在两台服务器上安装相同的数据库版本、进行相同的配置，用SAS或SCSI线连接一台磁盘阵列，将数据库数据文件存放到盘阵上。正常情况下用服务器A挂载盘阵分区，启动MySQL，绑定虚拟IP；如果服务器A宕机，则用服务器B挂载盘阵分区，启动MySQL，接管虚拟IP。从软件层解决的话，则可以借助DRBD等软件做镜像。（主从同步，负载均衡、高可用、缓存） IDC机房发生故障的概率较小，但如果发生的话，影响面也是最大的。如果所有服务器都托管在一个IDC机房，一旦该机房遭遇长时间流量攻击、断电、断网、地方政策性封网等，通常只能联系IDC去处理，除此之外束手无策，解决时间也比较长。如果成本允许，将网站服务器分布在两个以上的IDC机房，当某个IDC发生故障时，可以临时切换DNS域名解析来优先恢复服务。 虽然程序代码上线前，经过了测试人员的严格测试，但测试环境和生产环境毕竟有差异，所以一些会急剧影响性能、正常服务的Bug往往在程序上线之后，才会被发现，这就要求我们在发现Bug后，能够迅速回滚到上一正常版本。我们在SVN的基础上，开发了Web代码发布系统，会将每个发布版本之间的文件变更记录下来，一键实现程序代码在多台Web服务器上的发布和回滚。 遭遇DDOS分布式拒绝服务攻击，使用防火墙来对付半连接、假IP，还算比较容易。而那种专挑复杂动态应用程序URL进行的分布式CC攻击，来源为真实IP、真实HTTP请求，具有模拟正规浏览器User-Agent、单个IP的每秒请求数不高、有成千上万个攻击源等特征，很难与正常访问区分开，比较难对付。但是，正常通过浏览器访问一个URL，会加载该URL中引入的JavaScript脚本、CSS样式、图片等文件。遇到CC攻击，需要及时分析日志，找出访问量异常上涨的URL，然后用事先写好的shell脚本找出哪些IP的请求只访问了该URL，而不加载该URL引入的文件，对这些IP进行自动封锁。 系统架构设计时，需要事先考虑到高于目前访问量多少倍的突发访问。对于网游站点来说，访问量受广告集中时间段投放、线上活动的影响较大，带宽峰值时间不固定，对于静态内容，可以使用商业CDN，按实际使用量计费。对于动态内容，如果遇到突发访问人数剧增，超过现有服务器处理能力，最简单的临时处理办法就是增加服务器。上架新服务器需要时间，但是，同一个IDC机房内，可以借助其他业务的服务器，在不同端口开启一组新进程，加入到原有负载均衡池中。另外，可以临时关闭一些Web中的次要功能，来减少服务器消耗。 许式伟：您在任务切分上，有什么经验分享？您通过哪些手段保证任务的独立性？张宴：相信很多人都遇到过这种情况：在一个老项目上修改、增加一些新功能所花费的时间，不比重新来做一个包含所有功能的新项目时间用得少。一个需要长期维护的项目，不可避免地会面临老员工的离职、新员工的接手，很多时候，项目代码的可维护性将决定一个项目的生存周期。让一个新员工在规定开发时间的压力下，去面对一个文档不够详细、陌生的、功能复杂的庞大项目，短时间弄明白所有功能逻辑不是一件容易的事。所以，任务需要切分，将一个大的任务切分成一个个小模块之后，各模块之间可以做到代码独立，互不影响，可维护性也大大增强。 关于任务切分，我以本人今年负责的两个重要项目架构设计为例来介绍一下。在第一个项目：金山游戏官网的《用户行为分析系统》中，由于数据挖掘计算需要消耗较高的内存、CPU资源，一台服务器的处理能力不够，而商业的分布式数据仓库价格又太贵，所以，只有从程序应用中下手，进行任务切分。我们先按需要挖掘的数据指标，将整个数据挖掘任务切分成多个数据挖掘插件，每个插件可以在不同的服务器上运行，多个插件可以同时在多台服务器上。多个数据挖掘插件之间，如果用到相同的某项数据，那么，就将该项数据以冗余方式，复制几份提供给需要的插件，从而实现插件之间无交互、无关联，保证了超大数据量下插件的运算速度。 在第二个项目：金山游戏新版运营管理系统中，则将整个任务切分成了PHP Web管理界面、PHP Web API功能接口、C/C++中间件引擎三部分。这是一种分层结构切分，最上层的“PHP Web管理界面”调用“PHP Web API功能接口”，“PHP Web API功能接口”调用运行在游戏服务器端的“C/C++中间件引擎”，“C/C++中间件引擎”与“游戏服务器端进程”通过TCP、UDP二进制协议、信号、命令行等多种方式通信。四者之间相对独立，代码无关联，通过一层层API接口实现交互。“PHP Web管理界面”负责通用界面实现。“PHP Web API功能接口”内部，又按接入的游戏模块、子功能模块进行了更细的切分，各功能模块之间通过内部API交互。“C/C++中间件引擎”大而全，不处理具体指令，但兼容TCP、UDP、HTTP、HTTPS/SSL、信号、命令行等大多数通信方式，负责和各种类型的游戏服务端交互。这是一套完全由API接口驱动的系统架构，一款新游戏接入运营管理系统时，只需在“PHP Web API功能接口”中增加一个模块；一个游戏新管理功能的增加，只需要在“PHP Web API功能接口”中增加一个子模块。通过任务切分，将复杂功能简单化，也将原来接入一款新游戏所需要的几个月时间，缩短为1~2周。 许式伟：您通过哪些手段，来保障产品的质量？您倾向于多久更新一次您的网站？张宴：Web产品质量主要体现在架构、功能、性能、安全、代码唯一性、兼容性等方面。 架构方面，我会先设计一套架构方案，然后让和项目相关的人员、专家组成员参与进来，一起探讨和论证架构的利弊，提出改进意见，保证架构的可行性。所有重要项目的技术方案需要经过专家组的评估。 功能、性能方面，则会由专门的测试人员进行功能测试、压力测试、安全扫描，测试环境分为线下测试环境、线上准测试环境。 在代码唯一性方面，我们开发了一个Web配置信息管理平台及相关PHP扩展，提供给系统工程师，用于配置信息的统一管理。在新项目中，PHP程序配置文件中将不再出现MySQL、Memcached等各类IP和端口信息，统一用Web配置信息管理平台给出的变量代替。从“开发环境→线下测试环境→线上测试环境→线上正式环境”，连接的数据库各不相同，导致PHP开发工程师经常搞混淆或忘了修改，通过Web配置信息管理平台，使得PHP代码中的配置文件，在四个环境中无须作任何修改，保证了代码的一致性，降低了出错率，从而确保了产品质量。 在兼容性方面，我们从操作系统到PHP、MySQL版本，都保持开发环境、测试环境、线上环境的统一，所有的Web服务运行在CentOS Linux系统上。由于大多数PHP程序员习惯于在Windows上编写代码，而我们的程序中调用的一些接口、PHP扩展，只能在Linux下运行。为此，我们开发了一个小工具，可以将多名程序员在各自本机Windows上搭建的nginx虚拟主机、编写的程序文件，映射到一台Linux服务器，用Linux上的php-cgi执行Windows上的PHP代码。这样，PHP程序员修改完本机代码，保存一下，即可调试，多人之间互不影响。自己调试通过后，可以在Windows直接点击鼠标右键，将修改的代码提交到SVN版本库。 Web 2.0时代，讲究网站更新的实时性，动态网站不用说，静态网站的内容发布也要保证实时。我们开发了一款名为Sersync的开源软件（http://code.google.com/p/sersync/），使用Linux 2.6内核的inotify监控Linux文件系统事件，被监听目录下如果有文件发生修改，Sersync将通过内核自动捕获到事件，并将该文件利用rsync同步到CDN源站服务器。Sersync仅仅同步发生增、删、改事件的单个文件或目录，不像rsync镜像同步那样需要比对双方服务器整个目录下数千万的文件，并且支持多线程同步，因此效率非常高。金山游戏官网的CMS内容发布系统，无论网站编辑通过Web还是FTP上传图片、视频、附件，还是系统工程师直接去CMS发布服务器上增加、修改、删除文件，干完这些事情后不用做任何处理，Sersync 会自动将发生增、删、改事件的文件同步到CDN源站服务器，并可以在文件同步完成后，自动调用CDN缓存刷新接口，主动刷新发生修改、删除的文件的访问URL。 许式伟：您在面试时，通常关注应聘者的哪些方面？哪些问题经常会问呢？张宴：第一，需要具备岗位要求的基础技能知识，这方面我不再详述。 第二，注重项目经验与积累，不看重学历与工作年限。做一个项目，犹如打一场战役，身经百战，积累下来的成功经验可以让工作更得心应手，失败经验可以避免走很多弯路。 第三，能够在1~2个以上技术领域精通。所谓术业有专攻，能够在某几项技术领域做到精通的人，相信对于新的技术领域或者从未有过相关经验的新项目，也能够轻松胜任，做到尽善尽美。 第四，关注应聘者的知识广度。如今的项目，已经告别个人英雄时代，讲究团队的协作。知识面越广，尽管在非专攻领域的深度可能不够，但是，知己知彼，可以站在一个更高的角度上看问题，这对于团队协作开发、项目融合的益处是显而易见的。 第五，具备良好的领悟能力、思考能力、设计能力、创新能力。基础技能知识不够可以学习，经验不足可以积累，技术不精通可以钻研，知识面不广可以开拓，但要培养这四项能力，是一件非常困难的事。要打造一支优秀的团队，这四项能力不可缺少，它们的重要度甚至超过以上的四方面要求。 我不会经常去问固定的问题，但所问的问题，几乎都跟以上的这些方面相关。 许式伟：您曾尝试开放自己的程序代码吗？您对中国国内开源社区的现状有何看法？张宴：是否开源自己的程序代码，跟所在公司或部门的性质有着密切的关系。如果是在研发驱动型企业或部门，程序代码是公司生存的命脉，需要与竞争对手拼技术和保持技术领先的优势，因此，很难支持开源事业。反之，如果是在运营驱动型企业或部门，技术是用来提高运营质量、运营水平的工具之一。将纯粹的技术代码或产品，从公司的业务产品中提取出来，进行开源，可以按照开源产品的要求，提高公司内部技术产品的规范化、标准化，还可以引用更广大用户的使用、反馈和意见，解决未发现的潜在Bug，改进代码质量，提升技术水平。对于提高运营质量、运营水平来说，益处多多。我也尝试开源自己的一些代码，例如简单消息队列服务HTTPSQS（http://code.google.com/p/httpsqs/）、MySQL HTTP/REST客户端MySQL-UDF-HTTP（http://code.google.com/p/mysql-udf-http/），同时，也鼓励团队成员尝试开源，例如刚才提到的自动同步软件Sersync。 国内的开源社区在不断壮大，很多知名互联网公司都开源了自己的一些产品，但大多数还只停留在开源产品的使用、技术交流、汉化层面，真正参与到开源产品编码中的人还是较少，很多开源产品最终还是由原作者或原公司团队维护。国内开源社区的道路仍然漫长。]]></content>
      <categories>
        <category>转载</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[软件设计原则]]></title>
    <url>%2F2018%2F07%2F06%2Fjava%2Fjava_3%2F</url>
    <content type="text"><![CDATA[一些软件设计的原则软件设计的原则不单单只是软件开发，可能推广到其他生产活动中。甚至我们生活中遇到问题的思考。 Don’t Repeat Yourself(DRY)DRY是最简单法则。它关注的是我们在两个地方发现一些相似代码后。我们需要把他们共性抽离出来，形成一个唯一的方法。并改变现有地方的代码，以适合的参数调用新的方法。 Keep It Simple,StupidKISS原则在设计上是备受推崇的。在家装，界面，操作设计上。它的哲学是：把一个简单的事情搞复杂是一件简单的事情，但是把一个复杂的事情变简单，是件复杂的事情。 面向接口变成，非面向实现编程注重接口而不是实现，依赖接口而不是实现。这是由于接口的抽象是稳定的，实现则是多样化的。稳定的事物在操作起来更有安全感。 你可能不真正需要它原则只考虑设计必须的功能，避免过度设计。实现目前需要的功能，在以后考虑需要更多功能时候，再进行添加。 如无必要，无增加复杂性。软件开发是一场沟通博弈。 迪米特法则（Law of Demeter)迪米特法则又称之为 &quot;最少知识原则&quot; 它来源于1987年荷兰大学的一个Demeter项目。又被称作&quot;不要和陌生人说话&quot; 如果你想让你的狗跑的话 ，你会对狗狗说还是要对四条腿说？ 如果你去买东西，你是把钱缴费电源，还是把钱包交给店员让他自己拿？ 简言之，在对象调用对象的时候只出现一个&quot;.&quot; dog.run() 而非 dog.getFouLeg().move() 面向对象的S O L I D法则一般来说这是面向对象的五大设计原则。但是，我们可以把这些原则用于所有的软件开发。 Simple Responsiblility Principle(SRP) - 职责单一原则其核心思想是：一个类，只做一件事。并把事做好。它只有一个引起它变化的原因。它可以看做是低耦合高内聚的在面向对象上的引申，将职责定义为引发 变化的原因，提高内聚性来减少引起变化的外因。职责过多，引起变化的原因就变多。一般情况下设计成引起变化的因素只有一类就好。职责与职责之间不 产生依赖。从而降低了耦合度。 正向例子：Unix/Linux 反向例子：Windows Open/Closed Principle（OCP)-开闭原则核心思想是：模块是可以扩展的，不可修改的。也就是说，对扩展是开放的，而对修改是封闭的。 对扩展开放：意味着有新的需求或者变化时，可以对现有代码进行扩展。以适应新的业务需求。 对修改封闭：意味着一旦设计完成，就可以独立完成其工作。而不要对类进行任何修改。 Likov substitution principle(LSP) - 里氏代换原则子类必须能够被替换成他们的基类。 既：子类在任何地方时候都可以被他们的基类替换，代码还能正常工作。不应该在代码里进行if/else对子类的类型进行判断的条件。 LSP是开闭原则的一个重要保证。它也是我们进行类设计的重要思考条件。就像&quot;蜗牛不是牛&quot;，&quot;鲸鱼是鱼&quot;其判断条件方式就是里氏族代换原则来的。 Interface Segregation Principle(ISP) - 接口隔离原则接口隔离是把功能实现在接口中，而不是类中，使用多个专门的接口比使用单一的总接口要好。 例子：电脑有很多使用方式。比如：看电影，聊天，看电影，上网，变成等等。如果把这些都申明在电脑的抽象类里。那么我们的上网本，PC机，服务器 这些都要实现所有的这些接口。就太复杂了。所以，我们把这些功能都隔离开，比如：看电影接口，聊天接口，上网接口。这样不同功能的电脑就可以有 选择地进行继承实现这些接口。 这个原则让我们可以使用&quot;搭积木&quot;的方式进行软件开发。 Java中的Event listener 和Adapter就是用这种原则实现的。 Dependency Inversion Priciple(DIP) - 依赖倒置原则高层不应该依赖低层的实现。而是依赖于高层抽象。 墙面的开关不应该依赖于点灯的开关实现，而是依赖于一个抽象开关标准接口。当我们扩展程序时候，我们的开关同样可以控制其他不同的等，甚至不同的 电器。也就是说点灯和其他电器集成并实现我们的标准开关接口，我们的开关产商可以不需要关于要控制什么样的设备，只需要关心哪个标准开关就行。这 就是依赖倒置原则。 Common Closure Principle(CCP) - 共同封闭原则一个包中所有的类应该对同一种类型的变化关闭。一个变化影响一个包。便影响了包中的所有类。一个简单的说法是:一起修改的类，应该组合在一起（同个包里） 如果有需要改代码，我们希望所有的修改发生在意个包里，而不是分布在很多包里CCP实际上是对包的只能的相似进行聚合。对包的分配有指导作用。]]></content>
      <categories>
        <category>java</category>
        <category>设计</category>
      </categories>
      <tags>
        <tag>设计原则</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr tlog详解]]></title>
    <url>%2F2018%2F07%2F02%2Fsearch%2Fsolr_4%2F</url>
    <content type="text"></content>
      <categories>
        <category>搜索</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>tlog</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux手动修改/etc/shadow和/etc/passwd中的用户密码]]></title>
    <url>%2F2018%2F07%2F02%2Flinux%2Flinux_4%2F</url>
    <content type="text"><![CDATA[前言最近在玩GameShell这小玩意儿，发现没有root权限。于是想办法获取到。发现官方有提供img镜像下载。 img操作1：查看img信息:123456789101112[ 17:15:11-root@hadoop214:img ]#fdisk -lu clockworkos_v0.1.imgDisk clockworkos_v0.1.img: 7948 MB, 7948206080 bytes, 15523840 sectorsUnits = sectors of 1 * 512 = 512 bytesSector size (logical/physical): 512 bytes / 512 bytesI/O size (minimum/optimal): 512 bytes / 512 bytesDisk label type: dosDisk identifier: 0x9d1726e4 Device Boot Start End Blocks Id Systemclockworkos_v0.1.img1 8192 93814 42811+ c W95 FAT32 (LBA)clockworkos_v0.1.img2 94208 15523839 7714816 83 Linux 可以看到linxu文件从 94208 开始 扇区大小512k所以94208*512=482344962：挂载img1mount -o loop,offset=48234496 clockworkos_v0.1.img /home/gameshell/img/clockpi 密码修改1.手动修改/etc/shadow中的用户密码/etc/shadow文件说明： 第一字段：用户名（也被称为登录名），在/etc/shadow中，用户名和/etc/passwd 是相同的，这样就把passwd 和shadow中用的用户记录联系在一起；这个字段是非空的； 第二字段：密码（已被加密），这个字段是非空的； 第三字段：上次修改口令的时间；这个时间是从1970年01月01日算起到最近一次修改口令的时间间隔（天数），您可以通过passwd 来修改用户的密码，然后查看/etc/shadow中此字段的变化； 第四字段：两次修改口令间隔最少的天数；如果这个字段的值为空，帐号永久可用； 第五字段：两次修改口令间隔最多的天数；如果这个字段的值为空，帐号永久可用； 第六字段：提前多少天警告用户口令将过期；如果这个字段的值为空，帐号永久可用； 第七字段：在口令过期之后多少天禁用此用户；如果这个字段的值为空，帐号永久可用； 第八字段：用户过期日期；此字段指定了用户作废的天数（从1970年的1月1日开始的天数），如果这个字段的值为空，帐号永久可用； 第九字段：保留字段，目前为空，以备将来发展之用； /etc/shadow中格式如下 #testaccount:$1$acQMceF9$1SaCpG2qiKKA3eGolU4Fp0:13402:0:99999:7:::彩色段为加密后的密码，$1$表示采用的是md5加密，绿色段是简单的字符串，蓝色段为加密后的密码 只要删除 $1$acQMceF9$1SaCpG2qiKKA3eGolU4Fp0 它后，就删除了密码 2.linux忘记登陆密码修改/etc/passwd也可以 很简单的一个技巧，给大家介绍一下在这个界面 按任意键按 e键(编辑命令之前启动)选择第二项 在按e键(修改选定的命令在启动)输入single (注意空格)进入单用户模式选择b 启动输入 vi /etc/passwdroot:x:0:0:root:/root:/bin/bash光标移至x下面按delete 键 删除它输入：x！输入reboot重启重启后你会发现 没让你输密码，破译成功 原理解释：在 /etc/passwd 的文件里 保存着用户的信息文件 root:x:0:0:Administrator:/root:/bin/bash 在这一句中 root就是用户名， x是密码标志，只是说明密码的存放位置，具体呢是放在/etc/passwd的 至于密码别想了 全 是密文保存，看不懂的 0 用户id号 0 组id号 administrator估计是我在装系统的时候，原来的名字没改（虚拟机默认用户名），是用户说 明的意思， root 用户的家目录 /bin/bash 记录着用户登陆后所拥有的权限，即所拥有的shell 那么我们把密码标示删掉之后，自然就不会有问你密码的对话框了 当然 GRUB这个引导装载程序是可以设密码的 不过 设密码 我们可以光启 ，用安装光盘进入安全模式，在把密码清除 光启 是可以设BIOS密码的， BIOS 密码是可以拆机箱 扣电池的 那这么说来 传说中的linux岂不是很不安全，通常所说的安全是基于网络的 ，意思是连接互联网后，对方通过网络途径入侵你的linux计算机是很困难的，这种破译的方法只是以防万一，在万一你忘记密码的时候使用的，至于物理安全，那就看你怎么保护了. 本文转载于：http://blog.chinaunix.net/uid-15797451-id-3041560.html]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[solr 跨数据中心复制 CDCR升级过程]]></title>
    <url>%2F2018%2F06%2F14%2Fsearch%2Fsolr_3%2F</url>
    <content type="text"><![CDATA[前言 公司使用solr作为底层搜索引擎已经运行好多年了。从solr的 4.*版本到现在的 6.4.1经历了若干个大版本的修改和升级。稳定性一直在提高。 SOLR CDCR简介跨数据中心复制。是solr从6.0开始的新功能。其目标是实现两个数据集群间的备份。通过合理的二次开发，可以实现异地容灾的功能。 原有架构升级后架构升级过程注意事项总结]]></content>
      <categories>
        <category>搜索</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>cdcr</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[秒杀系统架构优化思路]]></title>
    <url>%2F2018%2F05%2F17%2Fhttp%2Fhttp_2%2F</url>
    <content type="text"><![CDATA[本文曾在“架构师之路”上发布过，近期支援Qcon-AS大会，在微信群里分享了该话题，故对原文进行重新整理与发布。 一、秒杀业务为什么难做1）im系统，例如qq或者微博，每个人都读自己的数据（好友列表、群列表、个人信息）； 2）微博系统，每个人读你关注的人的数据，一个人读多个人的数据； 3）秒杀系统，库存只有一份，所有人会在集中的时间读和写这些数据，多个人读一个数据。 例如：小米手机每周二的秒杀，可能手机只有1万部，但瞬时进入的流量可能是几百几千万。 又例如：12306抢票，票是有限的，库存一份，瞬时流量非常多，都读相同的库存。读写冲突，锁非常严重，这是秒杀业务难的地方。那我们怎么优化秒杀业务的架构呢？ 二、优化方向优化方向有两个（今天就讲这两个点）： （1）将请求尽量拦截在系统上游（不要让锁冲突落到数据库上去）。传统秒杀系统之所以挂，请求都压倒了后端数据层，数据读写锁冲突严重，并发高响应慢，几乎所有请求都超时，流量虽大，下单成功的有效流量甚小。以12306为例，一趟火车其实只有2000张票，200w个人来买，基本没有人能买成功，请求有效率为0。 （2）充分利用缓存，秒杀买票，这是一个典型的读多些少的应用场景，大部分请求是车次查询，票查询，下单和支付才是写请求。一趟火车其实只有2000张票，200w个人来买，最多2000个人下单成功，其他人都是查询库存，写比例只有0.1%，读比例占99.9%，非常适合使用缓存来优化。好，后续讲讲怎么个“将请求尽量拦截在系统上游”法，以及怎么个“缓存”法，讲讲细节。 三、常见秒杀架构常见的站点架构基本是这样的（绝对不画忽悠类的架构图） （1）浏览器端，最上层，会执行到一些JS代码 （2）站点层，这一层会访问后端数据，拼html页面返回给浏览器 （3）服务层，向上游屏蔽底层数据细节，提供数据访问 （4）数据层，最终的库存是存在这里的，mysql是一个典型（当然还有会缓存） 这个图虽然简单，但能形象的说明大流量高并发的秒杀业务架构，大家要记得这一张图。 后面细细解析各个层级怎么优化。 四、各层次优化细节第一层，客户端怎么优化（浏览器层，APP层）问大家一个问题，大家都玩过微信的摇一摇抢红包对吧，每次摇一摇，就会往后端发送请求么？回顾我们下单抢票的场景，点击了“查询”按钮之后，系统那个卡呀，进度条涨的慢呀，作为用户，我会不自觉的再去点击“查询”，对么？继续点，继续点，点点点。。。有用么？平白无故的增加了系统负载，一个用户点5次，80%的请求是这么多出来的，怎么整？ （a）产品层面，用户点击“查询”或者“购票”后，按钮置灰，禁止用户重复提交请求； （b）JS层面，限制用户在x秒之内只能提交一次请求； APP层面，可以做类似的事情，虽然你疯狂的在摇微信，其实x秒才向后端发起一次请求。这就是所谓的“将请求尽量拦截在系统上游”，越上游越好，浏览器层，APP层就给拦住，这样就能挡住80%+的请求，这种办法只能拦住普通用户（但99%的用户是普通用户）对于群内的高端程序员是拦不住的。firebug一抓包，http长啥样都知道，js是万万拦不住程序员写for循环，调用http接口的，这部分请求怎么处理？ 第二层，站点层面的请求拦截怎么拦截？怎么防止程序员写for循环调用，有去重依据么？ip？cookie-id？…想复杂了，这类业务都需要登录，用uid即可。在站点层面，对uid进行请求计数和去重，甚至不需要统一存储计数，直接站点层内存存储（这样计数会不准，但最简单）。一个uid，5秒只准透过1个请求，这样又能拦住99%的for循环请求。 5s只透过一个请求，其余的请求怎么办？缓存，页面缓存，同一个uid，限制访问频度，做页面缓存，x秒内到达站点层的请求，均返回同一页面。同一个item的查询，例如车次，做页面缓存，x秒内到达站点层的请求，均返回同一页面。如此限流，既能保证用户有良好的用户体验（没有返回404）又能保证系统的健壮性（利用页面缓存，把请求拦截在站点层了）。 页面缓存不一定要保证所有站点返回一致的页面，直接放在每个站点的内存也是可以的。优点是简单，坏处是http请求落到不同的站点，返回的车票数据可能不一样，这是站点层的请求拦截与缓存优化。 好，这个方式拦住了写for循环发http请求的程序员，有些高端程序员（黑客）控制了10w个肉鸡，手里有10w个uid，同时发请求（先不考虑实名制的问题，小米抢手机不需要实名制），这下怎么办，站点层按照uid限流拦不住了。 第三层 服务层来拦截（反正就是不要让请求落到数据库上去）服务层怎么拦截？大哥，我是服务层，我清楚的知道小米只有1万部手机，我清楚的知道一列火车只有2000张车票，我透10w个请求去数据库有什么意义呢？没错，请求队列！ 对于写请求，做请求队列，每次只透有限的写请求去数据层（下订单，支付这样的写业务） 1w部手机，只透1w个下单请求去db 3k张火车票，只透3k个下单请求去db 如果均成功再放下一批，如果库存不够则队列里的写请求全部返回“已售完”。 对于读请求，怎么优化？cache抗，不管是memcached还是redis，单机抗个每秒10w应该都是没什么问题的。如此限流，只有非常少的写请求，和非常少的读缓存mis的请求会透到数据层去，又有99.9%的请求被拦住了。 当然，还有业务规则上的一些优化。回想12306所做的，分时分段售票，原来统一10点卖票，现在8点，8点半，9点，…每隔半个小时放出一批：将流量摊匀。 其次，数据粒度的优化：你去购票，对于余票查询这个业务，票剩了58张，还是26张，你真的关注么，其实我们只关心有票和无票？流量大的时候，做一个粗粒度的“有票”“无票”缓存即可。 第三，一些业务逻辑的异步：例如下单业务与 支付业务的分离。这些优化都是结合 业务 来的，我之前分享过一个观点“一切脱离业务的架构设计都是耍流氓”架构的优化也要针对业务。 好了，最后是数据库层 浏览器拦截了80%，站点层拦截了99.9%并做了页面缓存，服务层又做了写请求队列与数据缓存，每次透到数据库层的请求都是可控的。db基本就没什么压力了，闲庭信步，单机也能扛得住，还是那句话，库存是有限的，小米的产能有限，透这么多请求来数据库没有意义。 全部透到数据库，100w个下单，0个成功，请求有效率0%。透3k个到数据，全部成功，请求有效率100%。 五、总结上文应该描述的非常清楚了，没什么总结了，对于秒杀系统，再次重复下我个人经验的两个架构优化思路： （1）尽量将请求拦截在系统上游（越上游越好）； （2）读多写少的常用多使用缓存（缓存抗读压力）； 浏览器和APP：做限速 站点层：按照uid做限速，做页面缓存 服务层：按照业务做写请求队列控制流量，做数据缓存 数据层：闲庭信步 并且：结合业务做优化 六、Q&amp;A问题1、按你的架构，其实压力最大的反而是站点层，假设真实有效的请求数有1000万，不太可能限制请求连接数吧，那么这部分的压力怎么处理？ 答：每秒钟的并发可能没有1kw，假设有1kw，解决方案2个： （1）站点层是可以通过加机器扩容的，最不济1k台机器来呗。 （2）如果机器不够，抛弃请求，抛弃50%（50%直接返回稍后再试），原则是要保护系统，不能让所有用户都失败。 问题2、“控制了10w个肉鸡，手里有10w个uid，同时发请求” 这个问题怎么解决哈？ 答：上面说了，服务层写请求队列控制 问题3：限制访问频次的缓存，是否也可以用于搜索？例如A用户搜索了“手机”，B用户搜索“手机”，优先使用A搜索后生成的缓存页面？ 答：这个是可以的，这个方法也经常用在“动态”运营活动页，例如短时间推送4kw用户app-push运营活动，做页面缓存。 问题4：如果队列处理失败，如何处理？肉鸡把队列被撑爆了怎么办？ 答：处理失败返回下单失败，让用户再试。队列成本很低，爆了很难吧。最坏的情况下，缓存了若干请求之后，后续请求都直接返回“无票”（队列里已经有100w请求了，都等着，再接受请求也没有意义了） 问题5：站点层过滤的话，是把uid请求数单独保存到各个站点的内存中么？如果是这样的话，怎么处理多台服务器集群经过负载均衡器将相同用户的响应分布到不同服务器的情况呢？还是说将站点层的过滤放到负载均衡前？ 答：可以放在内存，这样的话看似一台服务器限制了5s一个请求，全局来说（假设有10台机器），其实是限制了5s 10个请求，解决办法： 1）加大限制（这是建议的方案，最简单） 2）在nginx层做7层均衡，让一个uid的请求尽量落到同一个机器上 问题6：服务层过滤的话，队列是服务层统一的一个队列？还是每个提供服务的服务器各一个队列？如果是统一的一个队列的话，需不需要在各个服务器提交的请求入队列前进行锁控制？ 答：可以不用统一一个队列，这样的话每个服务透过更少量的请求（总票数/服务个数），这样简单。统一一个队列又复杂了。 问题7：秒杀之后的支付完成，以及未支付取消占位，如何对剩余库存做及时的控制更新？ 答：数据库里一个状态，未支付。如果超过时间，例如45分钟，库存会重新会恢复（大家熟知的“回仓”），给我们抢票的启示是，开动秒杀后，45分钟之后再试试看，说不定又有票哟~ 问题8：不同的用户浏览同一个商品 落在不同的缓存实例显示的库存完全不一样 请问老师怎么做缓存数据一致或者是允许脏读？ 答：目前的架构设计，请求落到不同的站点上，数据可能不一致（页面缓存不一样），这个业务场景能接受。但数据库层面真实数据是没问题的。 问题9：就算处于业务把优化考虑“3k张火车票，只透3k个下单请求去db”那这3K个订单就不会发生拥堵了吗？ 答：（1）数据库抗3k个写请求还是ok的；（2）可以数据拆分；（3）如果3k扛不住，服务层可以控制透过去的并发数量，根据压测情况来吧，3k只是举例； 问题10；如果在站点层或者服务层处理后台失败的话，需不需要考虑对这批处理失败的请求做重放？还是就直接丢弃？ 答：别重放了，返回用户查询失败或者下单失败吧，架构设计原则之一是“fail fast”。 问题11.对于大型系统的秒杀，比如12306，同时进行的秒杀活动很多，如何分流？ 答：垂直拆分 问题12、额外又想到一个问题。这套流程做成同步还是异步的？如果是同步的话，应该还存在会有响应反馈慢的情况。但如果是异步的话，如何控制能够将响应结果返回正确的请求方？ 答：用户层面肯定是同步的（用户的http请求是夯住的），服务层面可以同步可以异步。 问题13、秒杀群提问：减库存是在那个阶段减呢？如果是下单锁库存的话，大量恶意用户下单锁库存而不支付如何处理呢？ 答：数据库层面写请求量很低，还好，下单不支付，等时间过完再“回仓”，之前提过了。]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>秒杀，高并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网站QPS,TPS 预估方法]]></title>
    <url>%2F2018%2F05%2F17%2Fhttp%2Fhttp_tps%2F</url>
    <content type="text"><![CDATA[QPS/TPS是每秒响应的查询数量或处理的事务数量一、TPS：Transactions Per Second（每秒传输的事物处理个数），即服务器每秒处理的事务数。TPS包括一条消息入和一条消息出，加上一次用户数据库访问。（业务TPS = CAPS × 每个呼叫平均TPS） TPS是软件测试结果的测量单位。一个事务是指一个客户机向服务器发送请求然后服务器做出反应的过程。客户机在发送请求时开始计时，收到服务器响应后结束计时，以此来计算使用的时间和完成的事务个数。 一般的，评价系统性能均以每秒钟完成的技术交易的数量来衡量。系统整体处理能力取决于处理能力较低模块的TPS值。 二、QPS：每秒查询率QPS是对一个特定的查询服务器在规定时间内所处理流量多少的衡量标准，在因特网上，作为域名系统服务器的机器的性能经常用每秒查询率来衡量。 对应fetches/sec，即每秒的响应请求数，也即是较大吞吐能力 ====================================================================================================== QPS = req/sec = 请求数/秒 【QPS计算PV和机器的方式】 QPS统计方式 [一般使用 http_load 进行统计]QPS = 总请求数 / ( 进程总数 * 请求时间 )QPS: 单个进程每秒请求服务器的成功次数 单台服务器每天PV计算公式1：每天总PV = QPS 3600 6公式2：每天总PV = QPS 3600 8 服务器计算服务器数量 = ceil( 每天总PV / 单台服务器每天总PV ) 【峰值QPS和机器计算公式】 原理：每天80%的访问集中在20%的时间里，这20%时间叫做峰值时间公式：( 总PV数 80% ) / ( 每天秒数 20% ) = 峰值时间每秒请求数(QPS)机器：峰值时间每秒QPS / 单台机器的QPS = 需要的机器 问：每天300w PV 的在单台机器上，这台机器需要多少QPS？答：( 3000000 0.8 ) / (86400 0.2 ) = 139 (QPS) 问：如果一台机器的QPS是58，需要几台机器来支持？答：139 / 58 = 3 PS：下面是性能测试的主要概念和计算公式，记录下：一．系统吞度量要素： 一个系统的吞度量（承压能力）与request对CPU的消耗、外部接口、IO等等紧密关联。单个reqeust 对CPU消耗越高，外部系统接口、IO影响速度越慢，系统吞吐能力越低，反之越高。系统吞吐量几个重要参数：QPS（TPS）、并发数、响应时间 QPS（TPS）：每秒钟request/事务 数量 并发数： 系统同时处理的request/事务数 响应时间： 一般取平均响应时间（很多人经常会把并发数和TPS理解混淆）理解了上面三个要素的意义之后，就能推算出它们之间的关系：QPS（TPS）= 并发数/平均响应时间 或者 并发数 = QPS平均响应时间 一个典型的上班签到系统，早上8点上班，7点半到8点的30分钟的时间里用户会登录签到系统进行签到。公司员工为1000人，平均每个员上登录签到系统的时长为5分钟。可以用下面的方法计算。QPS = 1000/(3060) 事务/秒平均响应时间为 = 560 秒并发数= QPS平均响应时间 = 1000/(3060) (5*60)=166.7 一个系统吞吐量通常由QPS（TPS）、并发数两个因素决定，每套系统这两个值都有一个相对极限值，在应用场景访问压力下，只要某一项达到系统较高值，系统的吞吐量就上不去了，如果压力继续增大，系统的吞吐量反而会下降，原因是系统超负荷工作，上下文切换、内存等等其它消耗导致系统性能下降。决定系统响应时间要素我们做项目要排计划，可以多人同时并发做多项任务，也可以一个人或者多个人串行工作，始终会有一条关键路径，这条路径就是项目的工期。系统一次调用的响应时间跟项目计划一样，也有一条关键路径，这个关键路径是就是系统影响时间；关键路径是有CPU运算、IO、外部系统响应等等组成。二．系统吞吐量评估：我们在做系统设计的时候就需要考虑CPU运算、IO、外部系统响应因素造成的影响以及对系统性能的初步预估。而通常境况下，我们面对需求，我们评估出来的出来QPS、并发数之外，还有另外一个维度：日PV。通过观察系统的访问日志发现，在用户量很大的情况下，各个时间周期内的同一时间段的访问流量几乎一样。比如工作日的每天早上。只要能拿到日流量图和QPS我们就可以推算日流量。通常的技术方法： 1. 找出系统的较高TPS和日PV，这两个要素有相对比较稳定的关系（除了放假、季节性因素影响之外） 2. 通过压力测试或者经验预估，得出较高TPS，然后跟进1的关系，计算出系统较高的日吞吐量。B2B中文和淘宝面对的客户群不一样，这两个客户群的网络行为不应用，他们之间的TPS和PV关系比例也不一样。 A)淘宝淘宝流量图： 淘宝的TPS和PV之间的关系通常为 较高TPS：PV大约为 1 : 113600 （相当于按较高TPS访问11个小时，这个是商品详情的场景，不同的应用场景会有一些不同）B) B2B中文站B2B的TPS和PV之间的关系不同的系统不同的应用场景比例变化比较大，粗略估计在1 : 8个小时左右的关系（09年对offerdetail的流量分析数据）。旺铺和offerdetail这两个比例相差很大，可能是因为爬虫暂的比例较高的原因导致。在淘宝环境下，假设我们压力测试出的TPS为100，那么这个系统的日吞吐量=10011*3600=396万这个是在简单（单一url）的情况下，有些页面，一个页面有多个request，系统的实际吞吐量还要小。无论有无思考时间（T_think），测试所得的TPS值和并发虚拟用户数(U_concurrent)、Loadrunner读取的交易响应时间（T_response）之间有以下关系（稳定运行情况下）：TPS=U_concurrent / (T_response+T_think)。并发数、QPS、平均响应时间三者之间关系 上图横坐标是并发用户数。绿线是CPU使用率；紫线是吞吐量，即QPS；蓝线是时延。 开始，系统只有一个用户，CPU工作肯定是不饱合的。一方面该服务器可能有多个cpu，但是只处理单个进程，另一方面，在处理一个进程中，有些阶段可能是IO阶段，这个时候会造成CPU等待，但是有没有其他请 求进程可以被处理）。随着并发用户数的增加，CPU利用率上升，QPS相应也增加（公式为QPS=并发用户数/平均响应时间。）随着并发用户数的增加，平均响应时间也在增加，而且平均响应时间的增加是一个指数增加曲线。而当并发数增加到很大时，每秒钟都会有很多请求需要处理，会造成进程（线程）频繁切换，反正真正用于处理请求的时间变少，每秒能够处 理的请求数反而变少，同时用户的请求等待时间也会变大，甚至超过用户的心理底线。来源：http://www.cnblogs.com/jackei/软件性能测试的基本概念和计算公式一、软件性能的关注点对一个软件做性能测试时需要关注那些性能呢？我们想想在软件设计、部署、使用、维护中一共有哪些角色的参与，然后再考虑这些角色各自关注的性能点是什么，作为一个软件性能测试工程师，我们又该关注什么？首先，开发软件的目的是为了让用户使用，我们先站在用户的角度分析一下，用户需要关注哪些性能。对于用户来说，当点击一个按钮、链接或发出一条指令开始，到系统把结果已用户感知的形式展现出来为止，这个过程所消耗的时间是用户对这个软件性能的直观印象。也就是我们所说的响应时间，当相应时间较小时，用户体验是很好的，当然用户体验的响应时间包括个人主观因素和客观响应时间，在设计软件时，我们就需要考虑到如何更好地结合这两部分达到用户较佳的体验。如：用户在大数据量查询时，我们可以将先提取出来的数据展示给用户，在用户看的过程中继续进行数据检索，这时用户并不知道我们后台在做什么。用户关注的是用户操作的相应时间。其次，我们站在管理员的角度考虑需要关注的性能点。1、 相应时间2、 服务器资源使用情况是否合理3、 应用服务器和数据库资源使用是否合理4、 系统能否实现扩展5、 系统最多支持多少用户访问、系统较大业务处理量是多少6、 系统性能可能存在的瓶颈在哪里7、 更换那些设备可以提高性能8、 系统能否支持7×24小时的业务访问再次，站在开发（设计）人员角度去考虑。1、 架构设计是否合理2、 数据库设计是否合理3、 代码是否存在性能方面的问题4、 系统中是否有不合理的内存使用方式5、 系统中是否存在不合理的线程同步方式6、 系统中是否存在不合理的资源竞争那么站在性能测试工程师的角度，我们要关注什么呢？一句话，我们要关注以上所有的性能点。二、软件性能的几个主要术语1、响应时间：对请求作出响应所需要的时间网络传输时间：N1+N2+N3+N4应用服务器处理时间：A1+A3数据库服务器处理时间：A2响应时间=N1+N2+N3+N4+A1+A3+A22、并发用户数的计算公式系统用户数：系统额定的用户数量，如一个OA系统，可能使用该系统的用户总数是5000个，那么这个数量，就是系统用户数。同时在线用户数：在一定的时间范围内，较大的同时在线用户数量。同时在线用户数=每秒请求数RPS（吞吐量）+并发连接数+平均用户思考时间平均并发用户数的计算：C=nL / T其中C是平均的并发用户数，n是平均每天访问用户数（login session），L是一天内用户从登录到退出的平均时间（login session的平均时间），T是考察时间长度（一天内多长时间有用户使用系统）并发用户数峰值计算：C^约等于C + 3根号C其中C^是并发用户峰值，C是平均并发用户数，该公式遵循泊松分布理论。3、吞吐量的计算公式指单位时间内系统处理用户的请求数从业务角度看，吞吐量可以用：请求数/秒、页面数/秒、人数/天或处理业务数/小时等单位来衡量从网络角度看，吞吐量可以用：字节/秒来衡量对于交互式应用来说，吞吐量指标反映的是服务器承受的压力，他能够说明系统的负载能力以不同方式表达的吞吐量可以说明不同层次的问题，例如，以字节数/秒方式可以表示数要受网络基础设施、服务器架构、应用服务器制约等方面的瓶颈；已请求数/秒的方式表示主要是受应用服务器和应用代码的制约体现出的瓶颈。当没有遇到性能瓶颈的时候，吞吐量与虚拟用户数之间存在一定的联系，可以采用以下公式计算：F=VU R /其中F为吞吐量，VU表示虚拟用户个数，R表示每个虚拟用户发出的请求数，T表示性能测试所用的时间4、性能计数器是描述服务器或操作系统性能的一些数据指标，如使用内存数、进程时间，在性能测试中发挥着“监控和分析”的作用，尤其是在分析统统可扩展性、进行新能瓶颈定位时有着非常关键的作用。资源利用率：指系统各种资源的使用情况，如cpu占用率为68%，内存占用率为55%，一般使用“资源实际使用/总的资源可用量”形成资源利用率。5、思考时间的计算公式Think Time，从业务角度来看，这个时间指用户进行操作时每个请求之间的时间间隔，而在做新能测试时，为了模拟这样的时间间隔，引入了思考时间这个概念，来更加真实的模拟用户的操作。在吞吐量这个公式中F=VU R / T说明吞吐量F是VU数量、每个用户发出的请求数R和时间T的函数，而其中的R又可以用时间T和用户思考时间TS来计算：R = T / TS下面给出一个计算思考时间的一般步骤：A、首先计算出系统的并发用户数C=nL / T F=R×CB、统计出系统平均的吞吐量F=VU R / T R×C = VU R / TC、统计出平均每个用户发出的请求数量R=uC*T/VUD、根据公式计算出思考时间TS=T/R]]></content>
      <categories>
        <category>系统架构</category>
      </categories>
      <tags>
        <tag>高并发</tag>
        <tag>TPS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git使用场景集锦]]></title>
    <url>%2F2018%2F05%2F07%2Ftools%2Ftools_0%2F</url>
    <content type="text"><![CDATA[Git使用安装之后第一步安装 Git 之后，你要做的第一件事情就是去配置你的名字和邮箱，因为每一次提交都需要这些信息： 12git config --global user.name &quot;bukas&quot;git config --global user.email &quot;bukas@gmail.com&quot; 获取Git配置信息，执行以下命令： 1git config --list 创建版本库什么是版本库呢？版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 12mkdir testgit &amp;&amp; cd testgitgit init 瞬间Git就把仓库建好了，细心的读者可以发现当前目录下多了一个.git的目录，默认是隐藏的，用ls -ah命令就可以看见。 1git-init 把文件添加到版本库12touch readme.mdgit add readme.md 然后用命令git commit告诉Git把文件提交到仓库： 1git commit -m &quot;wrote a readme file&quot; 简单解释一下git commit命令，-m后面输入的是本次提交的说明，可以输入任意内容，当然最好是有意义的，这样你就能从历史记录里方便地找到改动记录。 一次可以add多个不同的文件，以空格分隔： 1git add a.txt b.txt c.txt 仓库状态1git status git status命令可以让我们时刻掌握仓库当前的状态。 但如果能看看具体修改了什么内容就更好了： 1git diff readme.md 版本回退每次提交git都会形成以个commit。我们通过git log可以查看到各个提交的历史。1git log git log –pretty=oneline 参数可以简化显示 1git log --pretty=oneline 在 Git中，用HEAD表示当前版本，也就是最新的提交commit id，上一个版本就是HEAD^，上上一个版本就是HEAD^^，当然往上100个版本写100个^比较容易数不过来，所以写成HEAD~100。 现在我们要把当前版本回退到上一个版本，就可以使用git reset命令： 1git reset --hard HEAD^ 然我们用git log再看看现在版本库的状态，最新的那个版本已经看不到了！好比你从21世纪坐时光穿梭机来到了19世纪，想再回去已经回不去了，肿么办？ git-reset 办法其实还是有的，只要上面的命令行窗口还没有被关掉，你就可以顺着往上找啊找啊，假设找到那个commit id是2e70fdf…，于是就可以指定回到未来的某个版本： 1git reset --hard 2e70fdf 版本号没必要写全，前几位就可以了，Git会自动去找。当然也不能只写前一两位，因为Git可能会找到多个版本号，就无法确定是哪一个了。 现在，你回退到了某个版本，关掉了电脑，第二天早上就后悔了，想恢复到新版本怎么办？找不到新版本的commit id怎么办？ Git提供了一个命令git reflog用来记录你的每一次命令： 1git reflog 终于舒了口气，于是你看到的commit id是2e70fdf，现在，你又可以乘坐时光机回到未来了。 工作区和暂存区Git和其他版本控制系统如SVN的一个不同之处就是有暂存区的概念。 工作区就是你在电脑里能看到的目录，比如我的testgit文件夹就是一个工作区。 工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Git的版本库里存了很多东西，其中最重要的就是称为stage（或者叫index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向 master的一个指针叫HEAD。 前面讲了我们把文件往 Git 版本库里添加的时候，是分两步执行的： 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区； 第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。 因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以现在git commit就是往master分支上提交更改。 你可以简单理解为，git add命令实际上就是把要提交的所有修改放到暂存区（Stage），然后执行git commit就可以一次性把暂存区的所有修改提交到分支。 一旦提交后，如果你又没有对工作区做任何修改，那么工作区就是“干净”的。 修改与撤销用git diff HEAD – readme.md命令可以查看工作区和版本库里面最新版本的区别。 git checkout – file可以丢弃工作区的修改： 1git checkout -- readme.md 命令git checkout – readme.md意思就是，把readme.md文件在工作区的修改全部撤销，即让这个文件回到最近一次git commit或git add时的状态。 当然也可以用git reset命令。 删除文件一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用rm命令删了： 1rm readme.md 这个时候，Git 知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了。 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： 12git rm readme.mdgit commit -m &quot;remove readme.md&quot; 现在，文件就从版本库中被删除了。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1git checkout -- readme.md 生成SSH key创建 SSH Key。在用户主目录下，看看有没有.ssh目录，如果有，再看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。如果没有，打开 Shell（Windows下打开Git Bash），创建SSH Key： 1ssh-keygen -t rsa -C &quot;youremail@example.com&quot; 你需要把邮件地址换成你自己的邮件地址，然后一路回车，使用默认值即可。 如果一切顺利的话，可以在用户主目录里找到.ssh目录，里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 然后登录GitHub（或者其它Git代码托管平台），打开Account settings，SSH Keys页面，点Add SSH Key，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容。 为什么GitHub需要SSH Key呢？因为GitHub需要识别出你推送的提交确实是你推送的，而不是别人冒充的，而Git支持SSH协议，所以GitHub只要知道了你的公钥，就可以确认只有你自己才能推送。 当然，GitHub允许你添加多个Key。假定你有若干电脑，你一会儿在公司提交，一会儿在家里提交，只要把每台电脑的Key都添加到GitHub，就可以在每台电脑上往GitHub推送了。 远程服务器Git 最强大的功能之一是可以有一个以上的远程服务器（另一个事实，你总是可以运行一个本地仓库）。你不一定总是需要写访问权限，你可以从多个服务器中读取（用于合并），然后写到另一个服务器中。添加一个远程服务器很简单： git remote add origin(别名，根据爱好命名) git@github.com:bukas/bukas.git如果你想查看远程服务器的相关信息，你可以这样做： 12345# shows URLs of each remote servergit remote -v # gives more details about origingit remote show origin(别名) 下一步，就可以把本地库的所有内容推送到远程库上： 1git push -u origin master 把本地库的内容推送到远程，用git push命令，实际上是把当前分支master推送到远程。 由于远程库是空的，我们第一次推送master分支时，加上了-u参数，Git不但会把本地的master分支内容推送的远程新的master分支，还会把本地的master分支和远程的master分支关联起来，在以后的推送或者拉取时就可以简化命令。 从现在起，只要本地作了提交，就可以通过命令把本地master分支的最新修改推送至GitHub： 1git push origin master SSH警告当你第一次使用Git的clone或者push命令连接GitHub时，会得到一个警告： 12345The authenticity of host ‘github.com (xx.xx.xx.xx)’ can’t be established.RSA key fingerprint is xx.xx.xx.xx.xx.Are you sure you want to continue connecting (yes/no)? 这是因为Git使用SSH连接，而SSH连接在第一次验证GitHub服务器的Key时，需要你确认 GitHub的Key的指纹信息是否真的来自GitHub的服务器，输入yes回车即可。 从远程库克隆 当已经有一个远程库的时候，我们可以用命令git clone克隆一个本地库： 1git clone git@github.com:test/testgit.git 你也许还注意到，GitHub给出的地址不止一个，还可以用https://github.com/test/testgit.git这样的地址。实际上Git支持多种协议，默认的git://使用ssh，但也可以使用 https等其他协议。使用https除了速度慢以外，还有个最大的麻烦是每次推送都必须输入口令，但是在某些只开放http端口的公司内部就无法使用ssh协议而只能用https。 创建与合并分支首先我们创建dev分支，然后切换到dev分支： 1git checkout -b dev git checkout命令加上-b参数表示创建并切换，相当于以下两条命令： 12git branch devgit checkout dev 然后用git branch命令查看当前分支： 1git branch 我们在dev分支上进行添加修改操作，然后我们把dev分支的工作成果合并到master分支上： 12git checkout mastergit merge dev git merge命令用于合并指定分支到当前分支。 注意到git merge的信息里面可能有Fast-forward字样，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。 当然也不是每次合并都能Fast-forward。 合并完成后，就可以放心地删除dev分支了： 1git branch -d dev 如果要丢弃一个没有被合并过的分支，可以通过git branch -D 强行删除。 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致； 建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name； 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 解决冲突人生不如意之事十之八九，合并分支往往也不是一帆风顺的。 有时候我们进行合并的时候，会提示有冲突出现CONFLICT (content)，必须手动解决冲突后再提交。git status也可以告诉我们冲突的文件。 打开冲突文件我们会看到Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改后提交： 12git add readme.mdgit commit -m &quot;conflict fixed&quot; 用带参数的git log也可以看到分支的合并情况： 1git log --graph --pretty=oneline --abbrev-commit 分支管理策略通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。 如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。 下面我们实战一下–no-ff方式的git merge： 首先，仍然创建并切换dev分支： 1git checkout -b dev 修改readme.md文件，并提交一个新的commit： 12git add readme.mdgit commit -m &quot;add merge&quot; 现在，我们切换回master： 1git checkout master 准备合并dev分支，请注意–no-ff参数，表示禁用Fast forward： 1git merge --no-ff -m &quot;merge with no-ff&quot; dev Bug分支软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，等等，当前正在dev上进行的工作还没有提交。 并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？ 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作： 1git stash 现在，用git status查看工作区，就是干净的（除非有没有被 Git 管理的文件），因此可以放心地创建分支来修复bug。 首先确定要在哪个分支上修复bug，假定需要在master分支上修复，就从master创建临时分支： 12git checkout mastergit checkout -b issue-101 现在修复bug，然后提交： 12git add readme.mdgit commit -m &quot;fix bug 101&quot; 修复完成后，切换到master分支，并完成合并，最后删除issue-101分支： 123git checkout mastergit merge --no-ff -m &quot;merged bug fix 101&quot; issue-101 太棒了，原计划两个小时的bug修复只花了5分钟！现在，是时候接着回到dev分支干活了！ 12git checkout devgit status 工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看： 1git stash list 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法： 一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； 另一种方式是用git stash pop，恢复的同时把stash内容也删了： 1git stash pop 再用git stash list查看，就看不到任何stash内容了。 你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令 1git stash apply stash@&#123;0&#125; 标签管理发布一个版本时，我们通常先在版本库中打一个标签，这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 命令git tag 用于新建一个标签，默认为HEAD，也可以指定一个commit id。 1git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息。 还可以通过-s用私钥签名一个标签： 1git tag -s v0.5 -m &quot;signed version 0.2 released&quot; fec145a git tag可以查看所有标签。 用命令git show 可以查看某个标签的详细信息。 如果标签打错了，也可以删除： 1git tag -d v0.1 因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除。 如果要推送某个标签到远程，使用命令git push origin ： 1git push origin v1.0 或者，一次性推送全部尚未推送到远程的本地标签：1git push origin --tags 如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除： 1git tag -d v0.9 然后，从远程删除。删除命令也是push，但是格式如下： 1git push origin :refs/tags/v0.9 ####忽略特殊文件 在安装Git一节中，我们已经配置了user.name 和user.email，实际上，Git还有很多可配置项。 比如，让Git显示颜色，会让命令输出看起来更醒目： 1git config --global color.ui true 有些时候，你必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件啦，等等，每次git status都会显示Untracked files…，有强迫症的童鞋心里肯定不爽。 好在Git考虑到了大家的感受，这个问题解决起来也很简单，在 Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。 不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：https://github.com/github/gitignore 当然也可以配置全局忽略的文件，这样就不用每个项目都加gitignore了： 1git config --global core.excludesfile &apos;~/.gitignore&apos; 配置别名有没有经常敲错命令？比如git status？status这个单词真心不好记。 如果敲git st就表示git status那就简单多了，当然这种偷懒的办法我们是极力赞成的。 我们只需要敲一行命令，告诉Git，以后st就表示status：1git config --global alias.st status 当然还有别的命令可以简写： 123git config --global alias.co checkoutgit config --global alias.ci commitgit config --global alias.br branch –global参数是全局参数，也就是这些命令在这台电脑的所有Git仓库下都有用。 在撤销修改一节中，我们知道，命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区。既然是一个unstage操作，就可以配置一个unstage别名： 1git config --global alias.unstage &apos;reset HEAD&apos; 配置一个git last，让其显示最后一次提交信息： 1git config --global alias.last &apos;log -1&apos; 甚至还有人把lg配置成了： 1git config --global alias.lg &quot;log --color --graph --pretty=format:&apos;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset&apos; --abbrev-commit&quot; ####配置文件 配置Git的时候，加上–global是针对当前用户起作用的，如果不加，那只针对当前的仓库起作用。 配置文件放哪了？每个仓库的Git配置文件都放在.git/config文件中。 而当前用户的Git配置文件放在用户主目录下的一个隐藏文件.gitconfig中。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[swagger2 注解说明]]></title>
    <url>%2F2018%2F04%2F04%2Fspring%2Fswagger%2Fswagger_1%2F</url>
    <content type="text"><![CDATA[整体说明 swagger2的在线文档功能异常强大。 1234567891011121314151617181920212223242526272829303132@Api：用在请求的类上，表示对类的说明 tags=&quot;说明该类的作用，可以在UI界面上看到的注解&quot; value=&quot;该参数没什么意义，在UI界面上也看到，所以不需要配置&quot;@ApiOperation：用在请求的方法上，说明方法的用途、作用 value=&quot;说明方法的用途、作用&quot; notes=&quot;方法的备注说明&quot;@ApiImplicitParams：用在请求的方法上，表示一组参数说明 @ApiImplicitParam：用在@ApiImplicitParams注解中，指定一个请求参数的各个方面 name：参数名 value：参数的汉字说明、解释 required：参数是否必须传 paramType：参数放在哪个地方 · header --&gt; 请求参数的获取：@RequestHeader · query --&gt; 请求参数的获取：@RequestParam · path（用于restful接口）--&gt; 请求参数的获取：@PathVariable · body（不常用） · form（不常用） dataType：参数类型，默认String，其它值dataType=&quot;Integer&quot; defaultValue：参数的默认值@ApiResponses：用在请求的方法上，表示一组响应 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 code：数字，例如400 message：信息，例如&quot;请求参数没填好&quot; response：抛出异常的类@ApiModel：用于响应类上，表示一个返回响应数据的信息 （这种一般用在post创建的时候，使用@RequestBody这样的场景， 请求参数无法使用@ApiImplicitParam注解进行描述的时候） @ApiModelProperty：用在属性上，描述响应类的属性 详细说明 @Api：用在请求的类上，说明该类的作用 123@Api：用在请求的类上，说明该类的作用 tags=&quot;说明该类的作用&quot; value=&quot;该参数没什么意义，所以不需要配置&quot; @ApiOperation：用在请求的方法上，说明方法的作用 123@ApiOperation：&quot;用在请求的方法上，说明方法的作用&quot; value=&quot;说明方法的作用&quot; notes=&quot;方法的备注说明 示例：1@ApiOperation(value=&quot;用户注册&quot;,notes=&quot;手机号、密码都是必输项，年龄随边填，但必须是数字&quot;) @ApiImplicitParams：用在请求的方法上，包含一组参数说明 12345678910111213@ApiImplicitParams：用在请求的方法上，包含一组参数说明 @ApiImplicitParam：用在 @ApiImplicitParams 注解中，指定一个请求参数的配置信息 name：参数名 value：参数的汉字说明、解释 required：参数是否必须传 paramType：参数放在哪个地方 · header --&gt; 请求参数的获取：@RequestHeader · query --&gt; 请求参数的获取：@RequestParam · path（用于restful接口）--&gt; 请求参数的获取：@PathVariable · body（不常用） · form（不常用） dataType：参数类型，默认String，其它值dataType=&quot;Integer&quot; defaultValue：参数的默认值 示列： 12345@ApiImplicitParams(&#123; @ApiImplicitParam(name=&quot;mobile&quot;,value=&quot;手机号&quot;,required=true,paramType=&quot;form&quot;), @ApiImplicitParam(name=&quot;password&quot;,value=&quot;密码&quot;,required=true,paramType=&quot;form&quot;), @ApiImplicitParam(name=&quot;age&quot;,value=&quot;年龄&quot;,required=true,paramType=&quot;form&quot;,dataType=&quot;Integer&quot;)&#125;) @ApiResponses：用于请求的方法上，表示一组响应 12345@ApiResponses：用于请求的方法上，表示一组响应 @ApiResponse：用在@ApiResponses中，一般用于表达一个错误的响应信息 code：数字，例如400 message：信息，例如&quot;请求参数没填好&quot; response：抛出异常的类 示例： 12345@ApiOperation(value = &quot;select1请求&quot;,notes = &quot;多个参数，多种的查询参数类型&quot;)@ApiResponses(&#123; @ApiResponse(code=400,message=&quot;请求参数没填好&quot;), @ApiResponse(code=404,message=&quot;请求路径没有或页面跳转路径不对&quot;)&#125;) @ApiModel：用于响应类上，表示一个返回响应数据的信息 1234@ApiModel：用于响应类上，表示一个返回响应数据的信息 （这种一般用在post创建的时候，使用@RequestBody这样的场景， 请求参数无法使用@ApiImplicitParam注解进行描述的时候） @ApiModelProperty：用在属性上，描述响应类的属性 示例: 12345678910111213141516171819import io.swagger.annotations.ApiModel;import io.swagger.annotations.ApiModelProperty;import java.io.Serializable;@ApiModel(description= "返回响应数据")public class RestMessage implements Serializable&#123; @ApiModelProperty(value = "是否成功") private boolean success=true; @ApiModelProperty(value = "返回对象") private Object data; @ApiModelProperty(value = "错误编号") private Integer errCode; @ApiModelProperty(value = "错误信息") private String message; /* getter/setter */&#125;]]></content>
      <categories>
        <category>spring</category>
        <category>swagger</category>
      </categories>
      <tags>
        <tag>swagger</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Socket,WebSocket,HTML5]]></title>
    <url>%2F2018%2F02%2F06%2Fhttp%2Fhexo%2F</url>
    <content type="text"><![CDATA[背景对Web项目来讲，一般都用http请求来解决。 问题 Socket 和 WebSocket 有哪些区别和联系？ WebSocket 和 HTML5 是什么关系？ 必须在浏览器中才能使用 WebSocket 吗？ WebSocket 能和 Socket 一样传输 raw 数据么？ WebSocket 和 Socket 相比会多耗费流量么？ 概述选择了 WebSocket 技术之后，不可避免的，我要将它和其他协议以及技术做一下比较。最常见的，就是需要比较 WebSocket 与 HTTP、Socket 技术的异同。 WebSocket 是为了满足基于 Web 的日益增长的实时通信需求而产生的。在传统的 Web 中，要实现实时通信，通用的方式是采用 HTTP 协议不断发送请求。但这种方式即浪费带宽（HTTP HEAD 是比较大的），又消耗服务器 CPU 占用（没有信息也要接受请求）。（下图来自 WebSocket.org） Latency comparison between the polling and WebSocket applications 而是用 WebSocket 技术，则会大幅降低上面提到的消耗：（下图来自websocket.org） Comparison of the unnecessary network throughput overhead between the polling and the WebSocket applications 关于更详细的描述，尹立的这篇文章讲得非常好：WebSocket（2）–为什么引入WebSocket协议 。 那么，WebSocket 到底与 HTTP 协议到底是一个什么样的关系呢？它和 Socket 又有什么联系？这就要讲到 OSI 模型和 TCP/IP 协议族。 OSI 模型与 TCP/IP以下是 维基百科 中关于OSI 模型的说明： 开放式系统互联通信参考模型（英语：Open System Interconnection Reference Model，ISO/IEC 7498-1），简称为OSI模型（OSI model），一种概念模型，由国际标准化组织（ISO）提出，一个试图使各种计算机在世界范围内互连为网络的标准框架。 而 TCP/IP 协议可以看做是对 OSI 模型的一种简化（以下内容来自 维基百科）： 它将软件通信过程抽象化为四个抽象层，采取协议堆叠的方式，分别实作出不同通信协议。协议套组下的各种协议，依其功能不同，被分别归属到这四个阶层之中7，常被视为是简化的七层OSI模型。 这里有一张图详细介绍了 TCP/IP 协议族中的各个协议在 OSI模型 中的分布，一图胜千言（下图来自 科来）： TCP/IP 和 OSI 模型 TCP/IP 协议和 OSI 模型的内容，在互联网上有很多。我没有必要再次介绍它们。在这里，我们只需要知道，HTTP、WebSocket 等协议都是处于 OSI 模型的最高层： 应用层 。而 IP 协议工作在网络层（第3层），TCP 协议工作在传输层（第4层）。 至于 OSI 模型的各个层次都有什么系统和它们对应，这里有篇很好的文章可以满足大家的求知欲：OSI七层模型详解 。 WebSocket、HTTP 与 TCP从上面的图中可以看出，HTTP、WebSocket 等应用层协议，都是基于 TCP 协议来传输数据的。我们可以把这些高级协议理解成对 TCP 的封装。 既然大家都使用 TCP 协议，那么大家的连接和断开，都要遵循 TCP 协议中的三次握手和四次握手 ，只是在连接之后发送的内容不同，或者是断开的时间不同。 更详细内容可阅读：wireshark抓包图解 TCP三次握手/四次挥手详解 对于 WebSocket 来说，它必须依赖 HTTP 协议进行一次握手 ，握手成功后，数据就直接从 TCP 通道传输，与 HTTP 无关了。 Socket 与 WebScoketSocket 其实并不是一个协议。它工作在 OSI 模型会话层（第5层），是为了方便大家直接使用更底层协议（一般是 TCP 或 UDP ）而存在的一个抽象层。 最早的一套 Socket API 是 Berkeley sockets ，采用 C 语言实现。它是 Socket 的事实标准，POSIX sockets 是基于它构建的，多种编程语言都遵循这套 API，在 JAVA、Python 中都能看到这套 API 的影子。 下面摘录一段更容易理解的文字（来自 http和socket之长连接和短连接区别）： Socket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。 Socket是什么 Socket通信过程 主机 A 的应用程序要能和主机 B 的应用程序通信，必须通过 Socket 建立连接，而建立 Socket 连接必须需要底层 TCP/IP 协议来建立 TCP 连接。建立 TCP 连接需要底层 IP 协议来寻址网络中的主机。我们知道网络层使用的 IP 协议可以帮助我们根据 IP 地址来找到目标主机，但是一台主机上可能运行着多个应用程序，如何才能与指定的应用程序通信就要通过 TCP 或 UPD 的地址也就是端口号来指定。这样就可以通过一个 Socket 实例唯一代表一个主机上的一个应用程序的通信链路了。 而 WebSocket 则不同，它是一个完整的 应用层协议，包含一套标准的 API 。 所以，从使用上来说，WebSocket 更易用，而 Socket 更灵活。 HTML5 与 WebSocketWebSocket API 是 HTML5 标准的一部分， 但这并不代表 WebSocket 一定要用在 HTML 中，或者只能在基于浏览器的应用程序中使用。 实际上，许多语言、框架和服务器都提供了 WebSocket 支持，例如： 基于 C 的 libwebsocket.org基于 Node.js 的 Socket.io基于 Python 的 ws4py基于 C++ 的 WebSocket++Apache 对 WebSocket 的支持： Apache Module mod_proxy_wstunnelNginx 对 WebSockets 的支持： NGINX as a WebSockets Proxy 、 NGINX Announces Support for WebSocket Protocol 、WebSocket proxyinglighttpd 对 WebSocket 的支持：mod_websocket 原文链接]]></content>
      <categories>
        <category>http</category>
        <category>WebSocket</category>
      </categories>
      <tags>
        <tag>socket</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Twitter Snowflake 有序ID生成算法]]></title>
    <url>%2F2018%2F01%2F31%2Fjava%2Fjava_2%2F</url>
    <content type="text"><![CDATA[概述分布式系统，各种系统平台建设中，需要用到全局唯一的ID场景，可以统一地进行一些简单的统计和排序。这时候我们需要一个统一的ID生成系统来做这个事情。Twitter Snowflake 可以作为一个满足基础需求的原始样本算法。可以以此为蓝本开发自己的业务ID生成算法。 结构snowflake的结构如下(每部分用-分开):0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000第一位为未使用，接下来的41位为毫秒级时间(41位的长度可以使用69年)，然后是5位datacenterId和5位workerId(10位的长度最多支持部署1024个节点） ，最后12位是毫秒内的计数（12位的计数顺序号支持每个节点每毫秒产生4096个ID序号）一共加起来刚好64位，为一个Long型。(转换成字符串后长度最多19)snowflake生成的ID整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞（由datacenter和workerId作区分），并且效率较高。经测试snowflake每秒能够产生26万个ID。 Java实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144/** * Twitter_Snowflake&lt;br&gt; * SnowFlake的结构如下(每部分用-分开):&lt;br&gt; * 0 - 0000000000 0000000000 0000000000 0000000000 0 - 00000 - 00000 - 000000000000 &lt;br&gt; * 1位标识，由于long基本类型在Java中是带符号的，最高位是符号位，正数是0，负数是1，所以id一般是正数，最高位是0&lt;br&gt; * 41位时间截(毫秒级)，注意，41位时间截不是存储当前时间的时间截，而是存储时间截的差值（当前时间截 - 开始时间截) * 得到的值），这里的的开始时间截，一般是我们的id生成器开始使用的时间，由我们程序来指定的（如下下面程序IdWorker类的startTime属性）。41位的时间截，可以使用69年，年T = (1L &lt;&lt; 41) / (1000L * 60 * 60 * 24 * 365) = 69&lt;br&gt; * 10位的数据机器位，可以部署在1024个节点，包括5位datacenterId和5位workerId&lt;br&gt; * 12位序列，毫秒内的计数，12位的计数顺序号支持每个节点每毫秒(同一机器，同一时间截)产生4096个ID序号&lt;br&gt; * 加起来刚好64位，为一个Long型。&lt;br&gt; * SnowFlake的优点是，整体上按照时间自增排序，并且整个分布式系统内不会产生ID碰撞(由数据中心ID和机器ID作区分)，并且效率较高，经测试，SnowFlake每秒能够产生26万ID左右。 */public class SnowflakeIdWorker &#123; // ==============================Fields=========================================== /** 开始时间截 (2015-01-01) */ private final long twepoch = 1420041600000L; /** 机器id所占的位数 */ private final long workerIdBits = 5L; /** 数据标识id所占的位数 */ private final long datacenterIdBits = 5L; /** 支持的最大机器id，结果是31 (这个移位算法可以很快的计算出几位二进制数所能表示的最大十进制数) */ private final long maxWorkerId = -1L ^ (-1L &lt;&lt; workerIdBits); /** 支持的最大数据标识id，结果是31 */ private final long maxDatacenterId = -1L ^ (-1L &lt;&lt; datacenterIdBits); /** 序列在id中占的位数 */ private final long sequenceBits = 12L; /** 机器ID向左移12位 */ private final long workerIdShift = sequenceBits; /** 数据标识id向左移17位(12+5) */ private final long datacenterIdShift = sequenceBits + workerIdBits; /** 时间截向左移22位(5+5+12) */ private final long timestampLeftShift = sequenceBits + workerIdBits + datacenterIdBits; /** 生成序列的掩码，这里为4095 (0b111111111111=0xfff=4095) */ private final long sequenceMask = -1L ^ (-1L &lt;&lt; sequenceBits); /** 工作机器ID(0~31) */ private long workerId; /** 数据中心ID(0~31) */ private long datacenterId; /** 毫秒内序列(0~4095) */ private long sequence = 0L; /** 上次生成ID的时间截 */ private long lastTimestamp = -1L; //==============================Constructors===================================== /** * 构造函数 * @param workerId 工作ID (0~31) * @param datacenterId 数据中心ID (0~31) */ public SnowflakeIdWorker(long workerId, long datacenterId) &#123; if (workerId &gt; maxWorkerId || workerId &lt; 0) &#123; throw new IllegalArgumentException(String.format("worker Id can't be greater than %d or less than 0", maxWorkerId)); &#125; if (datacenterId &gt; maxDatacenterId || datacenterId &lt; 0) &#123; throw new IllegalArgumentException(String.format("datacenter Id can't be greater than %d or less than 0", maxDatacenterId)); &#125; this.workerId = workerId; this.datacenterId = datacenterId; &#125; // ==============================Methods========================================== /** * 获得下一个ID (该方法是线程安全的) * @return SnowflakeId */ public synchronized long nextId() &#123; long timestamp = timeGen(); //如果当前时间小于上一次ID生成的时间戳，说明系统时钟回退过这个时候应当抛出异常 if (timestamp &lt; lastTimestamp) &#123; throw new RuntimeException( String.format("Clock moved backwards. Refusing to generate id for %d milliseconds", lastTimestamp - timestamp)); &#125; //如果是同一时间生成的，则进行毫秒内序列 if (lastTimestamp == timestamp) &#123; sequence = (sequence + 1) &amp; sequenceMask; //毫秒内序列溢出 if (sequence == 0) &#123; //阻塞到下一个毫秒,获得新的时间戳 timestamp = tilNextMillis(lastTimestamp); &#125; &#125; //时间戳改变，毫秒内序列重置 else &#123; sequence = 0L; &#125; //上次生成ID的时间截 lastTimestamp = timestamp; //移位并通过或运算拼到一起组成64位的ID return ((timestamp - twepoch) &lt;&lt; timestampLeftShift) // | (datacenterId &lt;&lt; datacenterIdShift) // | (workerId &lt;&lt; workerIdShift) // | sequence; &#125; /** * 阻塞到下一个毫秒，直到获得新的时间戳 * @param lastTimestamp 上次生成ID的时间截 * @return 当前时间戳 */ protected long tilNextMillis(long lastTimestamp) &#123; long timestamp = timeGen(); while (timestamp &lt;= lastTimestamp) &#123; timestamp = timeGen(); &#125; return timestamp; &#125; /** * 返回以毫秒为单位的当前时间 * @return 当前时间(毫秒) */ protected long timeGen() &#123; return System.currentTimeMillis(); &#125; //==============================Test============================================= /** 测试 */ public static void main(String[] args) &#123; SnowflakeIdWorker idWorker = new SnowflakeIdWorker(0, 0); for (int i = 0; i &lt; 1000; i++) &#123; long id = idWorker.nextId(); System.out.println(Long.toBinaryString(id)); System.out.println(id); &#125; &#125;&#125;]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>ID生成</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[观察者模式]]></title>
    <url>%2F2018%2F01%2F23%2Fjava%2Fpatterndesign%2Fpatterndesign_15%2F</url>
    <content type="text"><![CDATA[概念实现场景总结观察者模式提供了一种对象设计,让主题和观察者之间耦合度降得很低,为什么呢?关于观察者的一切,主题只知道观察者实现了Observer接口,并不需要观察者具体的类是谁,做了什么或者其他细节.这样的话,由于松耦合,改变主题或者观察者其中一方,并不会影响另一方,只要他们之间的接口仍被遵守,就可以自由地改变它.降低对象之间的耦合度,也是面设对象设计的一个很重要的原则.]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>观察者</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[模板方法模式]]></title>
    <url>%2F2018%2F01%2F23%2Fjava%2Fpatterndesign%2Fpatterndesign_14%2F</url>
    <content type="text"><![CDATA[概念 模板方法模式 在意个方法中定义一个算法的骨架，而将这些具体步骤实现延迟到子类中。模板方法使得子类可以在不改变算法结构的情况下，重新定义算法中的某些步骤 实现 AbstraceClass:12 场景总结]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>模板方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[策略模式]]></title>
    <url>%2F2018%2F01%2F23%2Fjava%2Fpatterndesign%2Fpatterndesign_13%2F</url>
    <content type="text"><![CDATA[概念 策略模式定义了算法族，分别封装起来，让他们之间可以相互替换，此模式让算法的变化独立于使用算法的客户。 实现策略模式涉及到三个角色： 环境(Context)角色：持有一个Strategy的引用。 抽象策略(Strategy)角色：这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。 具体策略(ConcreteStrategy)角色：包装了相关的算法或行为。 PriceStrategy:12345678package com.littlehui.design.strategy;/** * Created by littlehui on 2018/2/27. */public interface PriceStrategy &#123; public Double caculaPrice(Double price);&#125; MemberPriceStrategy:12345678910package com.littlehui.design.strategy;/** * Created by littlehui on 2018/2/27. */public class MemberPriceStrategy implements PriceStrategy &#123; public Double caculaPrice(Double price) &#123; return price * 0.9; &#125;&#125; VipPriceStrategy:1234567891011package com.littlehui.design.strategy;/** * Created by littlehui on 2018/2/27. */public class VipPriceStrategy implements PriceStrategy &#123; public Double caculaPrice(Double price) &#123; return price * 0.8; &#125;&#125; Price:1234567891011121314151617package com.littlehui.design.strategy;/** * Created by littlehui on 2018/2/27. */public class Price &#123; private Double price; public Price(Double price) &#123; this.price = price; &#125; public Double cacularPrice(PriceStrategy priceStrategy) &#123; return priceStrategy.caculaPrice(price); &#125;&#125; Client:123456789101112package com.littlehui.design.strategy;/** * Created by littlehui on 2018/2/27. */public class Client &#123; public static void main(String[] args) &#123; Price price = new Price(5D); System.out.println(price.cacularPrice(new VipPriceStrategy())); &#125;&#125; 场景总结策略模式的优点 （1）策略模式提供了管理相关的算法族的办法。策略类的等级结构定义了一个算法或行为族。恰当使用继承可以把公共的代码移到父类里面，从而避免代码重复。 （2）使用策略模式可以避免使用多重条件(if-else)语句。多重条件语句不易维护，它把采取哪一种算法或采取哪一种行为的逻辑与算法或行为的逻辑混合在一起，统统列在一个多重条件语句里面，比使用继承的办法还要原始和落后。 策略模式的缺点 （1）客户端必须知道所有的策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类。换言之，策略模式只适用于客户端知道算法或行为的情况。 （2）由于策略模式把每个具体的策略实现都单独封装成为类，如果备选的策略很多的话，那么对象的数目就会很可观。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>策略模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux免密码登录]]></title>
    <url>%2F2018%2F01%2F17%2Flinux%2Flinux_3%2F</url>
    <content type="text"><![CDATA[前言有两台机器 A,B。现在要实现A访问B免密码登录。 步骤生成秘钥在A主机上执行12345ssh-keygen -t rsa -f rsa_for_174回车回车回车 -t 类型 -f 指定生成秘钥文件名 追加认证 将生成的秘钥拷贝到B主机，可以手动ftp，也可以用命令。1scp -i ~/.ssh/CY6034_rsa_4096 ./rsa_for_174.pub root@10.5.121.144:~/.ssh/ 1ssh -i CY6034_rsa_4096 root@10.5.121.144 追加1cat ~/.ssh/rsa_for_174 &gt;&gt; ~/.ssh/authorized_keys DONE]]></content>
      <categories>
        <category>linux</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Linux服务器性能评估]]></title>
    <url>%2F2017%2F11%2F21%2Flinux%2Flinux_2%2F</url>
    <content type="text"><![CDATA[前言 Web服务在部署到Linux系统运行期间，可能会遇到各种问题。程序上的BUG，数据上的问题，这些排查起来较为简单。当排除这些问题后，往往需要深入到服务器层面来寻找影响程序运行的稳定因素。 基本信息查看CPU信息查看 查看CPU个数 1cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq | wc -l 查看CPU中core个数 1cat /proc/cpuinfo | grep &quot;cpu cores&quot; | wc -l 查看CPU逻辑个数 1cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l 逻辑CPU数 = 物理CPU个数*核心数 内存信息查看 内存使用情况12345#free -m total used free shared buffers cachedMem: 64376 37881 26494 0 308 17273-/+ buffers/cache: 20299 44076Swap: 16383 0 16383 total: 总内存数 used: 已用内存数 free: 空闲内存 shared: 多进程共享的内存总数 - buffers/cache: 已用缓存总数 used-buffer-cached + buffers/cache: 可用缓存数 free+buffer+cached Buffer Cache 用于针对磁盘块的写 Page Cache用于针对文件inode的读写，这些cache能够缩短I/O时间 free / used是系统可用/暂用的内存 对于程序来说 -/+ buffers/cache是可用/占用内存，因为 buffers/cache很容易就会被使用到 硬盘查看 查看硬盘分区信息 1fdisk -l 查看文件系统磁盘暂用情况 1df -h 查看硬盘的I/O性能 1234567iostat -d -k 1Linux 2.6.32-358.el6.x86_64 (fzck-10-59-107-216.h.173ops.com) 2017年11月21日 _x86_64_ (32 CPU)Device: tps kB_read/s kB_wrtn/s kB_read kB_wrtnsda 13.35 31.70 161.99 2867672698 14655271354sdb 0.86 17.77 29.61 1607620286 2679034433 参数解释： tps：该设备每秒的传输次数（Indicate the number of transfers per second that were issued to the device.）。”一次传输”意思是”一次I/O请求”。多个逻辑请求可能会被合并为”一次I/O请求”。”一次传输”请求的大小是未知的。 kB_read/s：每秒从设备（drive expressed）读取的数据量； kB_wrtn/s：每秒向设备（drive expressed）写入的数据量； kB_read：读取的总数据量； kB_wrtn：写入的总数量数据量；这些单位都为Kilobytes。 指定监控的设备名称为sda，该命令的输出结果和上面命令完全相同。1iostat -d sda 2 默认监控所有的硬盘设备，现在指定只监控sda。 -x 参数123456789101112131415iostat -d -x -k 1 10Linux 2.6.32-358.el6.x86_64 (fzck-10-59-107-216.h.173ops.com) 2017年11月21日 _x86_64_ (32 CPU)Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.01 28.67 1.47 11.89 31.70 161.99 29.01 0.01 0.57 2.76 0.30 0.16 0.21sdb 0.00 0.00 0.47 0.39 17.77 29.61 109.69 0.00 1.61 0.55 2.87 0.38 0.03Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00Device: rrqm/s wrqm/s r/s w/s rkB/s wkB/s avgrq-sz avgqu-sz await r_await w_await svctm %utilsda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 rrqm/s：每秒这个设备相关的读取请求有多少被Merge了（当系统调用需要读取数据的时候，VFS将请求发到各个FS，如果FS发现不同的读取请求读取的是相同Block的数据，FS会将这个请求合并Merge）； wrqm/s：每秒这个设备相关的写入请求有多少被Merge了. rsec/s：每秒读取的扇区数. wsec/：每秒写入的扇区数。 rKB/s：The number of read requests that were issued to the device per second； wKB/s：The number of write requests that were issued to the device per second； avgrq-sz 平均请求扇区的大小 avgqu-sz 是平均请求队列的长度。毫无疑问，队列长度越短越好。 await： 每一个IO请求的处理的平均时间（单位是微秒毫秒）。这里可以理解为IO的响应时间，一般地系统IO响应时间应该低于5ms，如果大于10ms就比较大了。 这个时间包括了队列时间和服务时间，也就是说，一般情况下，await大于svctm，它们的差值越小，则说明队列时间越短，反之差值越大，队列时间越长，说明系统出了问题。 svctm 表示平均每次设备I/O操作的服务时间（以毫秒为单位）。如果svctm的值与await很接近，表示几乎没有I/O等待，磁盘性能很好，如果await的值远高于svctm的值，则表示I/O队列等待太长， 系统上运行的应用程序将变慢。%util： 在统计时间内所有处理IO时间，除以总共统计时间。例如，如果统计间隔1秒，该设备有0.8秒在处理IO，而0.2秒闲置，那么该设备的%util = 0.8/1 = 80%，所以该参数暗示了设备的繁忙程度。一般地，如果该参数是100%表示设备已经接近满负荷运行了（当然如果是多磁盘，即使%util是100%，因为磁盘的并发能力，所以磁盘使用未必就到了瓶颈）。 内存性能指标磁盘性能指标网络IO指标系统评估指标 性能因素 好 坏 糟糕 CPU user% + sys% &lt; 70% user% + sys% =85% user% + sys% &gt;= 90% 内存 Swap In(si) = 0 Swap Out(so) = 0 Per CPU with 10 pages/s More Swap In &amp; Swap Out 磁盘 iowait%&lt; 20% iowat%=35% iowat% &gt;= 50%]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>性能优化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux自动删除N天日志]]></title>
    <url>%2F2017%2F11%2F20%2Flinux%2Flinux_1%2F</url>
    <content type="text"><![CDATA[前言 web在部署到linux环境后，一般都是把日志等级设置加高。只输出Error信息或者直接关闭Log。但是某些系统需要搜集容器的access日志来做数据分析。如果本地存储的话，日子久了，日志就越来越大。占用大量磁盘资源，直接影响到系统的正常运行。所以，这种情况下，对日志做定期清理，是成本最低的方法了。 过程删除文件脚本1find 对应目录 -mtime +天数 -name &quot;文件名&quot; -exec rm -rf &#123;&#125; \; 例如:删除3天前 /opt/logs下 search开头的日志。1find /opt/logs/ -mtime +3 -name &quot;search*.log&quot; exec rm -rf &#123;&#125; \; 说明：find：linux的查找命令，用户查找指定条件的文件/opt/logs/：想要进行清理的任意目录；-mtime：标准语句写法；+3：查找30天前的文件，这里用数字代表天数；“search*.log”：支持范式匹配-exec：执行rm -rf：强制删除命令{} \; ：固定写法，一对大括号+空格++; 计划任务将以上命令放置到可执行shell脚本中再通过cron调度执行。创建shell:12touch ~/bin/auto-del-30-days-ago-log.shchmod +x auto-del-30-days-ago-log.sh 编辑shell脚本1vim auto-del-3-days-ago-log.sh 内容如下：12#!/bin/shfind /opt/logs/ -mtime +3 -name &quot;search*.log&quot; exec rm -rf &#123;&#125; \; 添加计划调度：执行：110 0 * * * ~/auto-del-3-days-ago-log.sh &gt;/dev/null 2&gt;&amp;1 设置是每天凌晨0点10分执行auto-del-3-days-ago-log.sh文件进行数据清理任务了。 总结THE END.]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>系统维护</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：装饰器模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_7%2F</url>
    <content type="text"><![CDATA[概念 动态地将责任附加到对象上，若要扩展功能，装饰者提供了比继承更有弹性的替代方案。 实现 类图： Money:1234567891011package com.littlehui.design.decorator;/** * Created by littlehui on 2018/1/15. */public interface Money &#123; public Double totalMoney(); public String getDescription();&#125; Wages:123456789101112131415package com.littlehui.design.decorator;/** * Created by littlehui on 2018/1/15. */public class Wages implements Money &#123; public Double totalMoney() &#123; return 10D; &#125; public String getDescription() &#123; return "基础工资，"; &#125;&#125; Reward:123456789101112131415161718192021package com.littlehui.design.decorator;/** * Created by littlehui on 2018/1/15. */public class Reward implements Money &#123; Money money; public Reward(Money money) &#123; this.money = money; &#125; public Double totalMoney() &#123; return 4d + money.totalMoney(); &#125; public String getDescription() &#123; return "加上额外奖励" + money.getDescription(); &#125;&#125; Bonuses:123456789101112131415161718192021package com.littlehui.design.decorator;/** * Created by littlehui on 2018/1/15. */public class Bonuses implements Money &#123; Money money; public Bonuses(Money money) &#123; this.money = money; &#125; public Double totalMoney() &#123; return 5d + money.totalMoney(); &#125; public String getDescription() &#123; return "加上奖金" + money.getDescription(); &#125;&#125; Client:12345678910111213141516package com.littlehui.design.decorator;/** * Created by littlehui on 2017/11/9. */public class Client &#123; public static void main(String[] args) &#123; Money wages = new Wages(); //奖金装饰它 wages = new Bonuses(wages); //额外奖励装饰它 wages = new Reward(wages); System.out.println("工资：" + wages.getDescription() + wages.totalMoney()); &#125;&#125; 场景Java.io包里就使用了装饰器。BufferedInputStream及LineNumberInputStream都扩展自FilterInputStream，而FilterInputStream是一个抽象的装饰类。 总结 装饰器模式体现了设计模式里的 开放-关闭原则。 装饰者和被装饰者对象有相同的父类 可以使用一个或者多个装饰者包装一个对象。 在任何需要原始对象他们可以相互替换。 装饰者可以在所委托被装饰者的行为之前与/或之后，加上自己的行为，以达到特定的目的。 对象可以在任何时候被装饰，所以可以在运行时动态，不限量地使用。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：外观模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_9%2F</url>
    <content type="text"><![CDATA[概念 外观模式用于简化系统中一个或者多个复杂的类。外观模式相当直接，容易理解。它提供了一个统一的接口，用来访问子系统中的一群接口。外观定义了一个高层接口，让子同更容易使用。 实现 类图： MyOneDay:12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576package com.littlehui.design.facade;/** * Created by littlehui on 2018/1/16. * 我的一天 */public class MyOneDay &#123; private Morning morning; private Road road; private Back back; private Office office; private Rest rest; public MyOneDay(Morning morning, Office office, Back back, Road road, Rest rest) &#123; this.morning = morning; this.office = office; this.back = back; this.road = road; this.rest = rest; &#125; /** * 在家起床 */ public void morning() &#123; morning.morningStepA(); morning.morningStepB(); morning.moringStepC(); &#125; /** * 去公司 */ public void goToOffice() &#123; road.onLoadA(); road.onLoadB(); road.onLoadC(); road.onLoadE(); &#125; /** * 工作，coding */ public void work() &#123; office.officeA(); office.officeB(); office.officeC(); &#125; public void backToHome() &#123; back.backStepA(); back.backStepB(); back.backStepC(); back.backStepD(); &#125; public void rest() &#123; rest.restA(); rest.restB(); rest.restC(); rest.restD(); &#125; public void myWholeDay() &#123; morning(); goToOffice(); work(); backToHome(); rest(); &#125;&#125; Morning:12345678910111213141516171819package com.littlehui.design.facade;/** * Created by littlehui on 2018/1/16. */public class Morning &#123; public void morningStepA() &#123; System.out.println("起床刷牙洗脸"); &#125; public void morningStepB() &#123; System.out.println("吃早饭"); &#125; public void moringStepC() &#123; System.out.println("带上背包出门"); &#125;&#125; Office:12345678910111213141516171819package com.littlehui.design.facade;/** * Created by littlehui on 2018/1/16. */public class Office &#123; public void officeA() &#123; System.out.println("放下背包"); &#125; public void officeB() &#123; System.out.println("去除电脑开机"); &#125; public void officeC() &#123; System.out.println("打开IDEA 愉快地codeing"); &#125;&#125; Client:1234567891011package com.littlehui.design.facade;/** * Created by littlehui on 2018/1/16. */public class Client &#123; public static void main(String[] args) &#123; MyOneDay myOneDay = new MyOneDay(new Morning(), new Office(), new Back(), new Road(), new Rest()); myOneDay.myWholeDay(); &#125;&#125; 以上其他类略：详情github链接： 设计模式 场景总结 外观模式体现了设计模式中 最少知识原则。不让太多的类耦合在一起。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：代理模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_8%2F</url>
    <content type="text"><![CDATA[概念 为另外一个对象提供一个替身或者占位符以控制对这个对象的访问。 实现静态代理 类图： Car：123456789package com.littlehui.design.proxy.statics;/** * Created by littlehui on 2018/1/16. */public interface Car &#123; public void run();&#125; Bus:1234567891011package com.littlehui.design.proxy.statics;/** * Created by littlehui on 2018/1/16. */public class Bus implements Car &#123; public void run() &#123; System.out.println("bus run"); &#125;&#125; BusProxy:12345678910111213package com.littlehui.design.proxy.statics;/** * Created by littlehui on 2018/1/16. */public class BusProxy implements Car &#123; public void run() &#123; System.out.println("car proxy"); Car bus = new Bus(); bus.run(); &#125;&#125; Client:123456789101112package com.littlehui.design.proxy.statics;/** * Created by littlehui on 2018/1/16. */public class Client &#123; public static void main(String[] args) &#123; Car bus = new BusProxy(); bus.run(); &#125;&#125; car为接口 Bus实现car的run方法 BusProxy负责控制Bus访问方法。 动态代理 类图： 动态代理是JDK支持的一种方式 实现例子如下： Car12345678package com.littlehui.design.proxy.dymatic;/** * Created by littlehui on 2018/1/16. */public interface Car &#123; public void run();&#125; Bus:12345678910package com.littlehui.design.proxy.dymatic;/** * Created by littlehui on 2018/1/16. */public class Bus implements Car &#123; public void run() &#123; System.out.println("car run"); &#125;&#125; BusProxyFactory:123456789101112131415161718192021222324252627282930313233package com.littlehui.design.proxy.dymatic;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;/** * Created by littlehui on 2018/1/16. */public class BusProxyFactory &#123; private Object target; public BusProxyFactory(Object car) &#123; this.target = car; &#125; public Object getNewInstance() &#123; return Proxy.newProxyInstance( target.getClass().getClassLoader(), target.getClass().getInterfaces(), new InvocationHandler() &#123; public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; System.out.println("开始事务"); //执行目标对象方法 Object returnValue = method.invoke(target, args); System.out.println("提交事务"); return returnValue; &#125; &#125; ); &#125;&#125; Client:123456789101112131415package com.littlehui.design.proxy.dymatic;/** * Created by littlehui on 2018/1/16. */public class Client &#123; public static void main(String[] args) &#123; Car bus = new Bus(); Car newBus = (Car) new BusProxyFactory(bus).getNewInstance(); System.out.println(newBus.getClass()); newBus.run(); &#125;&#125; CGLIB动态代理，引入包spring-core-XX.jarBusProxyFactoryCglib1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.littlehui.design.proxy.dymatic;import org.springframework.cglib.proxy.Enhancer;import org.springframework.cglib.proxy.MethodInterceptor;import org.springframework.cglib.proxy.MethodProxy;import java.lang.reflect.Method;/** * Created by littlehui on 2018/1/16. */public class BusProxyFactoryCglib implements MethodInterceptor &#123; private Object target;//业务类对象，供代理方法中进行真正的业务方法调用 //相当于JDK动态代理中的绑定 public Object getInstance(Object target) &#123; this.target = target; //创建加强器，用来创建动态代理类 Enhancer enhancer = new Enhancer(); //为加强器指定要代理的业务类（即：为下面生成的代理类指定父类） enhancer.setSuperclass(this.target.getClass()); //设置回调：对于代理类上所有方法的调用，都会调用CallBack，而Callback则需要实现intercept()方法进行拦 enhancer.setCallback(this); // 创建动态代理类对象并返回 return enhancer.create(); &#125; //CGLIB的特有方式，不指定 具体对象，只指定类 public Object getInstanceByClass(Class targetClass) &#123; //创建加强器，用来创建动态代理类 Enhancer enhancer = new Enhancer(); //为加强器指定要代理的业务类（即：为下面生成的代理类指定父类） enhancer.setSuperclass(targetClass); //设置回调：对于代理类上所有方法的调用，都会调用CallBack，而Callback则需要实现intercept()方法进行拦 enhancer.setCallback(this); // 创建动态代理类对象并返回 return enhancer.create(); &#125; public Object intercept(Object o, Method method, Object[] objects, MethodProxy methodProxy) throws Throwable &#123; System.out.println("预处理"); methodProxy.invokeSuper(o, objects); //调用业务类（父类中）的方法 System.out.println("调用后操作"); return null; &#125;&#125; Client:12345678910111213141516171819package com.littlehui.design.proxy.dymatic;/** * Created by littlehui on 2018/1/16. */public class Client &#123; public static void main(String[] args) &#123; Car bus = new Bus(); Car newBus = (Car) new BusProxyFactory(bus).getNewInstance(); System.out.println(newBus.getClass()); newBus.run(); System.out.println("-----------cglib------------"); Car cglibBus = (Car)new BusProxyFactoryCglib().getInstanceByClass(Bus.class); cglibBus.run(); System.out.println(cglibBus.getClass()); &#125;&#125; result:1234567891011121314/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/bin/java -agentlib:jdwp=transport=dt_socket,address=127.0.0.1:62751,suspend=y,server=n -Dfile.encoding=UTF-8 -classpath &quot;/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/deploy.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/javaws.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jfxswt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/management-agent.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/plugin.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/packager.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_111.jdk/Contents/Home/lib/tools.jar:/Users/littlehui/WorkSpaces/Home/pattern/proxy/target/classes:/Users/littlehui/software/repository/org/springframework/spring-core/5.0.0.RC3/spring-core-5.0.0.RC3.jar:/Users/littlehui/software/repository/org/springframework/spring-jcl/5.0.0.RC3/spring-jcl-5.0.0.RC3.jar:/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar&quot; com.littlehui.design.proxy.dymatic.ClientConnected to the target VM, address: &apos;127.0.0.1:62751&apos;, transport: &apos;socket&apos;class com.sun.proxy.$Proxy0开始事务car run提交事务-----------cglib------------预处理Disconnected from the target VM, address: &apos;127.0.0.1:62751&apos;, transport: &apos;socket&apos;car run调用后操作class com.littlehui.design.proxy.dymatic.Bus$$EnhancerByCGLIB$$1e8a65c7Process finished with exit code 0 Cglib生成的动态代理类是业务类的子类，重写业务方法进行代理。可以看到CGLIB调用的 class是 class com.littlehui.design.proxy.dymatic.Bus 这个在Spring类的装配和其他涉及获取Class的地方相当有用。 car为接口 Bus实现Car的run方法 Bus对象的访问交给了BusProxyFactory控制。 BusProxyFactory执行中动态地加入了run方法执行前后的标识。场景 Spring AOP 如果目标对象实现了接口，默认情况下会采用JDK的动态代理实现AOP 如果目标对象实现了接口，可以强制使用CGLIB实现AOP 如果目标对象没有实现了接口，必须采用CGLIB库，spring会自动在JDK动态代理和CGLIB之间转换 总结 代理和装饰模式看起来很相似，但是深入理解，本质上是有区别的 主要的区别是：使用代理模式，代理和真实对象之间的的关系通常在编译时就已经确定了，而装饰者能够在运行时递归地被构造。也就是那句话：代理模式可以控制被代理的对象可以控制被代理对象的访问，而装饰模式是被装饰对象的增强。不体现在控制上。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列:享元模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_12%2F</url>
    <content type="text"><![CDATA[概念 一个类的实例有多种 “虚拟实例”。 虚拟实例通过共享数据的方式存在。 实现 类图： Tree:1234567891011121314151617181920212223242526272829303132333435363738394041424344package com.littlehui.design.flyweight;/** * Created by littlehui on 2018/1/23. */public class Tree &#123; private int x; private int y; private int age; public Tree(int x, int y, int age) &#123; this.x = x; this.y = y; this.age = age; &#125; public int getX() &#123; return x; &#125; public void setX(int x) &#123; this.x = x; &#125; public int getY() &#123; return y; &#125; public void setY(int y) &#123; this.y = y; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public void display() &#123; System.out.println("坐标x:" + x + "坐标y:" + y + "年龄age:" + age); &#125;&#125; 123456789101112131415161718192021222324252627282930package com.littlehui.design.flyweight;import java.util.HashMap;import java.util.Map;/** * Created by littlehui on 2018/1/23. */public class TreeManager &#123; Map&lt;String, Tree&gt; allTrees = new HashMap&lt;String, Tree&gt;(); public Tree createTree(int x, int y, int age) &#123; String treeHash = new StringBuffer().append(x).append(y).append(age).toString(); if (allTrees.get(treeHash) != null) &#123; return allTrees.get(treeHash); &#125; else &#123; Tree tree = new Tree(x, y, age); allTrees.put(treeHash, tree); return allTrees.get(treeHash); &#125; &#125; public void displayAllTrees() &#123; for (String key : allTrees.keySet()) &#123; Tree tree = allTrees.get(key); tree.display(); &#125; &#125;&#125; 123456789101112131415161718package com.littlehui.design.flyweight;/** * Created by littlehui on 2018/1/23. */public class Client &#123; public static void main(String[] args) &#123; TreeManager treeManager = new TreeManager(); Tree tree1 = treeManager.createTree(1,2,3); Tree tree2 = treeManager.createTree(1,2,3); Tree tree3 = treeManager.createTree(1,2,4); System.out.println("实例个数：" + "tree1, tree2, tree3"); System.out.println("真实实例个数："); treeManager.displayAllTrees(); &#125;&#125; 场景在java应用中,会出现许多String a=”123”,String b=”123”之类的String类型的变量,如果只是小应用,到还好,假设是一个庞大的系统,有好多处都需要用定义String a=”223”,那开销可想而知,而JDK的开发者自然想到了这点,采用了享元模式解决创建大量相同String变量带来的开销问题 总结 享元模式，其功能是在运行时减少实例的个数，节省内存。当一个类有许多的实例，而这些实例能被统一个方法控制到时候，可以用享元模式。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列:组合模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_11%2F</url>
    <content type="text"><![CDATA[概念 组合模式是把对象堆起来形成集合的一种方式。它组织对象形成集合，结合迭代器模式，可以对客户隐藏具体对象实现。不至于暴露集合内部信息。形式上经常将组合你模式用于对象的树形结构表示。 实现 背景 村子里面养了 鸡，鸭，本地鸭，外地鸭，猪。这些家畜都会跑。现在要将他们集合起来，进行统一管理。每天数数，防止丢失。 抽象我们可以用组合模式来进行管理这些家畜。首先家畜的集合进行抽象树形结构。第一层：普通家畜第二层：普通家禽下有 猪，禽类第三层：禽类 下面有 鸡，鸭第四层：鸭子下面有 本地鸭，外地鸭。 类图 对象关联图： 部分代码： AnimalType: 1234567891011package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;/** * Created by littlehui on 2018/1/18. */public interface AnimalType &#123; public AnimalIterator createIterator();&#125; 123456789101112131415package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;/** * Created by littlehui on 2018/1/18. */public interface Animal &#123; public void run(); public void each(); public AnimalIterator iterator();&#125; 12345678910111213141516171819202122232425262728293031323334353637383940package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;import com.littlehui.design.iterate.HomeAnimalIterator;import com.littlehui.design.iterate.IteratorComposite;import java.util.ArrayList;import java.util.List;/** * Created by littlehui on 2018/1/18. */public class Bird extends IteratorAnimal implements AnimalType &#123; List&lt;Animal&gt; birds; public Bird() &#123; this.birds = new ArrayList&lt;Animal&gt;(); birds.add(new Chicken()); birds.add(new Duck()); &#125; public void run() &#123; System.out.println("鸟类跑"); &#125; public void each() &#123; for (Animal animal : birds) &#123; animal.each(); &#125; &#125; public AnimalIterator iterator() &#123; return new HomeAnimalIterator(birds); &#125; public AnimalIterator createIterator() &#123; return new IteratorComposite(this.iterator()); &#125;&#125; 12345678910111213141516package com.littlehui.design.composite;/** * Created by littlehui on 2018/1/18. */public class Chicken extends IteratorAnimal &#123; public void run() &#123; System.out.println("小鸡跑"); &#125; public void each() &#123; System.out.println("鸡"); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;import com.littlehui.design.iterate.HomeAnimalIterator;import com.littlehui.design.iterate.IteratorComposite;import java.util.ArrayList;import java.util.List;/** * Created by littlehui on 2018/1/18. */public class Duck extends IteratorAnimal implements AnimalType &#123; List&lt;Animal&gt; ducks; public Duck() &#123; ducks = new ArrayList&lt;Animal&gt;(); ducks.add(new LocalDuck()); ducks.add(new ForignDuck()); &#125; public void run() &#123; System.out.println("鸭类跑"); &#125; public void each() &#123; System.out.println("鸭"); &#125; public AnimalIterator iterator() &#123; return new HomeAnimalIterator(ducks); &#125; public AnimalIterator createIterator() &#123; return new IteratorComposite(this.iterator()); &#125;&#125; 1234567891011121314package com.littlehui.design.composite;/** * Created by littlehui on 2018/1/18. */public class ForignDuck extends IteratorAnimal &#123; public void run() &#123; System.out.println("外地鸭跑"); &#125; public void each() &#123; System.out.println("外地鸭"); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;import com.littlehui.design.iterate.HomeAnimalIterator;import com.littlehui.design.iterate.IteratorComposite;import java.util.ArrayList;import java.util.List;/** * Created by littlehui on 2018/1/18. */public class HomeAnimal extends IteratorAnimal implements AnimalType &#123; private List&lt;Animal&gt; homeAnimals = new ArrayList&lt;Animal&gt;(); public HomeAnimal() &#123; homeAnimals.add(new Pig()); homeAnimals.add(new Bird()); &#125; public void run() &#123; System.out.println("家养牲畜跑"); &#125; public void each() &#123; System.out.println("家养牲畜跑"); &#125; public AnimalIterator iterator() &#123; return new HomeAnimalIterator(homeAnimals); &#125; public AnimalIterator createIterator() &#123; return new IteratorComposite(this.iterator()); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;/** * Created by littlehui on 2018/1/18. */public class IteratorAnimal implements Animal &#123; public IteratorAnimal() &#123; &#125; public void run() &#123; &#125; public void each() &#123; &#125; public AnimalIterator iterator() &#123; final IteratorAnimal baseAnimal = this; return new AnimalIterator() &#123; public boolean hasNext() &#123; if (baseAnimal != null ) &#123; return true; &#125; return false; &#125; public Animal next() &#123; return baseAnimal; &#125; &#125;; &#125;&#125; 123456789101112131415package com.littlehui.design.composite;/** * Created by littlehui on 2018/1/18. */public class LocalDuck extends IteratorAnimal &#123; public void run() &#123; System.out.println("本地鸭跑"); &#125; public void each() &#123; System.out.println("本地鸭"); &#125;&#125; 123456789101112131415package com.littlehui.design.composite;/** * Created by littlehui on 2018/1/18. */public class Pig extends IteratorAnimal &#123; public void run() &#123; System.out.println("猪在跑"); &#125; public void each() &#123; System.out.println("猪"); &#125;&#125; Client:12345678910111213141516171819package com.littlehui.design.composite;import com.littlehui.design.iterate.AnimalIterator;/** * Created by littlehui on 2018/1/18. */public class Client &#123; public static void main(String[] args) &#123; HomeAnimal homeAnimal = new HomeAnimal(); AnimalIterator homeAnimalIterator = homeAnimal.createIterator(); while (homeAnimalIterator.hasNext()) &#123; Animal animal = homeAnimalIterator.next(); animal.run(); &#125; &#125;&#125; 迭代器组合部分 设计模式-组合模式 解析实际上，上面的代码分两个部分理解：1：家畜动物们的关联组合2：对动物们遍历的迭代组合(代码略)组合模式经常会应用到迭代模式，这里也都写上了。 场景HtmlPaser包，解析Html页面。就是典型的组合模式。 总结组合模式关注的重点是对对象的结合方式。结合后暴露统一的接口管理。正如上所表达的，动物们集合后通过组合迭代器的方式统一暴露了一个遍历的方法口。屏蔽了内部实现，调用端只需调用迭代方法就可以实现遍历管理了。有了迭代组合还可以个性化的筛选，等等趋向业务逻辑的实现。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列:桥接模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_10%2F</url>
    <content type="text"><![CDATA[概念 类的继承是类本身的垂直维度变化。如果需要水平维度上的变化扩展。继承是不好实现的。这时候我们可以引入桥接方式。桥接模式的做法是把变化部分抽象出来，使变化部分与主类分离开来，从而将多个维度的变化彻底分离。最后，提供一个管理类来组合不同维度上的变化，通过这种组合来满足业务的需要。 实现 Fruit:12345678package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public interface Fruit &#123; public void eat();&#125; AbstractFruit:12345678910111213141516171819package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public class AbstractFruit implements Fruit &#123; EatHandler eatHandler; public AbstractFruit(EatHandler eatHandler) &#123; this.eatHandler = eatHandler; &#125; @Override public void eat() &#123; eatHandler.handle(); System.out.println("开始吃水果。"); &#125;&#125; Apple:123456789101112131415package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public class Apple extends AbstractFruit &#123; public Apple(EatHandler eatHandler) &#123; super(eatHandler); &#125; public void enjoy() &#123; eat(); &#125;&#125; WaterMelon:1234567891011121314151617package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public class WaterMelon extends AbstractFruit &#123; public WaterMelon(EatHandler eatHandler) &#123; super(eatHandler); &#125; public void eat() &#123; super.eat(); System.out.println("吃西瓜"); &#125;&#125; EatHandler:1234567891011package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public abstract class EatHandler &#123; public void handle() &#123; System.out.println("吃水果前处理。"); &#125;&#125; PeelHandler :123456789101112package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public class PeelHandler extends EatHandler &#123; @Override public void handle() &#123; super.handle(); System.out.println("削皮"); &#125;&#125; EatHandlerDivid:1234567891011package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public class EatHandlerDivid extends EatHandler &#123; public void handle() &#123; super.handle(); System.out.println("切块。"); &#125;&#125; 123456789101112131415package com.littlehui.design.bridge;/** * Created by littlehui on 2018/1/17. */public class Client &#123; public static void main(String[] args) &#123; EatHandler eatHandler = new PeelHandler(); EatHandler eatHandler1 = new EatHandlerDivid(); Fruit apple = new Apple(eatHandler); Fruit waterMelon = new WaterMelon(eatHandler1); apple.eat(); waterMelon.eat(); &#125;&#125; 如上代码解释： 抽象部分 1：吃水果接口抽象，2.吃水果前处理抽象。 具体实现部分 : apple里的enjoy 这里就分离了水果关于吃水果和水果处理的部分。Apple里的enjoy是具体的实现，可以eat，可以做其他操作。我们可以切换 水果处理 Handle来改变 Appleenjoy具体时动作。 场景略 总结桥接的重点是 将实现解耦，抽象和实现独立开，不影响对方。桥接一般用于跨多个平台的图形和窗口系统上。当需要不同的方式改变借口和实现时，可以用桥接。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：适配器模式]]></title>
    <url>%2F2017%2F11%2F09%2Fjava%2Fpatterndesign%2Fpatterndesign_6%2F</url>
    <content type="text"><![CDATA[概念 适配器模式是将一个类的接口，转换成客户期望 的另一个接口。适配器让原来接口不兼容的类可以合作。 实现 类图： 对象适配器 类适配器 Target:12345678package com.littlehui.design.adapter.target;/** * Created by littlehui on 2017/11/9. */public interface Target &#123; public void doTargetThings(String value);&#125; Adapter12345678910111213141516171819package com.littlehui.design.adapter.source;import com.littlehui.design.adapter.target.Target;/** * Created by littlehui on 2017/11/9. */public class Adapter implements Target &#123; Adaptee adaptee; public Adapter(Adaptee adaptee) &#123; this.adaptee = adaptee; &#125; public void doTargetThings(String value) &#123; adaptee.doSourceThings(value); &#125;&#125; Adaptee12345678910package com.littlehui.design.adapter.source;/** * Created by littlehui on 2017/11/9. */public class Adaptee &#123; public void doSourceThings(String value) &#123; System.out.println("value is : " + value + "adaptee"); &#125;&#125; ConcreteTarget1234567891011package com.littlehui.design.adapter.target;/** * Created by littlehui on 2017/11/9. */public class ConcreteTarget implements Target &#123; public void doTargetThings(String value) &#123; System.out.println("value is:" + value); &#125;&#125; Client:12345678910111213141516171819package com.littlehui.design.adapter;import com.littlehui.design.adapter.source.Adaptee;import com.littlehui.design.adapter.source.Adapter;import com.littlehui.design.adapter.target.ConcreteTarget;import com.littlehui.design.adapter.target.Target;/** * Created by littlehui on 2017/11/9. */public class Client &#123; public static void main(String[] args) &#123; Target target = new ConcreteTarget(); Target adaTarget = new Adapter(new Adaptee()); target.doTargetThings("意外"); adaTarget.doTargetThings("意外"); &#125;&#125; 以上利用组合的方式，以修改的接口包装适配者。这种实现方式称之为对象的适配器。它带来的有点是：被适配者的任意子类，都可以搭配适配器使用。 场景以Java举例，早先JDK里使用了枚举器（Enumeration)随着版本的更迭后来被 迭代器(Iterator)取代了。为了兼容之前的接口，枚举器仍然被保留着。并且枚举器添加了删除元素的接口。如果遇到保留着 枚举器的客户端代码，依赖于枚举接口，完全可以使用适配器模式将迭代器转换成枚举。 总结设计模式的原则有一条：组合优于继承。体现在适配器模式上就有两种适配方式。组合的称之为 对象时适配模式，继承的称之为对象适配模式。组合的有点是不仅可以适配某个类，还可以适配这个类的子类属于垂直扩展。类适配器不需要重新实现整个适配者。必要的时候覆盖被适配行为就可以。两种各有优劣。站在java的角度思考，类适配多更需要多继承的方式（虽然也可以用接口）。java不支持多继承。所以 建议使用对象适配。既组合。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：概况]]></title>
    <url>%2F2017%2F10%2F25%2Fjava%2Fpatterndesign%2Fparterndesign_0%2F</url>
    <content type="text"><![CDATA[概况 设计模式是对于Java变成过程中遇到的特定抽象场景总结出的一套通用方法,常用的用23种几年模式。 分类总体来说设计模式分为三大类： 创建型模式，共五种： 工厂方法模式 定义一个创建对象的接口，让子类决定将哪一个类实例化。使一个类实例化延迟到子类。 抽象工厂模式 提供一个创建一系列产品或相互依赖对象接口无需指定具体的类。 单例模式 保证一个类仅有一个实例，并提供一个访问它的全局访问。 建造者模式 将一个复杂对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。 原型模式 用原型实例指定创建对象的种类，并且通过拷贝这个原型来创建新的对象。 结构型模式，共七种： 适配器模式 将一个类的接口转换成客户希望的另一个接口，Adapter使原本由于接口不兼容而不能正常工作的类可以正常工作。 装饰器模式 动态地给一个对象添加一些额外的职责，就扩展功能而言，它比生成子类方式更加灵活。 代理模式 为其他对象提供一个代理以控制对这个对象的访问。 外观模式 为子系统中的一组接口提供一个一致的界面，Facade模式定义一个高层接口，这个接口使得这一子系统更加容易使用。 桥接模式 将抽象部分与实现部分分离，使它们可以独立变化。 组合模式 将对象组合成树形结构以表示 “部分-整体”的层次结构。它使得客户对单个对象和符合对象的使用具有一致性。 享元模式 运用共享技术有效支持大量细粒度对象。 行为型模式，共十一种： 策略模式 定义一系列算法，把他们封装起来，并且使它们可以相互替换，本模式使得算法的变化可以独立与使用它的客户端。 模板方法模式 定义一个操作中的算法骨架，而将一些步骤延迟到子类，Template Method使得子类可以不改变一个算法结构即可重新定义该算法某些特定步骤。 观察者模式 定义对象间的一种一对多的依赖关系，以便当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并自动刷新。 迭代器模式 提供一种方法顺序访问一个聚合对象中各个元素而又不暴露该对象内部表示。 责任链模式 为接触请求的发送者和接受者之间的耦合，而使多个对象都有机会处理这个请求。将这些对象连成一条链，并沿着这条连传递该请求，知道有一个对象处理。 命令模式 将一个请求封装为一个对象，从而使可以用不同的请求对客户进行参数化，对请求排队或记录请求日志，以及支持可取消操作。 备忘录模式 在不破坏封装性的前提下，捕获一个对象的内部状态，并在该对象之外保存这个状态，这样以后就可将该对象回复到保存的状态。 状态模式 允许一个对象在其内部状态改变时改变它的行为，对象看起来似乎属于一个新的类。 访问者模式 表示一个作用于某对象结构中的各元素的操作。它使你可以在不改变各元素的类的前提下定义作用于这些元素的新操作。 中介者模式用一个中介对象来封装一系列对象交互，中介者使各对象不需要显示的相互引用，从而使得其耦合松散，而且可以独立的改变他们之间的交互。 解释器模式给定一个语言，定义它的文法的一种表示，并定义一个解释器，该解释器使用该表示来解释语言中句子。原则 开闭原则（Open Close Principle） 开闭原则就是说对扩展开放，对修改关闭。在程序需要进行拓展的时候，不能去修改原有的代码，实现一个热插拔的效果。 里氏代换原则（Liskov Substitution Principle） 子类的能力必须大于等于父类，即父类可以使用的方法，子类都可以使用。 返回值也是同样的道理。假设一个父类方法返回一个List，子类返回一个ArrayList，这当然可以。如果父类方法返回一个ArrayList，子类返回一个List，就说不通了。这里子类返回值的能力是比父类小的。 还有抛出异常的情况。任何子类方法可以声明抛出父类方法声明异常的子类。而不能声明抛出父类没有声明的异常。一句话理解：所有使用父类的地方都可以被子类替换 依赖倒转原则（Dependence Inversion Principle） 面向接口编程，依赖于抽象而不依赖于具体。 接口隔离原则（Interface Segregation Principle） 使用多个隔离的接口，比使用单个接口要好。还是一个降低类之间的耦合度的意思，从这儿我们看出，其实设计模式就是一个软件的设计思想， 从大型软件架构出发，为了升级和维护方便。所以上文中多次出现：降低依赖，降低耦合。 迪米特法则（最少知道原则）（Demeter Principle）一个实体应当尽量少的与其他实体之间发生相互作用，使得系统功能模块相对独立。 合成复用原则（Composite Reuse Principle） 当聚合和继成都可以实现时候，应该使用聚合。聚合由于继承。 总结 上面提到的23种是经典和常用的模式。在实际生产中按照需求总结提取使用。有利于提高代码和程序功能的扩展维护性。然而也不能一味的以使用模式而是用，这就本末倒置了。随着更深入的理解和学习设计模式，会渐渐意识到：设计模式真正的的模式是 “无模式”。看似无招胜似有招。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：原型模式]]></title>
    <url>%2F2017%2F10%2F25%2Fjava%2Fpatterndesign%2Fparterndesign_5%2F</url>
    <content type="text"><![CDATA[概念 原型模式是特殊的创建模式，它创建对象不通过直接new的方式产生，而是通过已有的对象复制。 实现浅拷贝 类图： Product123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Product implements Serializable &#123; private String name; private String value; private ProductB productB; public ProductB getProductB() &#123; return productB; &#125; public void setProductB(ProductB productB) &#123; this.productB = productB; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public void display() &#123; System.out.println("product name:" + this.name); System.out.println("product value:" + this.value); System.out.println("productB name:" + this.productB.getName()); &#125; public Product deepClone()&#123; try &#123; ByteArrayOutputStream bo = new ByteArrayOutputStream(); ObjectOutputStream oo = new ObjectOutputStream(bo); oo.writeObject(this); ByteArrayInputStream bi = new ByteArrayInputStream(bo.toByteArray()); ObjectInputStream oi = new ObjectInputStream(bi); return (Product)oi.readObject(); &#125; catch (IOException | ClassNotFoundException e) &#123; // TODO Auto-generated catch block e.printStackTrace(); return null; &#125; &#125;&#125; ProductB123456789101112public class ProductB implements Serializable &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; Client12345678910111213141516public class Client &#123; public static void main(String[] args) &#123; Product product = new Product(); ProductB productB = new ProductB(); productB.setName("B"); product.setName("X"); product.setValue("XX"); product.setProductB(productB); Product clone = product.deepClone(); clone.getProductB().setName("newXX"); product.display(); clone.display(); &#125;&#125; 执行结果：Connected to the target VM, address: ‘127.0.0.1:58904’, transport: ‘socket’product name:Xproduct value:XXproductB name:Bproduct name:Xproduct value:XXproductB name:newXXDisconnected from the target VM, address: ‘127.0.0.1:58904’, transport: ‘socket’ 深拷贝 类图： Product1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class Product implements Cloneable &#123; private String name; private String value; private ProductB productB; public ProductB getProductB() &#123; return productB; &#125; public void setProductB(ProductB productB) &#123; this.productB = productB; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125; public String getValue() &#123; return value; &#125; public void setValue(String value) &#123; this.value = value; &#125; public void display() &#123; System.out.println("product name:" + this.name); System.out.println("product value:" + this.value); System.out.println("productB name:" + this.productB.getName()); &#125; public Product clone() &#123; try &#123; return (Product)super.clone(); &#125; catch (CloneNotSupportedException e) &#123; e.printStackTrace(); &#125; return null; &#125;&#125; ProductB123456789101112public class ProductB &#123; private String name; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; Client12345678910111213141516public class Client &#123; public static void main(String[] args) &#123; Product product = new Product(); ProductB productB = new ProductB(); productB.setName("B"); product.setName("X"); product.setValue("XX"); product.setProductB(productB); Product clone = product.clone(); clone.getProductB().setName("newXX"); product.display(); clone.display(); &#125;&#125; 执行结果：Connected to the target VM, address: ‘127.0.0.1:58962’, transport: ‘socket’product name:Xproduct value:XXproductB name:newXXproduct name:Xproduct value:XXproductB name:newXXDisconnected from the target VM, address: ‘127.0.0.1:58962’, transport: ‘socket’ 场景总结 浅拷贝 拷贝后的对象如果有嵌套的复杂对象，那么改变嵌套对象会跟着改变。只拷贝表层的对象信息。深拷贝，是所有的都拷贝，包括嵌套对象。这里的实现是通过序列化的方式实现。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：建造者模式]]></title>
    <url>%2F2017%2F10%2F25%2Fjava%2Fpatterndesign%2Fparterndesign_4%2F</url>
    <content type="text"><![CDATA[概念 当要创建的对象相对复杂，可以将复杂对象的创建过程分离成若干过程。此时只要改变不同过程中的参数就可以产生不同的实例。 实现 类图 代码Builder12345678910public interface Builder &#123; public void buildProductPartA(); public void buildProductPartB(); public void buildProductPartC(); public Product buildProduct();&#125; Director123456789public class Director &#123; public Product constructProduct(Builder productBuilder)&#123; productBuilder.buildProductPartA(); productBuilder.buildProductPartB(); productBuilder.buildProductPartC(); return productBuilder.buildProduct(); &#125;&#125; ProductBuilder12345678910111213141516171819202122232425public class ProductBuilder implements Builder &#123; private Product product; @Override public void buildProductPartA() &#123; &#125; @Override public void buildProductPartB() &#123; &#125; @Override public void buildProductPartC() &#123; &#125; @Override public Product buildProduct() &#123; return null; &#125;&#125; Product123public class Product &#123;&#125; Client1234567public class Client &#123; public static void main(String[] args) &#123; Director director = new Director(); director.constructProduct(new ProductBuilder()); &#125;&#125; 场景我们看饮料机的工作步骤可以分为 倒入水，倒入饮料剂，制造饮料。定义一个饮料机就可以当做建造饮料的过程。 总结 建造者模式的核心是把类的创建过程分解成一个个过程。每个过程是一个单独的执行流程，可以通过不同参数指定流程的结果。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：抽象工厂模式]]></title>
    <url>%2F2017%2F10%2F25%2Fjava%2Fpatterndesign%2Fparterndesign_3%2F</url>
    <content type="text"><![CDATA[概念 抽象工厂模式在原有的工厂方法模式上扩展，在工厂方面进行了抽象。从而增加产品规格的需求，可以更高地抽象成增加工厂类型。降低了耦合。 实现类图 代码Factory123456public interface Factory &#123; public Product createSoftProduct(); public Product createHardProduct();&#125; FactoryA123456789101112public class ProductFactoryA implements Factory &#123; public Product createSoftProduct() &#123; System.out.println("工厂A:"); return new SoftProductA(); &#125; public Product createHardProduct() &#123; System.out.println("工厂A:"); return new HardProductA(); &#125;&#125; FactoryB12345678910111213public class ProductFactoryB implements Factory &#123; public Product createSoftProduct() &#123; System.out.println("工厂B:"); return new SoftProductB(); &#125; public Product createHardProduct() &#123; System.out.println("工厂B:"); return new HardProductB(); &#125;&#125; Product123public interface Product &#123; public String getName();&#125; ProductB123456public class ProductB implements Product &#123; public String getName() &#123; return "我是产品B"; &#125;&#125; SoftProductB12345678910public class SoftProductB extends ProductB &#123; public SoftProductB() &#123; System.out.println("创建 产品B:特性:柔软"); &#125; public String getName() &#123; return super.getName() + "柔软"; &#125;&#125; HardProductB123456789public class HardProductB extends ProductB &#123; public HardProductB() &#123; System.out.println("创建 产品B:特性:坚硬"); &#125; public String getName() &#123; return super.getName() + "坚硬"; &#125;&#125; ProductA123456public class ProductA implements Product &#123; public String getName() &#123; return "我是产品A"; &#125;&#125; SoftProductA12345678910public class SoftProductA extends ProductA &#123; public SoftProductA() &#123; System.out.println("创建 产品A:特性:柔软"); &#125; public String getName() &#123; return super.getName() + "柔软"; &#125;&#125; HardProductA12345678910public class HardProductA extends ProductA &#123; public HardProductA() &#123; System.out.println("创建 产品A:特性:坚硬"); &#125; public String getName() &#123; return super.getName() + "坚硬"; &#125;&#125; Client12345678910111213141516public class Client &#123; public static void main(String[] args) &#123; Factory factoryA = new ProductFactoryA(); Factory factoryB = new ProductFactoryB(); Product productAHard = factoryA.createHardProduct(); System.out.println(productAHard.getName()); Product productBHard = factoryB.createHardProduct(); System.out.println(productBHard.getName()); Product productASoft = factoryA.createSoftProduct(); System.out.println(productASoft.getName()); Product productBSoft = factoryB.createSoftProduct(); System.out.println(productBSoft.getName()); &#125;&#125; 场景如上产品 族 A B都有两个子类型，或者说特性。Hard or Soft。如果某个时候业务需求添加产品族C。此时扩展就很方便了，只要实现产品C的工厂类，就可以。不用修改原来的代码，耦合度低。 总结 抽象工厂优点: 扩展产品族（类型）容易缺点: 扩展产品族下的子产品难，需要整体结构调整。]]></content>
      <categories>
        <category>java</category>
        <category>设计模式</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：工厂方法模式]]></title>
    <url>%2F2017%2F10%2F25%2Fjava%2Fpatterndesign%2Fparterndesign_2%2F</url>
    <content type="text"><![CDATA[概念 工厂与一类产品的关系。用于调用端从复杂的构造逻辑中解耦。 实现简单工厂模式产品：Product1234public interface Product &#123; public void methodOne(); public void methodTwo();&#125; ProductA12345678public class ProductA implements Product &#123; public void methodOne() &#123; System.out.print("A产品方法1"); &#125; public void methodTwo() &#123; System.out.print("A产品方法2"); &#125;&#125; ProductB12345678public class ProductB implements Product &#123; public void methodOne() &#123; System.out.print("B产品方法1"); &#125; public void methodTwo() &#123; System.out.print("B产品方法2"); &#125; &#125; 工厂：Factory123public interface Factory &#123; public Product createProduct(String type);&#125; SimpleFactory123456789101112public class SimpleFactory implements Factory &#123; public Product createProduct(String type) &#123; switch (type) &#123; case "A": return new ProductA(); case "B": return new ProductB(); default: return null; &#125; &#125;&#125; PS：这里用Java7的写法，switch支持String。 客户端：Client123456789public class Client &#123; public static void main(String[] args) &#123; Factory factory = new SimpleFactory(); Product product = factory.createProduct("A"); product.methodOne(); product.methodTwo(); //业务代码 &#125;&#125; 场景A是一个接口，它的实现由 ClassA1,ClassA2,ClassA3。B是一段业务代码，需要new一个A1并且进行操作。这时候就可以用工厂模式。某天业务的修改，需操作到另外一个ClassA2的方法。在工厂类进行修改就可以了。业务端代码就不用修改。 总结 工厂方法模式有良好的封装性，代码结构清晰。扩展性非常优秀。在增加产品类的情况下，只要适当地修改具体或扩展工厂类即可。调用者它只需要关心产品的接口。 可以对调用端调用复杂的构造逻辑进行解耦。]]></content>
      <categories>
        <category>Java</category>
        <category>设计模式</category>
        <category>工厂方法</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多线程系列：volatile关键字]]></title>
    <url>%2F2017%2F10%2F25%2Fjava%2Fmultithread%2Fmultithread_1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[设计模式系列：单例模式]]></title>
    <url>%2F2017%2F10%2F23%2Fjava%2Fpatterndesign%2Fparterndesign_1%2F</url>
    <content type="text"><![CDATA[概念单例模式，顾名思义就是在Java应用中，类的实例保证只有一个在JVM中。他有几个好处 减少创建开销 减少内存使用频率，GC压力 保证流程独立 实现第一种 饿汉法123456789101112public class Singleton &#123; private static Singleton singleton = new Singleton(); private Singleton() &#123; &#125; public static Singleton getSignleton()&#123; return singleton; &#125;&#125; 代码简单，但是无法延迟加载。 第二种 单线程安全1234567891011public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 这种方式可以实现基本要求，但是在多线程情况下就会出现可能New出多个实例的情况。由此引入synchronized关键字，我们有如下实现： 第三种 多线程安全123456789101112131415public class Singleton &#123; private static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 第二种方法引入了syncronized 关键字，在调用getInstance方法的时候进行了并发处理。然而在多线程情况下仍然有问题情况如下： a&gt;A、B线程同时进入了第一个if判断 b&gt;A首先进入synchronized块，由于instance为null，所以它执行instance = new Singleton(); c&gt;在new对象的过程中，由于JVM的优化，指令进行重排序，JVM先画出了一些分配给Singleton实例的空白内存，并赋值给instance成员（注意此时JVM没有开始初始化这个实例），然后A离开了synchronized块。 d&gt;B进入synchronized块，由于instance此时不是null，因此它马上离开了synchronized块并将结果返回给调用该方法的程序。 e&gt;此时B线程打算使用Singleton实例，却发现它没有被初始化，于是错误发生了。 这里引入volatile关键字禁止对instance操作的指令重排。 第四种 多线程安全”多重锁检查””123456789101112131415public class Singleton &#123; private volatile static Singleton instance = null; private Singleton() &#123; &#125; public static Singleton getInstance() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 这种方法就是有名的DCL的单利模式。基本已经完善多线程下的单例模式。需要提醒的是， volatile屏蔽指令重排的语义在JDK 1.5中才被修复，所以JDK1.5之前的JAVA无法使用这一方式。 第五种 静态内部类方法静态类方法：1234567891011public class Singleton &#123; private static class Holder &#123; private static Singleton singleton = new Singleton(); &#125; private Singleton()&#123;&#125; public static Singleton getSingleton()&#123; return Holder.singleton; &#125;&#125; 类只加载一次，所以这中方式也是线程安全的。不过以上的方法都存在一些问题： 进行序列化时需要额外的工作进行序列化(Serializable,transient,readResolve())等操作。否则每次序列化都是创建一个新的实例。 构造器虽然是私有的，但是还是可以通过反射来强行调用创建实例。一个方法是在构造器里判断已经创建过实例抛异常。如何更优雅地解决以上两个缺陷呢，我们可以使用枚举单例。 第六种 枚举方法12345678910public enum Singleton &#123; INSTANCE; private String name; public String getName()&#123; return name; &#125; public void setName(String name)&#123; this.name = name; &#125;&#125; 枚举不仅线程安全，防止反射强行调用构造器外。还提供了自动化序列机制，繁殖序列化的时候创建新对象。更接近与”完美”的单利模式。 场景程序执行时候只需要一个实例执行的时候就可以用单例来：经典的场景有:线程池，驱动管理，通用的计算模块，工具类代码等等 总结 单例模式既熟悉，又陌生。看起来简单的功能，算法，要写好，无瑕疵，还是需要很大专研精神。避免遇到快很多坑。]]></content>
      <categories>
        <category>Java</category>
        <category>设计模式</category>
        <category>单例</category>
      </categories>
      <tags>
        <tag>设计模式，单例</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM系列：JVM内存优化实例]]></title>
    <url>%2F2017%2F10%2F11%2Fjava%2Fjvm%2Fjvm_3%2F</url>
    <content type="text"><![CDATA[优化实例java application项目（非web项目） 改进前：1234567891011-Xms128m-Xmx128m-XX:NewSize=64m-XX:PermSize=64m-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=78-XX:ThreadStackSize=128-Xloggc:logs/gc.log-Dsun.rmi.dgc.server.gcInterval=3600000-Dsun.rmi.dgc.client.gcInterval=3600000-Dsun.rmi.server.exceptionTrace=true 问题:permsize 设置较小,很容易达到报警范围(0.8)没有设置MaxPermSize，堆增长会带来额外压力。NewSize较大，old gen 剩余空间64m，一方面可能会带来old区容易增长到报警范围（监控数据显示oldgenused长期在50m左右，接近78%，容易出现full gc）,另一方面也存在promontion fail风险改进后：1234567891011121314151617181920212223-Xms128m-Xmx128m-Xmn24m-XX:PermSize=80m-XX:MaxPermSize=80m-Xss256k-XX:SurvivorRatio=1-XX:MaxTenuringThreshold=20-XX:+UseParNewGC-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=75-XX:+UseCMSCompactAtFullCollection-XX:+CMSParallelRemarkEnabled-XX:CMSFullGCsBeforeCompaction=2-XX:SoftRefLRUPolicyMSPerMB=0-XX:+PrintClassHistogram-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+PrintHeapAtGC-Xloggc:logs/gc.log-Dsun.rmi.dgc.server.gcInterval=3600000-Dsun.rmi.dgc.client.gcInterval=3600000-Dsun.rmi.server.exceptionTrace=true 修改点：PermSize与MaxPermSize都设置为80，一方面避免non heap warn(报警阀值0.8 非对内存一般占用到60M以内），一方面避免堆伸缩带来的压力通过设置Xmn=24M及SurvivorRatio=1 使得Eden区=from space=to space=8M,降低了Eden区大小，降低YGC的时间(降低到3-4ms左右),同时通过设MaxTenuringThreshold=20，使得old gen的增长很缓慢。带来的问题是YGC的次数明显提高了很多。其他参数优化 修改后带来的好处见JVM参数设置再次改进后1234567891011121314151617181920212223-Xms128m-Xmx128m-Xmn36m-XX:PermSize=80m-XX:MaxPermSize=80m-Xss256k-XX:SurvivorRatio=1-XX:MaxTenuringThreshold=20-XX:+UseParNewGC-XX:+UseConcMarkSweepGC-XX:CMSInitiatingOccupancyFraction=73-XX:+UseCMSCompactAtFullCollection-XX:+CMSParallelRemarkEnabled-XX:CMSFullGCsBeforeCompaction=2-XX:SoftRefLRUPolicyMSPerMB=0-XX:+PrintClassHistogram-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+PrintHeapAtGC-Xloggc:logs/gc.log-Dsun.rmi.dgc.server.gcInterval=3600000-Dsun.rmi.dgc.client.gcInterval=3600000-Dsun.rmi.server.exceptionTrace=true 修改点： 在上面的基础上调整Xmn大小到36M，设置CMSInitiatingOccupancyFraction=73。 Dden区与Survivor区大小都增加到12M，通过CMSInitiatingOccupancyFraction计算公式,计算得出value为73是，可以避免promotion faild问题，同时满足堆内存监控报警值在80%：内存大小128M*80%=102.4M 102.4M-36M=66.4M(老生代达到此值报警） 老生代达到67.15M（92M*0.73）将发生Full GC，所以在老生代大小达到66.4M时也就是WARN报警时将很有可能出现Full GC。 增大了Eden和Survivor区的值，会减小YGC的次数，但由于空间变大理论上也会相应的增加YGC的时间，不过由于新生代本身就很小（才36M）这点儿变化可以忽略掉。实际的监控值显示YGC的时间在4-5ms之间。是可以接受范围。 SurvivorRatio 这个值还得在仔细考虑下,有待优化中 网上某个牛人的配置 :每天几百万pv一点问题都没有，网站没有停顿12345678910111213141516171819202122232425262728$JAVA_ARGS.=&quot;-Dresin.home=$SERVER_ROOT-server-Xms6000M-Xmx6000M-Xmn500M-XX:PermSize=500M-XX:MaxPermSize=500M-XX:SurvivorRatio=65536-XX:MaxTenuringThreshold=0-Xnoclassgc-XX:+DisableExplicitGC-XX:+UseParNewGC-XX:+UseConcMarkSweepGC-XX:+UseCMSCompactAtFullCollection-XX:CMSFullGCsBeforeCompaction=0-XX:+CMSClassUnloadingEnabled-XX:-CMSParallelRemarkEnabled-XX:CMSInitiatingOccupancyFraction=90-XX:SoftRefLRUPolicyMSPerMB=0-XX:+PrintClassHistogram-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+PrintHeapAtGC-Xloggc:log/gc.log&quot;; 说明一下， -XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0就是去掉了救助空间；-Xnoclassgc禁用类垃圾回收，性能会高一点；-XX:+DisableExplicitGC禁止System.gc()，免得程序员误调用gc方法影响性能；-XX:+UseParNewGC，对年轻代采用多线程并行回收，这样收得快；带CMS参数的都是和并发回收相关的，不明白的可以上网搜索；CMSInitiatingOccupancyFraction，这个参数设置有很大技巧，基本上满足(Xmx-Xmn)(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn就不会出现promotion failed。在我的应用中Xmx是6000，Xmn是500，那么Xmx-Xmn是5500兆，也就是年老代有5500兆，CMSInitiatingOccupancyFraction=90说明年老代到90%满的时候开始执行对年老代的并发垃圾回收（CMS），这时还剩10%的空间是550010%=550兆，所以即使Xmn（也就是年轻代共500兆）里所有对象都搬到年老代里，550兆的空间也足够了，所以只要满足上面的公式，就不会出现垃圾回收时的promotion failed；SoftRefLRUPolicyMSPerMB这个参数我认为可能有点用，官方解释是softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap，我觉得没必要等1秒；123456789101112131415161718192021222324-Xmx4000M-Xms4000M-Xmn600M-XX:PermSize=500M-XX:MaxPermSize=500M-Xss256K-XX:+DisableExplicitGC-XX:SurvivorRatio=1-XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled-XX:+UseCMSCompactAtFullCollection-XX:CMSFullGCsBeforeCompaction=0-XX:+CMSClassUnloadingEnabled-XX:LargePageSizeInBytes=128M-XX:+UseFastAccessorMethods-XX:+UseCMSInitiatingOccupancyOnly-XX:CMSInitiatingOccupancyFraction=80-XX:SoftRefLRUPolicyMSPerMB=0-XX:+PrintClassHistogram-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+PrintHeapAtGC-Xloggc:log/gc.log 改进方案：上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio=1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。某网友:123456789101112131415161718192021222324252627282930$JAVA_ARGS.=&quot;-Dresin.home=$SERVER_ROOT-server-Xmx3000M-Xms3000M-Xmn600M-XX:PermSize=500M-XX:MaxPermSize=500M-Xss256K-XX:+DisableExplicitGC-XX:SurvivorRatio=1-XX:+UseConcMarkSweepGC-XX:+UseParNewGC-XX:+CMSParallelRemarkEnabled-XX:+UseCMSCompactAtFullCollection-XX:CMSFullGCsBeforeCompaction=0-XX:+CMSClassUnloadingEnabled-XX:LargePageSizeInBytes=128M-XX:+UseFastAccessorMethods-XX:+UseCMSInitiatingOccupancyOnly-XX:CMSInitiatingOccupancyFraction=70-XX:SoftRefLRUPolicyMSPerMB=0-XX:+PrintClassHistogram-XX:+PrintGCDetails-XX:+PrintGCTimeStamps-XX:+PrintHeapAtGC-Xloggc:log/gc.log&quot;; 64位jdk参考设置，年老代涨得很慢，CMS执行频率变小，CMS没有停滞，也不会有promotion failed问题，内存回收得很干净 总结 打印并分析进行垃圾回收的时间，内容，具体数值。如果有OOM导出OOM时内存使用情况。 分析垃圾回收时候的数据信息，（年轻区，年老区，方法区）查看OOM时内存使用情况。 优化的权重顺序可以按照，FullGc&gt;频繁YGC]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM系列：JVM内存优化建议]]></title>
    <url>%2F2017%2F10%2F09%2Fjava%2Fjvm%2Fjvm_2%2F</url>
    <content type="text"><![CDATA[优化原则JAVA程序在运行时 加快GC速度 减少FullGC 减少停顿 杜绝GC出错 GC优化的策略本质上JVM运行中通过参数的变换调和达到运行平衡的过程。it is an art. 经验配置 垃圾搜集器新生代收集器：有Serial收集器、ParNew收集器、Parallel Scavenge收集器老生代收集器：Serial Old收集器、Parallel Old收集器、CMS收集器、G1收集器以上所有的垃圾收集器都会发生STW，只不过FGC的STW时间更长。 常用搜集器： CMSGC CMS(Concurrent Mark-Sweep)是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合，因此我们又叫它低延迟垃圾收集器。在启动JVM参数加上-XX:+UseConcMarkSweepGC ，这个参数表示对于老年代的回收采用CMS，注意此时新生代默认使用的是ParNew。CMS采用的基础算法是：标记—清除。 MSCGC vs CMSGC 和普通序列化整理（MSC）区别在于有三个mark阶段（实际上还有个预清理过程，但对于解释清楚CMSGC没有帮助就忽略了）。CMSGC的精髓在于因为做到了不STW的情况下进行mark，我们得到了更短的总STW时间，代价是因为并行mark产生了『脏数据』即在mark的同时又生成了需要mark的对象，我们必须再进行一次STW，并收尾（remark）。同时，我们要注意到得到更短的STW的同时，我们牺牲了系统吞吐量，CMSGC总吞吐量比ParOld要更低。 G1GC 作为最新的垃圾收集器，有可能在jdk9中成为默认的垃圾收集器。主要思路是将新生代老生代进一步分为多个region，每次gc可以针对部分region而不是整个堆内存。由此可以降低stw的单次最长时间，代价是可能在总时间上会更高。G1GC让系统在整体吞吐量略降的情况下变得更加平滑稳定。 响应时间优先的应用 年轻代选择尽可能设大,直到接近系统的最低响应时间限制(根据实际情况选择).在此种情况下,年轻代收集发生的频率也是最小的.同时,减少到达年老代的对象. 年老代选择年老代使用并发收集器,所以其大小需要小心设置,一般要考虑并发会话率和会话持续时间等一些参数.如果堆设置小了,可以会造成内存碎片,高回收频率以及应用暂停而使用传统的标记清除方式;如果堆大了,则需要较长的收集时间.最优化的方案,一般需要参考以下数据获得: 并发垃圾收集信息. 持久代并发收集次数. 传统GC信息. 花在年轻代和年老代回收上的时间比例。吞吐量优先的应用 年轻代选择尽可能的设置大,可能到达Gbit的程度.因为对响应时间没有要求,垃圾收集可以并行进行,一般适合8CPU以上的应用.避免设置过小.当新生代设置过小时会导致: YGC次数更加频繁 可能导致YGC对象直接进入旧生代,如果此时旧生代满了,会触发FGC. 年老代选择一般吞吐量优先的应用都有一个很大的年轻代和一个较小的年老代.这样可以尽可能回收掉大部分短期对象,减少中期的对象,而年老代尽存放长期存活对象. 碎片问题因为年老代的并发收集器使用标记,清除算法,所以不会对堆进行压缩.当收集器回收时,他会把相邻的空间进行合并,这样可以分配给较大的对象.但是,当堆空间较小时,运行一段时间以后,就会出现”碎片”,如果并发收集器找不到足够的空间,那么并发收集器将会停止,然后使用传统的标记,清除方式进行回收.如果出现”碎片”,可能需要进行如下配置: -XX:+UseCMSCompactAtFullCollection:使用并发收集器时,开启对年老代的压缩.-XX:CMSFullGCsBeforeCompaction=0:上面配置开启的情况下,这里设置多少次Full GC后,对年老代进行压缩 promotion failed问题可能是两种原因产生： 1. 第一个原因是救助空间不够，救助空间里的对象还不应该被移动到年老代，但年轻代又有很多对象需要放入救助空间。 2. 第二个原因是年老代没有足够的空间接纳来自年轻代的对象；这两种情况都会转向Full GC，网站停顿时间较长。 用64位操作系统，Linux下64位的jdk比32位jdk要慢一些，但是吃得内存更多，吞吐量更大XMX和XMS设置一样大，MaxPermSize和MinPermSize设置一样大，这样可以减轻伸缩堆大小带来的压力 使用CMS的好处是用尽量少的新生代，经验值是128M－256M， 老生代利用CMS并行收集，这样能保证系统低延迟的吞吐效率。 cms的收集停顿时间非常的短，2G的内存， 大约20－80ms的应用程序停顿时间系统停顿的时候可能是GC的问题也可能是程序的问题，多用jmap和jstack查看，或者killall -3 java，然后查看java控制台日志，能看出很多问题。 如果用了缓存，那么年老代应该大一些，缓存的HashMap不应该无限制长，建议采用LRU算法的Map做缓存，LRUMap的最大长度也要根据实际情况设定。 采用并发回收时，年轻代小一点，年老代要大，因为年老大用的是并发回收，即使时间长点也不会影响其他程序继续运行，网站不会停顿JVM参数的设置(特别是 –Xmx –Xms –Xmn -XX:SurvivorRatio -XX:MaxTenuringThreshold等参数的设置没有一个固定的公式，需要根据PV old区实际数据 YGC次数等多方面来衡量。为了避免promotion faild可能会导致xmn设置偏小，也意味着YGC的次数会增多，处理并发访问的能力下降等问题。每个参数的调整都需要经过详细的性能测试，才能找到特定应用的最佳配置。 解决方方案一： 第一个原因最终解决办法是去掉救助空间，设置-XX:SurvivorRatio=65536 -XX:MaxTenuringThreshold=0即可， 第二个原因我的解决办法是设置CMSInitiatingOccupancyFraction为某个值（假设70），这样年老代空间到70%时就开始执行CMS，年老代有足够的空间接纳来自年轻代的对象。解决方案一的改进方案： 又有改进了，上面方法不太好，因为没有用到救助空间，所以年老代容易满，CMS执行会比较频繁。我改善了一下，还是用救助空间，但是把救助空间加大，这样也不会有promotion failed。具体操作上，32位Linux和64位Linux好像不一样，64位系统似乎只要配置MaxTenuringThreshold参数，CMS还是有暂停。为了解决暂停问题和promotion failed问题，最后我设置-XX:SurvivorRatio=1 ，并把MaxTenuringThreshold去掉，这样即没有暂停又不会有promotoin failed，而且更重要的是，年老代和永久代上升非常慢（因为好多对象到不了年老代就被回收了），所以CMS执行频率非常低，好几个小时才执行一次，这样，服务器都不用重启了。 -Xmx4000M -Xms4000M -Xmn600M -XX:PermSize=500M -XX:MaxPermSize=500M -Xss256K -XX:+DisableExplicitGC -XX:SurvivorRatio=1 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -XX:+CMSParallelRemarkEnabled -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 -XX:+CMSClassUnloadingEnabled -XX:LargePageSizeInBytes=128M -XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -XX:CMSInitiatingOccupancyFraction=80 -XX:SoftRefLRUPolicyMSPerMB=0 -XX:+PrintClassHistogram -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC -Xloggc:log/gc.log CMSInitiatingOccupancyFraction值与Xmn的关系公式 上面介绍了promontion faild产生的原因是EDEN空间不足的情况下将EDEN与From survivor中的存活对象存入To survivor区时,To survivor区的空间不足，再次晋升到old gen区，而old gen区内存也不够的情况下产生了promontion faild从而导致full gc.那可以推断出：eden+from survivor &lt; old gen区剩余内存时，不会出现promontion faild的情况，即：(Xmx-Xmn)*(1-CMSInitiatingOccupancyFraction/100)&gt;=(Xmn-Xmn/(SurvivorRatior+2)) 进而推断出： CMSInitiatingOccupancyFraction &lt;=((Xmx-Xmn)-(Xmn-Xmn/(SurvivorRatior+2)))/(Xmx-Xmn)*100 例如： 当xmx=128 xmn=36 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-36)-(36-36/(1+2)))/(128-36)*100 =73.913 当xmx=128 xmn=24 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((128.0-24)-(24-24/(1+2)))/(128-24)*100=84.615… 当xmx=3000 xmn=600 SurvivorRatior=1时 CMSInitiatingOccupancyFraction&lt;=((3000.0-600)-(600-600/(1+2)))/(3000-600)*100=83.33 CMSInitiatingOccupancyFraction低于70% 需要调整xmn或SurvivorRatior值。 令： 网上一童鞋推断出的公式是：:(Xmx-Xmn)*(100-CMSInitiatingOccupancyFraction)/100&gt;=Xmn 这个公式个人认为不是很严谨，在内存小的时候会影响xmn的计算。 参考：http://www.cnblogs.com/redcreen/archive/2011/05/05/2038331.htmlhttp://www.jianshu.com/p/c9ac99b87d56]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM系列：基础概念]]></title>
    <url>%2F2017%2F09%2F28%2Fjava%2Fjvm%2Fjvm_1%2F</url>
    <content type="text"><![CDATA[jvm内存区域分三种：栈，堆，方法区。设计上Java还可以使用到直接内存，在Java NIO包里使用DirectBuffer可以显示地调用申请堆外内存。 栈 栈是基于线程执行而言的，它描述的是一个线程执行的流程路线。并且在整个业务执行流程中需要用到的各种局部变量（简单类型保存值，对象保存地址），以及基本类型。 这个路线可以认为是通过方法区的程序执行流程，按照帧（Stack Frame)的方式一压入内存，在JVM内存 看起来就是Stack的存储。 栈的配置 JVM通过 -XSS指定配置每个线程所拥有栈大小。默认值随着虚拟机版本以及操作系统影响，官网上指定： In Java SE 6, the default on Sparc is 512k in the 32-bit VM, and 1024k in the 64-bit VM. On x86 Solaris/Linux it is 320k in the 32-bit VM and 1024k in the 64-bit VM. 。 栈的大小直接影响可以创建的线程数量。 线程数 = （系统空闲内存-堆内存（-Xms, -Xmx）- perm方法区内存(-XX:MaxPermSize)) / 线程栈大小(-Xss) 如上我们可知栈设置越小，可以创建的线程数就越多，但是也是有限制的。限制有两个方便： 操作系统配置也可以限制数量。例如ubuntu里/proc/sys/kernel/threads-max设置最大线程数。 异常 调用链太长，栈不够时会抛出StackOverFlow。一般是发生了递归而产生。 堆 堆是JVM最大的内存部分，它负责存放对象实例，JVM所有对象实例都在这里。（变量，对象属性但是不包括方法里的简单类型变量）。它保存了执行所需要的 各种东西。 堆的配置 参数名称 含义 默认值 其他 -Xms 初始堆大小 物理内存的1/64(&lt;1GB) 默认(MinHeapFreeRatio参数可以调整)空余堆内存小于40%时，JVM就会增大堆直到-Xmx的最大限制. -Xmx 最大堆大小 物理内存的1/4(&lt;1GB) 默认(MaxHeapFreeRatio参数可以调整)空余堆内存大于70%时，JVM会减少堆直到 -Xms的最小限制 -Xmn 年轻代大小 (1.4or lator) 注意：此处的大小是（eden+ 2 survivor space).与jmap -heap中显示的New gen是不同的。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小.增大年轻代后,将会减小年老代大小.此值对系统性能影响较大,Sun官方推荐配置为整个堆的3/8 -XX:NewSize 设置年轻代大小(for 1.3/1.4) -XX:MaxNewSize 年轻代最大值(for 1.3/1.4) -XX:PermSize 设置持久代(perm gen)初始值 物理内存的1/64 -XX:MaxPermSize 设置持久代最大值 物理内存的1/4 -Xss 每个线程的堆栈大小 JDK5.0以后每个线程堆栈大小为1M,以前每个线程堆栈大小为256K.更具应用的线程所需内存大小进行 调整.在相同物理内存下,减小这个值能生成更多的线程.但是操作系统对一个进程内的线程数还是有限制的,不能无限生成,经验值在3000~5000左右 一般小的应用，如果栈不很深， 应该是128k够用的大的应用建议使用256k。这个选项对性能影响比较大，需要严格的测试。（校长）和threadstacksize选项解释很类似,官方文档似乎没有解释,在论坛中有这样一句话:”-Xss is translated in a VM flag named ThreadStackSize”一般设置这个值就可以了 -XX:ThreadStackSize Thread Stack Size (0 means use default stack size) [Sparc: 512; Solaris x86: 320 (was 256 prior in 5.0 and earlier); Sparc 64 bit: 1024; Linux amd64: 1024 (was 0 in 5.0 and earlier); all others 0.] -XX:NewRatio 年轻代(包括Eden和两个Survivor区)与年老代的比值(除去持久代) -XX:NewRatio=4表示年轻代与年老代所占比值为1:4,年轻代占整个堆栈的1/5 Xms=Xmx并且设置了Xmn的情况下，该参数不需要进行设置。 -XX:SurvivorRatio Eden区与Survivor区的大小比值 设置为8,则两个Survivor区与一个Eden区的比值为2:8,一个Survivor区占整个年轻代的1/10 -XX:LargePageSizeInBytes 内存页的大小 不可设置过大， 会影响Perm的大小 =128m -XX:+UseFastAccessorMethods 原始类型的快速优化 -XX:+DisableExplicitGC 关闭System.gc() 这个参数需要严格的测试 -XX:MaxTenuringThreshold 垃圾最大年龄 如果设置为0的话,则年轻代对象不经过Survivor区,直接进入年老代. 对于年老代比较多的应用,可以提高效率.如果将此值设置为一个较大值,则年轻代对象会在Survivor区进行多次复制,这样可以增加对象再年轻代的存活 时间,增加在年轻代即被回收的概率该参数只有在串行GC时才有效. -XX:+AggressiveOpts 加快编译 -XX:+UseBiasedLocking 锁机制的性能改善 -Xnoclassgc 禁用垃圾回收 -XX:SoftRefLRUPolicyMSPerMB 每兆堆空闲空间中SoftReference的存活时间 1s softly reachable objects will remain alive for some amount of time after the last time they were referenced. The default value is one second of lifetime per free megabyte in the heap -XX:PretenureSizeThreshold 对象超过多大是直接在旧生代分配 0 单位字节 新生代采用Parallel Scavenge GC时无效另一种直接在旧生代分配的情况是大的数组对象,且数组中无外部引用对象. -XX:TLABWasteTargetPercent TLAB占eden区的百分比 1% -XX:+CollectGen0First FullGC时是否先YGC false 并行收集器相关参数 参数名称 含义 默认值 其他 -XX:+UseParallelGC Full GC采用parallel MSC 选择垃圾收集器为并行收集器.此配置仅对年轻代有效.即上述配置下,年轻代使用并发收集,而年老代仍旧使用串行收集.(此项待验证) -XX:+UseParNewGC 设置年轻代为并行收集 可与CMS收集同时使用 JDK5.0以上,JVM会根据系统配置自行设置,所以无需再设置此值 -XX:ParallelGCThreads 并行收集器的线程数 此值最好配置与处理器数目相等 同样适用于CMS -XX:+UseParallelOldGC 年老代垃圾收集方式为并行收集(Parallel Compacting) 这个是JAVA 6出现的参数选项 -XX:MaxGCPauseMillis 每次年轻代垃圾回收的最长时间(最大暂停时间) 如果无法满足此时间,JVM会自动调整年轻代大小,以满足此值. -XX:+UseAdaptiveSizePolicy 自动选择年轻代区大小和相应的Survivor区比例 设置此选项后,并行收集器会自动选择年轻代区大小和相应的Survivor区比例,以达到目标系统规定的最低相应时间或者收集频率等,此值建议使用并行收集器时,一直打开. -XX:GCTimeRatio 设置垃圾回收时间占程序运行时间的百分比 公式为1/(1+n) -XX:+ScavengeBeforeFullGC Full GC前调用YGC true Do young generation GC prior to a full GC. (Introduced in 1.4.1.) CMS相关参数 参数名称 含义 默认值 其他 -XX:+UseConcMarkSweepGC 使用CMS内存收集 测试中配置这个以后,-XX:NewRatio=4的配置失效了,原因不明.所以,此时年轻代大小最好用-Xmn设置.??? -XX:+AggressiveHeap 试图是使用大量的物理内存 长时间大内存使用的优化，能检查计算资源（内存， 处理器数量）至少需要256MB内存大量的CPU／内存， （在1.4.1在4CPU的机器上已经显示有提升） -XX:CMSFullGCsBeforeCompaction 多少次后进行内存压缩 由于并发收集器不对内存空间进行压缩,整理,所以运行一段时间以后会产生”碎片”,使得运行效率降低.此值设置运行多少次GC以后对内存空间进行压缩,整理. -XX:+CMSParallelRemarkEnabled 降低标记停顿 -XX+UseCMSCompactAtFullCollection 在FULL GC的时候， 对年老代的压缩 CMS是不会移动内存的， 因此， 这个非常容易产生碎片， 导致内存不够用， 因此， 内存的压缩这个时候就会被启用。 增加这个参数是个好习惯。 可能会影响性能,但是可以消除碎片 -XX:+UseCMSInitiatingOccupancyOnly 使用手动定义初始化定义开始CMS收集 禁止hostspot自行触发CMS GC -XX:CMSInitiatingOccupancyFraction=70 使用cms作为垃圾回收 使用70％后开始CMS收集 92 为了保证不出现promotion failed(见下面介绍)错误,该值的设置需要满足以下公式CMSInitiatingOccupancyFraction计算公式 -XX:CMSInitiatingPermOccupancyFraction 设置Perm Gen使用到达多少比率时触发 92 -XX:+CMSIncrementalMode 设置为增量模式 用于单CPU情况 -XX:+CMSClassUnloadingEnabled 辅助信息 参数名称 含义 默认值 其他 -XX:+PrintGC 输出形式:[GC 118250K-&gt;113543K(130112K), 0.0094143secs][Full GC 121376K-&gt;10414K(130112K), 0.0650971 secs] -XX:+PrintGCDetails 输出形式:[GC [DefNew: 8614K-&gt;781K(9088K), 0.0123035 secs] 118250K-&gt;113543K(130112K), 0.0124633 secs][GC [DefNew: 8614K-&gt;8614K(9088K), 0.0000665 secs][Tenured: 112761K-&gt;10414K(121024K), 0.0433488 secs] 121376K-&gt;10414K(130112K), 0.0436268 secs] -XX:+PrintGCTimeStamps -XX:+PrintGC:PrintGCTimeStamps 可与-XX:+PrintGC -XX:+PrintGCDetails混合使用 输出形式:11.851: [GC 98328K-&gt;93620K(130112K), 0.0082960 secs] -XX:+PrintGCApplicationStoppedTime 打印垃圾回收期间程序暂停的时间.可与上面混合使用 输出形式:Total time for which application threads were stopped: 0.0468229 seconds -XX:+PrintGCApplicationConcurrentTime 打印每次垃圾回收前,程序未中断的执行时间.可与上面混合使用 输出形式:Application time: 0.5291524 seconds -XX:+PrintHeapAtGC 打印GC前后的详细堆栈信息 -Xloggc:filename 把相关日志信息记录到文件以便分析.与上面几个配合使用 -XX:+PrintClassHistogram garbage collects before printing the histogram. -XX:+PrintTLAB 查看TLAB空间的使用情况 XX:+PrintTenuringDistribution 查看每次minor GC后新的存活周期的阈值 Desired survivor size 1048576 bytes, new threshold 7 (max 15) new threshold 7即标识新的存活周期的阈值为7。 方法区 又叫静态区，跟堆一样，被所有的线程共享。方法区包含所有的class和static变量；方法区中包含的都是在程序中永远的唯一的元素。特别说明的是枚举 是存放在方法区，而单例是某个类在内存中唯一的对象实例，是存放在堆中的。 配置方法 -XX:PermSize=10M -XX:MaxPermSize=10M 值得一提的是 JAVA8 将方法区 改成了 MateSpace (元数据区。) 同时 PerSize MaxPermSize参数也移除了。带来了几个新的参数： -XX:MetaspaceSize，class metadata的初始空间配额，以bytes为单位，达到该值就会触发垃圾收集进行类型卸载，同时GC会对该值进行调整：如果释放了大量的空间，就适当的降低该值；如果释放了很少的空间，那么在不超过MaxMetaspaceSize（如果设置了的话），适当的提高该值。 -XX:MaxMetaspaceSize，可以为class metadata分配的最大空间。默认是没有限制的。 -XX:MinMetaspaceFreeRatio,在GC之后，最小的Metaspace剩余空间容量的百分比，减少为class metadata分配空间导致的垃圾收集 -XX:MaxMetaspaceFreeRatio,在GC之后，最大的Metaspace剩余空间容量的百分比，减少为class metadata释放空间导致的垃圾收集XX:MaxMetaspaceSize总结图]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm调优</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis简介]]></title>
    <url>%2F2017%2F09%2F26%2Fredis%2Fredis_2%2F</url>
    <content type="text"><![CDATA[Redis简介 Redis 是一个非常快速的非关系内存型数据库。Redis非常有区分度的是它提供的5种不同类型的数据结构，其数据结构是有针对地为解决问题而生的数据结构，区分于其他数据库的一个显著特点。可以说，Redis核心问题和功能都围绕着五种数据结构展开的，另外，它方便的扩展功能，可以支持到数百GB级数据。 与其他数据库和软件的对比 Redis的特点决定了它在存储工具里的定位，它经常被用来与其他数据库进行对比。这里，我们介于内存键值存储 Memcached 与MongoDB对Redis进行一次比较。 名称 类型 存储 查询 附加功能 Redis 内存存储（in-memmory)的非关系数据库 字符串,列表，集合，散列表，有序集合 每种数据类型都有自己的专属命令，还有批操作和不完整的事务支持 发布与订阅，主从复制，持久化，脚本 Memcached 使用内存存储的键值缓存 键值之间的映射 创建，读取，更新删除等命令 多线程服务支持 MongoDB 硬盘存储的非关系文档存储 每个数据库可以包含多个个表，每个表包含多个schema 的BSON文档 更新，读取，删除，条件查询等命令 支持map-reduce操作，主从复制，分片，空间索引（spatial index） 使用Redis的理由Redis之于缓存界：使用memcached 时，没有原生的列表结构，只能用Append命令将数据添加到已有字符串末尾。可以认为那个字符串就是一个列表。但是删除这些就比较困难了。memcached采用的办法是通过黑名单来隐藏列表里的元素，从而避免对元素进行读取，更新，写入。相反地，Redis的LIST和SET允许用户直接添加或者删除元素。 Redis之于数据库：当数据库用于存储长期数据报告，报表。并将这些数据作为固定时间范围内聚合。数据库的做法是：将各个行插入一个报表中，通过扫描这些行进行聚合数据。这样就要频繁地对表里数据进行 读，写。Redis可以使用原子的INCR命令来进行聚合计算。并且Redis存储在内存里。并且查询不通过数据库的分析器，查询优化器等，所以对Redis存储的数据行随机写的速度是非常迅速的。Redis之于NoSql数据库：避免写入不必要的临时数据。免去了临时数据进行扫描删除的麻烦。可以改上程序的性能。]]></content>
  </entry>
  <entry>
    <title><![CDATA[HttpURLConnection Post请求自动重传机制]]></title>
    <url>%2F2017%2F09%2F26%2Fjava%2Fjava_1%2F</url>
    <content type="text"><![CDATA[背景故事 之前负责的一个商城项目，需要从供应商库进行订单下单同步，服务器间通讯通过http请求。 加密方式采用DES加密方式。在运行初期一切正常，几个月后供应商发现有重复订单存在，而客户端这边接收到异常生成订单异常信息，订单生成不同步。供应商的处理逻辑我们无从得知，只能从自身角度思考为什么会有这种问题，在排除了一系列原因后，定位到一个问题。那就是 HttpURLConnection的post请求重发机制。 场景再现Http请求是通过HttpUrlConnection封装的一套Java请求客户端 部分源码如下：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273 //请求的header如下 protected Map&lt;String, Object&gt; getDefaultHeaders() &#123; Map&lt;String, Object&gt; defaultHeaders = new HashMap&lt;&gt;(); defaultHeaders.put("Accept", "*/*"); defaultHeaders.put("Connection", "Keep-Alive"); defaultHeaders.put("User-Agent", "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1)"); defaultHeaders.put("Accept-Charset", "utf-8"); defaultHeaders.put("Content-Type", "application/x-www-form-urlencoded;charset=utf-8"); headers = defaultHeaders; return headers; &#125;//初始化 httpConnectionprotected void initConnection() throws IOException &#123; if ("POST".equals(method)) &#123; this.url = new URL(getUrl); this.postJson = JsonUtil.toJson(param); &#125; else &#123; this.url = new URL(getUrl + urlParams); &#125; httpConnection = (HttpURLConnection) url.openConnection(); for (String keyset : headers.keySet()) &#123; httpConnection.setRequestProperty(keyset, headers.get(keyset).toString()); &#125; /** * 然后把连接设为输出模式。URLConnection通常作为输入来使用，比如下载一个Web页。 * 通过把URLConnection设为输出，你可以把数据向你个Web页传送。： */ httpConnection.setRequestMethod(method); httpConnection.setUseCaches(false); if ("POST".equals(method)) &#123; httpConnection.setDoOutput(true); &#125; else &#123; httpConnection.setDoOutput(true); &#125; httpConnection.setDoInput(true); &#125; //执行Http请求 public String doRequest() &#123; this.toUrlParams(); OutputStreamWriter out = null; try &#123; this.initConnection(); // 一旦发送成功，用以下方法就可以得到服务器的回应： String sTotalString; InputStream urlStream; out = new OutputStreamWriter(httpConnection.getOutputStream(), charSet); if (method.equals("POST")) &#123; out.write(this.postJson); //向页面传递数据。post的关键所在！ &#125; // remember to clean up out.flush(); urlStream = httpConnection.getInputStream(); logger.debug("连接状态:" + urlStream.available()); //new InputStreamReader(l_urlStream,) sTotalString = IOUtil.in2Str(urlStream, charSet); return sTotalString; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new SystemException(e); &#125; finally &#123; if (out != null) &#123; try &#123; out.close(); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new SystemException(e); &#125; &#125; httpConnection.disconnect(); &#125; &#125; Java代码调用doRequest通过HttpUrlConnection模拟一个Post请求。结果服务端会收到两次请求。 原因分析HttpURLConnection 采用 Sun 私有的一个 HTTP 协议实现类： HttpClient.java 123456789101112131415161718192021222324252627282930313233343536public boolean parseHTTP(MessageHeader var1, ProgressSource var2, HttpURLConnection var3) throws IOException &#123; try &#123; this.serverInput = this.serverSocket.getInputStream(); if(this.capture != null) &#123; this.serverInput = new HttpCaptureInputStream(this.serverInput, this.capture); &#125; this.serverInput = new BufferedInputStream(this.serverInput); return this.parseHTTPHeader(var1, var2, var3); &#125; catch (SocketTimeoutException var6) &#123; if(this.ignoreContinue) &#123; this.closeServer(); &#125; throw var6; &#125; catch (IOException var7) &#123; this.closeServer(); this.cachedHttpClient = false; if(!this.failedOnce &amp;&amp; this.requests != null) &#123; this.failedOnce = true; if(!this.getRequestMethod().equals("CONNECT") &amp;&amp; !this.streaming &amp;&amp; (!var3.getRequestMethod().equals("POST") || retryPostProp)) &#123; this.openServer(); if(this.needsTunneling()) &#123; MessageHeader var5 = this.requests; var3.doTunneling(); this.requests = var5; &#125; this.afterConnect(); this.writeRequests(this.requests, this.poster); return this.parseHTTP(var1, var2, var3); &#125; &#125; throw var7; &#125; &#125; 当发生IOException就会执行判断是否进行重试。failedOnce 默认是 false，表示是否已经失败过一次了。这也就限制了最多发送 2 次请求。var3 是请求信息retryPostProp 默认是 true ，可以通过命令行参数( -Dsun.net.http.retryPost=false )来指定值。streaming：默认 false 。 true if we are in streaming mode (fixed length or chunked) 。 bug链接：http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6427251这个Bug很早就有了，归根结底原因就是sun提供的实现与Http对于Post请求的规范有不同。Http协议里Post不是幂等的，不能进行重试。 解决方案 使用Apache Client请求 修改JVM启动参数 添加：-Dsun.net.http.retryPost=false http://bugs.java.com/bugdatabase/view_bug.do?bug_id=6427251 总结心得 http协议方面：http规定的部分是规范，实现有千种方法。有的符合协议，有的又有所区别，在对接过程中，指定接入方式，形成书面文档规范。有利于后续问题职责归属。 在寻找问题方面，无法完整获取所有信息时，从已掌握的信息出发，避免任何一点得出结论的依据。]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>http</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr 的edismax插件扩展方式]]></title>
    <url>%2F2017%2F09%2F19%2Fsearch%2Fsolr_2%2F</url>
    <content type="text"><![CDATA[前言 solr 通过插件的方式实现对edismax的支持。在熟悉了solr插件的执行流程后我，我们也可以对solr插件功能进行定制。实现更加强大的功能。 类图 ExtendedDismaxQParserPlugin NamedListInitializedPlugin 作用NameList作为solr存储map的方式，可以看成是一个key value容器。NameListInitializedPlugin是可以通过NameList初始化的插件。只有一个init方法。接收NameList参数。 SolrInfoMBean 作用提供Solr后台基础信息的Bean接口。3：QparserPlugin所有插件的父类，定义了默认方法，保存声明了所有solr已经实现的plugin。 ExtendedDismaxQparser ExtendDismaxQparser描述edismax支持的配置和方法，集合了edismax所需功能的各种操作。并创建 ExtendedSolrDismaxQueryParser,为其提供职责更为简明的操作环境。是edismax语法支持类。 ExtendSolrQueryParser QueryBuilderlucene 提供用于创建查询器的工厂类 可以被当做自定义解析器的子类，使得查询解析器更容易地集成到分析链中。生成查询可以定制化。所有默认lucene提供的query在这里创建 SolrQueryParserBaseSolr继承自QueryBuilder的类。作为Solr标准查询解析器的父类。Solr对lucene的扩展，加入了 MagicFieldName RawQuery等支持。初始化时读入 schema配置。可以修改这个类来扩展schema标签功能。 QueryParser默认的query解析器 SolrQueryParserSolr’s 的默认查询解析器schema驱动的经典lucene查询解析方式。 ExtendedSolrQueryParser作为ExtendedSolrQParser的内部类存在。实施最终的查询解析。 请求过程SolrDispatchFilter（doFilter,execute） -&gt;SolrCore.execute -&gt;RequestHandlerBase.handleRequest -&gt;SearchHandler.handleRequestBody//有可能执行多个Component//query,facet,group等等，这里每个查询特性对应每个SearchComponent-&gt;QueryComponent.process -&gt;SolrIndexSearcher(search,getDocListC) 描述edismax请求到 SearchHandler 分析调用的SearchComponent链，其中QueryComponent 通过defType选择 插件ExtendedDismaxQParserPlugin 创建queryParser初始化插件paraer QueryComponent prepare调用QParser rqparser = QParser.getParser(rankQueryString, defType, req);先解析出参数语法包含的解析器信息 QueryComponent 源码部分 关键方法: 123456789101112131415161718192021222324252627282930313233343536public static QParser getParser(String qstr, String defaultParser, SolrQueryRequest req) throws SyntaxError &#123; ... //存在字符 &#123;！需要分析参数需要什么样的查询解析器选定信息 if (qstr != null &amp;&amp; qstr.startsWith(QueryParsing.LOCALPARAM_START)) &#123; localParams = new ModifiableSolrParams(); localParamsEnd = QueryParsing.parseLocalParams(qstr, 0, localParams, globalParams); String val = localParams.get(QueryParsing.V); if (val != null) &#123; // val was directly specified in localParams via v=&lt;something&gt; or v=$arg valFollowedParams = false; &#125; else &#123; // use the remainder of the string as the value valFollowedParams = true; val = qstr.substring(localParamsEnd); localParams.set(QueryParsing.V, val); &#125; &#125; ... //localParams 语法解析出来的localParams是否需要特殊的解析器来解析查询。 if (localParams == null) &#123; parserName = defaultParser; &#125; else &#123; //显示的defType与解析出来的信息一起判断优先级来选定解析器名称 parserName = localParams.get(QueryParsing.TYPE,defaultParser); qstr = localParams.get("v"); &#125; parserName = parserName==null ? QParserPlugin.DEFAULT_QTYPE : parserName; //确定后通过名称获取解析插件 QParserPlugin qplug = req.getCore().getQueryPlugin(parserName); QParser parser = qplug.createParser(qstr, localParams, req.getParams(), req); ... return parser; &#125; 对edisMax查询操作，QueryComponet返回了 ExtendDismaxQparser 作为查询解析器作为后续的查询支持 。默认解析器 lucene 扩展功能扩展步骤：1:新建 XXPlugin 继承 QParserPlugin 实现方法： 123456//定义插件名称，用于配置指定public static String NAME = "XXXqueryPlus";@Overridepublic QParser createParser(String qstr, SolrParams localParams, SolrParams params, SolrQueryRequest req) &#123; return new XXXQParser(qstr, localParams, params, req);&#125; 2:创建XXXQParser继承QParser, XXXSolrQueryParser继承SolrQueryParser可以采用组合方式，也可以采用内部类的方式。在XXXQParser里重写 parser()方法12345678@Overridepublic Query parse() throws SyntaxError &#123; //返回具体的 查询Query //可以结合XXXSolrQueryParser方法返回. //this.parser() --&gt; XXSolrQueryParser.parser()。 //提交给 SolrQuerybase执行，只要根据需要重写 SolrQueryBase里的 // protected Query getFieldQuery(String field, String queryText, //boolean quoted, boolean raw)方法即可。&#125; 3:solrConfig里配置123456&lt;requestHandler name="standard" class="solr.SearchHandler" default="true"&gt; &lt;lst name="defaults"&gt; &lt;str name="defType"&gt;XXXqueryPlus&lt;/str&gt; &lt;/lst&gt;&lt;/requestHandler&gt;]]></content>
      <categories>
        <category>搜索</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>edismax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据结构介绍]]></title>
    <url>%2F2017%2F09%2F18%2Fredis%2FRedisshujujiegou%2F</url>
    <content type="text"><![CDATA[Redis数据结构介绍 Redis数据结构分为 STRING,LIST,SET,HASH,ZSET五种。与其他数据库或者缓存有相互对应关系。又有他自己的特点。 结构类型 值类型 读写能力 STRING 字符串，整数，浮点数，基本类型 对整个字符串或者字符串其中的一部分进行操作，对整数和浮点数进行自增或者自减 LIST 一个链表，链表上的每个节点都包含了一个字符串 从链表的两端推入或者弹出元素，根据偏移量对链表进行修剪，读取单个或者多个元素；根据值查找或者移除元素 SET 包含字符串的无序搜集器（unordered collection)，并且被包含的每个字符串都是独一无二，各不相同的 添加，获取，移除单个元素；检查一个元素是否存在于集合中；计算交集，并集，差集；从集合里随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对；获取所有键值对 ZSET(有序集合) 字符串成员（member）与浮点数值（score）之间的有序映射，元素的排列顺序由分值的大小决定 添加、获取、删除、单个元素；根据分值范围（染个）或者成员来获取元素 字符串（STRING)基本操作： 命令 行为 GET 获取存储在给定键值中的值 SET 设置存储在给定键中的值 DEL 删除存储在给定键中的值（所有类型适用） 列表（List)基本操作： 命令 行为 RPUSH 给定的值推入列表的右端 LRANGE 获取列表在给定范围上的所有值 LINDEX 获取在列表给定位置上的单个元素 LPOP 从列表的左端弹出一个值，并返回被弹出的值 集合（SET） 和集合一样可以存储多个字符串，不同的是 列表中可以村粗多个相同的字符串。而集合则通过使用散列来保证自己存储的每个字符串都是各自不同的（这些散列只有键没有键值） 基本操作： 命令 行为 SADD 将给定元素添加到集合 SMEMBERS 返回集合包含的所有元素 SISMEMBER 检查给定元素是存在于集合中 SREM 如果给定的元素存在于集合中，那么移除这个元素 另外的操作SINTER,SUNION, SDIFF 分别执行交集计算、并集计算和差集计算。 散列（HASH） Redis的散列可以存储多个键值间的映射。其值，可以是字符串有可以是数字值。也可以对散列存储的值进行自增或自减。 散列在很多方面就是一个缩小版的Redis，不少字符串都有相应的散列版本。 基本操作： 命令 行为 HSET 在散列里面关联起给定的键值对 HGET 获取指定散列键的值 HGETALL 获取散列包含所有键值对 HDEL 如果给定键存在于散列里，那么移除这个键 Redis的散列可以看做文档数据库里的文档，在开发过程中可以很好的对应。在关系书库里可以看做关系数据库里的行。散列、文档、数据行这三者都允许用户同时访问或者修改一个火多个域。 有序集（ZSET） 和散列一样，有序集都用于存储键值对：有序集合的键称为 成员（member）每个成员都各不相同；有序集的值被称为分值（score）必须为浮点数。是唯一一个可以根据成员访问元素，又可以根据分值以及分值的排序来访问元素的结构。 基本操作： 命令 行为 ZADD 将一个带有给定成分值的成员添加到有序集合里 ZRANGE 根据元素在有序排列中所处处的位置，从有序集合中获取多个元素 ZRANGEBYSCORE 获取有序集合给定分值范围内的所有属性 ZREM 如果给定成员存在，移除这个成员]]></content>
      <categories>
        <category>缓存</category>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mysql缓存与Memcached,Redis区别]]></title>
    <url>%2F2017%2F09%2F18%2Fmysql%2FMysql_1%2F</url>
    <content type="text"><![CDATA[前言 我们在做Web开发的时候从至上而下的技术分层里，缓存始终贯穿其中。浏览器层–》业务层–》数据库层。每个层面上的缓存都有各自的功能与场景。我们今天探讨下业务层到数据库层上缓存的功能和区别。 业务层缓存MemcachedMemcached 严格上讲还不能说是完整的分布式缓存系统。它有很多第三方工具支撑其分布式功能。Memcached 通过内部固定的大小的chunk预申请内存数据。使得分配和回收内存的效率很高。读写性能也很高。64k对象的情况下，单机QPS可以达到15W以上。Memcached 的集群架构中，单个节点对其他节点是相互独立的，没有数据方面的通信。不具备failover能力。Memcached 支持多语言，有相当的稳定性。 RedisRedis 显著的特点是不仅支持普通的K，V 类型存储，还支持其独特的 五种数据结构 详见Redis数据结构Redis 也支持集群，Redis支持的集群是Master-Slave模式。其有点是可以在宕机时切换到备份机。可用性方面有一定的提升。Redis 单纯当做缓存存储在内存时速度和Memcached不相上下。存储到硬盘时，性能和速度会下降很多，介于 Memcahced 和mysql之间。Redis 有特殊的订阅功能，使得它经常被用于当做内存队列使用。Redis 扩展方面不如Memcached，无法做到持续的线性扩容。目前支持通过复制的方式，产生一主多备架构并升级容量。 数据库层缓存mysql缓存MySQL将缓存分为Buffer缓存和Cache缓存。Buffer缓存:由于硬盘的写入速度过慢，或者频繁的I/O，对于硬盘来说是极大的效率浪费。那么可以等到缓存中储存一定量的数据之后，一次性的写入到硬盘中。Buffer 缓存主要用于写数据，提升I/O性能。Cache 缓存:Cache 是在开启缓存功能前提下，在通过的每次sql进行hash计算，生成此条sql的唯一hash作为存储的Key值。SO select是区分大小写的。生成缓存之后，如果涉及的table有任何数据的变动（整个talbe),所有的cache就会被删除。如果Cache缓存已经存储满，则启用LRU算法，进行数据淘汰。淘汰掉最远未使用的数据，从而开辟新的存储空间。不过对于特大型的网站，依靠这种策略很难缓解高频率的读请求，一般会把访问非常频繁的数据静态化，直接由nginx返还给用户。程序和数据库I/O设备交互的越少，则效率越高。 问题既然有Memcached,Redis 为什么还要用Mysql缓存呢？ 解答 从整体架构上看，Memcached和Redis支持扩展分布式缓存。适用于大型Web项目。单从单节点功能上看，Mysql由于自身的cache 删除方式。使得其缓存有相对的局限性。并且无法简单的管控。需要更好的使用的话，需要对业务上进行更详尽细致的分析。在数据库的逻辑设计层面细分出能够说回合mysql缓存的场景。单节点，数据简单，无太多修改的数据面前，但根据场景来，mysql缓存还是有一定价值的。比Memcached Redis简单易用，效率更好。 参考：https://dev.mysql.com/doc/refman/5.7/en/query-cache.html]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linux expect ssh自动登录详解]]></title>
    <url>%2F2017%2F09%2F14%2Flinux%2Flinux%20expect%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[简介 Expect是用于自动化交互式应用程序的工具，如telnet，ftp，passwd，fsck，rlogin，tip等。使用起来很简单。 使用方法 首行加上/usr/bin/expect spawn: 后面加上需要执行的shell 命令，比如说spawn sudo touch testfile expect: 只有spawn 执行的命令结果才会被expect 捕捉到，因为spawn 会启动一个进程，只有这个进程的相关信息才会被捕捉到，主要包括：标准输入的提示信息，eof 和timeout。 send 和send_user：send 会将expect 脚本中需要的信息发送给spawn 启动的那个进程，而send_user 只是回显用户发出的信息，类似于shell 中的echo 而已。 实例1:远程拷贝文件1234567891011121314151617181920set timeout 10set host [lindex $argv 0]set username [lindex $argv 1]set password [lindex $argv 2]set src_file [lindex $argv 3]set dest_file [lindex $argv 4]spawn scp $src_file $username@$host:$dest_file expect &#123; &quot;(yes/no)?&quot; &#123; send &quot;yes\n&quot; expect &quot;*assword:&quot; &#123; send &quot;$password\n&quot;&#125; &#125; &quot;*assword:&quot; &#123; send &quot;$password\n&quot; &#125; &#125; expect &quot;100%&quot; expect eof 2:执行远程命令1234567891011121314151617181920set timeout 10set host [lindex $argv 0]set username [lindex $argv 1]set password [lindex $argv 2]set cmd [lindex $argv 3]spawn ssh -t -p $port $username@$host &apos;cmd&apos; expect &#123; &quot;(yes/no)?&quot; &#123; send &quot;yes\n&quot; expect &quot;*assword:&quot; &#123; send &quot;$password\n&quot;&#125; &#125; &quot;*assword:&quot; &#123; send &quot;$password\n&quot; &#125; &#125; expect &quot;100%&quot; expect eof 3：与SSH合用 123/usr/bin/expect &lt;&lt;-EOF//TODO这里写expect脚本 EOF]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[alfred结合七牛实现快速插入markdown格式图片]]></title>
    <url>%2F2017%2F09%2F13%2Fblog%2Falfred%E7%BB%93%E5%90%88%E4%B8%83%E7%89%9B%E5%AE%9E%E7%8E%B0%E5%BF%AB%E9%80%9F%E6%8F%92%E5%85%A5markdown%E6%A0%BC%E5%BC%8F%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[详细过程可以访问：https://github.com/tiann/markdown-img-upload 问题修复 由于retina截屏的图片会放大，所以在markdown脚本里做了处理：有遇到缩放的会进行指定宽度大小。所以会插入&lt;img 标签。但是这不符合markdown的图片方式，这里做了一下改进。将 计算后的size插入七牛的样式图片里就可以解决。 打开workflow的脚本修改保存1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# coding: utf-8from clipboard import get_paste_img_filefrom upload import upload_qiniuimport utilimport osimport subprocessimport sysimport timeif not os.path.exists(util.CONFIG_FILE): util.generate_config_file()config = util.read_config()if not config: util.notice('请先设置你的七牛图床信息') util.open_with_editor(util.CONFIG_FILE) sys.exit(0)url = '%s/%s' % (config['url'], config['prefix'])styleprefix = 'imageView2/2/w/'stylesubfix = '/h/640/format/jpg/q/100|watermark/2/text/d3d3LmxpbGh1aS5jb20=/font/5b6u6L2v6ZuF6buR/fontsize/400/fill/Izk2OEM4Qw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim'mkdprefix='![图片]('mkdsubfix=')'img_file, need_format, format = get_paste_img_file()if img_file: # has image # use time to generate a unique upload_file name, we can not use the tmp file name upload_name = "%s.%s" % (int(time.time() * 1000), format) if need_format: size_str = subprocess.check_output('sips -g pixelWidth %s | tail -n1 | cut -d" " -f4' % img_file.name, shell=True) size = int(size_str.strip()) / 2 #markdown_url = '&lt;img src="%s/%s-1960" width="%d"/&gt;' % (url, upload_name, size) markdown_url = '%s%s/%s?%s%d%s%s' % (mkdprefix, url, upload_name, styleprefix, size, stylesubfix, mkdsubfix) else: markdown_url = '%s%s/%s-960%s' % (mkdprefix, url, upload_name, mkdsubfix) # make it to clipboard os.system("echo '%s' | pbcopy" % markdown_url) os.system('osascript -e \'tell application "System Events" to keystroke "v" using command down\'') upload_file = util.try_compress_png(img_file, format!='gif') if not upload_qiniu(upload_file.name, upload_name): util.notice("上传图片到图床失败，请检查网络后重试")else: util.notice("剪切版里没有图片！") 上面是我修改后的脚本信息，修改的地方是：1markdown_url = &apos;%s%s/%s?%s%d%s%s&apos; % (mkdprefix, url, upload_name, styleprefix, size, stylesubfix, mkdsubfix) 注意这两个变量12styleprefix = &apos;imageView2/2/w/&apos; stylesubfix =&apos;/h/640/format/jpg/q/100|watermark/2/text/d3d3LmxpbGh1aS5jb20=/font/5b6u6L2v6ZuF6buR/fontsize/400/fill/Izk2OEM4Qw==/dissolve/100/gravity/SouthEast/dx/10/dy/10|imageslim&apos; 根据自己的七牛图片拼装 markdown_url即可。我这里用的是通过改变七牛提供的链接图片 size 进行替换。中间拼装计算好的原始图片size即变成正常大小。既： styleprefix + size + stylesubfix]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>alfred</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[solr高级查询edismax函数详解]]></title>
    <url>%2F2017%2F09%2F11%2Fsearch%2Fsolr%E9%AB%98%E7%BA%A7%E6%9F%A5%E8%AF%A2edismax%E5%87%BD%E6%95%B0%E8%AF%A6%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[前言 最近遇到solr查询中加入字段权重的需求，自然而然地想到了edismax这个功能。通过系统的学习和文档阅读，大概了解solr 对于函数式查询的支持方式。为了便于记忆，这里对常用公式进行整理说明。 使用方式详细见官方文档介绍，这里不做说明，我们重点讲solr edismax所涉及到的函数。 bf函数列表 constant 支持小数点的常量例如，1.5，查询表达式就是：_val_:1.5 fieldvalue 返回numberic field的名字.域必须是index的，非multivalue。格式为该域的名字。如果这个域没值，就返回0 ord ord，返回你要查询的那个特定的值在这个顺序中的排名。 非multiValued的，当没有值存在的时候，将返回0 例如：某个特定的域只能去三个值，“apple”、“banana”、“pear”，那么ord（“apple”）=1，ord（“banana”）=2，ord（“pear”）=3 需要注意的是，ord（）这个函数，依赖于值在索引中的位置，所以当有文档被删除、或者添加的时候，ord（）的值就会发生变化。当你使用MultiSearcher的时候，这个值也就是不定的了。 rord 函数将会返回与ord相对应的倒排序的排名。 格式: rord(myIndexedField). sum 就是表示多个数值的“和”。 格式： sum(x,1) sum(x,y) sum(sqrt(x),log(y),z,0.5) product 多个参数的乘积，参数可以是数值，也可以是函数，当为函数时，表示为此函数的计算值乘积。 格式： product(x,2) product(x,y) div 两个参数做除法。支持函数参数 格式： div(x,y) div(sum(x,100),max(y,1)) pow 幂值计算，pow(x,y)=x^y 。支持函数参数。 格式： pow(x,0.5) 标识开方 pow(x, log(y)) abs 返回表达式的绝对值，支持函数参数。 格式： abx(-5) abc(x) log返回对数操作，支持函数参数。格式：log(x)log(sum(x,100)) sqrt返回平方根。与pow(x，0.5)一样。格式：sqrt(2)sqrt(sum(x,100)) map区间检测如果 min&lt;=x&lt;=max，那么map(x,min,max,target)=target，如果x不在[min,max]这个区间内，那么map(x,min,max,target)=x. scala限制参数区间例如：scale(x,minTarget,maxTarget) 这个函数将会把x的值限制在[minTarget,maxTarget]范围内。 query计算subquery查询分数例如：query(subquery,default)表示返回给定的subquery的分数，如果subquery与文档不匹配，那么将会返回默认值。任何的查询类型都是受支持的。可以通过引用的方式，也可以直接指定查询串。q=product(popularity, query({!dismax v=’solr rocks’})) 将会返回popularity和通过dismax 查询得到的分数的乘积q=product(popularity, query($qq)&amp;qq={!dismax}solr rocks) 跟上一个例子的效果是一样的。不过这里使用的是引用的方式q=product(popularity, query($qq,0.1)&amp;qq={!dismax}solr rocks) 在前一个例子的基础上又加了一个默认值。 linear线性函数计算例如：liner(x,m,c)其中 x为变量或者函数，m,c为常量。整个函数取值为： xm+c的值。liner(x,2,4)=2x+4 reciprecip(x,m,a,b) 函数表达式 a/(m*x+b)其中，m、a、b是常量，x是变量或者一个函数。当a=b，并且x&gt;=0的时候，这个函数的最大值是1，值的大小随着x的增大而减小。 max比较大小例如：max(x,c) x可以为变量或者函数，c为常数，返回两个之间最大值。 场景应用 某地的新闻网页库中原本的逻辑是对仓库里的数据字段 subject，message进行搜索。默认是通过score检索字段匹配得分进行排序输出。随着时间的推移，大量的搜索可能会展示两年前，三年前匹配度更高的数据，这些搜索结果明显不合适的。那么我们需要对其进行改造，加入发布时间权重排序。 原本的参数： 1subject:武则天 OR message:武则天 搜索得出结果： 文档得分： 上面我们可以看到，tid为666811的文档排在第一位，得分27.811375 它的dateline时间是：1239781944明显早于第二位 tid：10364925的 1503334472，得分：26.519054。第三位是 tid:9759987 得分：26.511488。这样的搜索结果显然不是很令人满意的。 开启edismax 加入 1bf=sqrt(log(dateline))^100 搜索得出结果： 文档得分： 经过调整，我们得出的结果中排在第一位的是 tid:9759987 其时间dateline是1473820016 得分：330.85 是原本的第三位。原来的排第一的 tid:666811排在了第三位，得分 329.40 原来的第二tid:10364925 得分：329.50 调整后的排序大致满足我们的需求。那么为什么调整后会变成这样的排序呢？ 首先我们要清楚solr的打分机制默认是通过匹配度计算文档相似度得来的。也就是第一次搜索的默认得分，引入edismax的bf函数后我们来分析下最终的结果是怎样，以第一次搜索排名前三的数据为例子： tid | dateline | 初始得分 | 引入bf重新计算 —|—|—|— 666811 | 1239781944 | 27.811382 | 329.40198 10364925 | 1503334472 | 26.519054 |329.50174 9759987 | 1473820016 | 26.511488 | 330.85834 根据bf=sqrt(log(dateline))^100 分别计算上面三个的新得分 1234567891011121314151617sqrt(log(1239781944)) = 3.0155174194591075 权重乘100 得: 301.55174194591075再加 27.811382 =329.36312394591073sqrt(log(1503334472)) = 3.029365546794402权重乘100 得:302.9365546794402再加 26.519054=329.45560867944016sqrt(log(1473820016)) = 3.0279439311841663权重乘100 得:302.79439311841663再加 26.511488=329.3058811184166 纳尼。很奇怪为什么 9759987 计算最小 不对劲 于是翻看原来前面查询的debug列表分析仔细看原来是原图： 添加edismax后： 对比以上靓图，原来是我们的Qparser不一样。在普通查询的时候我么使用的是定制化的 SWMCLuceneQparser 查询解析器。而 用edimax后，解析器变成了 ExtendDismaxQparser 这两个差别在于 定制化的 SWMCLuceneQparser会将查询字段通过IK分词转换后进行查询。其parsedquery_tostring 变成1&quot;parsedquery&quot;:&quot;PhraseQuery(subject:\&quot;武 则 天\&quot;) PhraseQuery(message:\&quot;武 则 天\&quot;)&quot;, ExtendDismaxQparser的 parsedquery_tostringshi :1&quot;parsedquery_toString&quot;:&quot;+((subject:武 subject:则 subject:天) (message:武 message:则 message:天)) (sqrt(log(long(dateline))))^10.0&quot;, 两者稍有不同，所以在计算最终权重的时候有些差异。]]></content>
      <categories>
        <category>搜索</category>
      </categories>
      <tags>
        <tag>solr</tag>
        <tag>edismax</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo技术博客搭建日记]]></title>
    <url>%2F2017%2F09%2F02%2Fblog%2Fhexo%E6%8A%80%E6%9C%AF%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E6%97%A5%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[9月2号听说Hexo 案例1 案例2 9月3号了解Hexo搭建博客方式 hexo搭建参考1 hexo搭建参考2 hexo主题 9月5号搭建完成 9月6号添加域名解析 9月7号添加Gitment评论功能 Gitment的github地址 9月9号配置结合alfred + 七牛 快捷插入markdown图片工具 Github地址 9月11号第一篇文章登陆 9月12号完善主题配置 9月13日添加站点收录 npm install hexo-generator-sitemap –save npm install hexo-generator-baidu-sitemap –save 搜索引擎站点收录]]></content>
      <categories>
        <category>blog</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
        <tag>IntelliJ IDEA</tag>
        <tag>Git</tag>
        <tag>Github</tag>
      </tags>
  </entry>
</search>
